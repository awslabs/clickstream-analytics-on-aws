{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"architecture-details/","title":"How the solution works","text":"<p>The Clickstream Analytics on AWS solution has three components: a web console, SDKs, and data pipeline.</p>"},{"location":"architecture-details/#web-console","title":"Web console","text":"<p>This solution provides a web console which allows you to create clickstream projects, and configure, deploy, and manage  data pipeline for each clickstream project.</p>"},{"location":"architecture-details/#sdks","title":"SDKs","text":"<p>This solution provides native SDKs for help you easily collect and report in-app events from your applications to data pipelines.</p> <ul> <li>Android SDK</li> <li>Swift SDK</li> <li>Web SDK</li> </ul>"},{"location":"architecture-details/#data-pipeline","title":"Data pipeline","text":"<p>This solution uses the web console to manage the project and its data pipeline. The data pipeline consists of four modules.</p>"},{"location":"architecture-details/#ingestion-module","title":"Ingestion module","text":"<p>The ingestion module serves as web server for ingesting the Clickstream data. It supports the following features:</p> <ul> <li>specify the auto scaling group capability</li> <li>specify warm pool size to scale out faster and save costs</li> <li>support authenticate with OIDC</li> <li>support SSL</li> <li>support enabling AWS Global Accelerator for ELB</li> <li>support different data sinks, including S3, KDS and MSK</li> </ul>"},{"location":"architecture-details/#data-processing-module","title":"Data processing module","text":"<p>The data processing module transforms and enriches the ingested data to solution's data model by the Apache Spark application running in EMR serverless. It supports the following features:</p> <ul> <li>specify the batch interval of data processing</li> <li>specify the data refreshness age</li> <li>provider out-of-the-box enrichment plug-ins</li> <li>UA enrichment to parse OS, device, browser information from User Agent string of the HTTP request header</li> <li>IP enrichment to mapping device location information (for example, city, country, region) based on the request source IP</li> <li>support third-party transformer plug-ins</li> <li>support third-party enrichment plug-ins</li> </ul>"},{"location":"architecture-details/#data-modeling-module","title":"Data modeling module","text":"<p>The data modeling module loads the processed data into data lakehouse. It supports the following features:</p> <ul> <li>support both provisioned Redshift and Redshift Serverless as data warehouse</li> <li>support the data range for hot data keeping in Redshift</li> <li>specify the interval to update user dimension table</li> <li>support use Athena to query the data in data lake</li> </ul>"},{"location":"architecture-details/#reporting-module","title":"Reporting module","text":"<p>The reporting module connects to the processed data stored in data warehouse (i.e., Redshift) to provide the following data query and reporting features:</p> <ul> <li>provide pre-canned user life cycle dashboard.</li> <li>provide explorative analytics model to query and analyze clickstream data.</li> <li>support creating custom analysis and visualization in drag-and-drop manner</li> <li>auto-generate metadata for clickstream data, support metadata management</li> </ul>"},{"location":"architecture/","title":"Solution end-to-end architecture","text":"<p>Deploying this solution with the default parameters builds the following environment in AWS:</p> <p> </p> Figure 1: Clickstream Analytics on AWS architecture <p>This solution deploys the Amazon CloudFormation template in your AWS account and completes the following settings.</p> <ol> <li>Amazon CloudFront distributes the frontend web UI assets hosted in the Amazon S3 bucket, and the backend APIs hosted with Amazon API Gateway and AWS Lambda.</li> <li>The Amazon Cognito user pool or OpenID Connect (OIDC) is used for authentication.</li> <li>The web UI console uses Amazon DynamoDB to store persistent data.</li> <li>AWS Step Functions, AWS CloudFormation, AWS Lambda, and Amazon EventBridge are used for orchestrating the lifecycle management of data pipelines.</li> <li>The data pipeline is provisioned in the region specified by the system operator. It consists of Application Load Balancer (ALB), Amazon ECS, Amazon Managed Streaming for Kafka (Amazon MSK), Amazon Kinesis Data Streams, Amazon S3, Amazon EMR Serverless, Amazon Redshift, and Amazon QuickSight.</li> </ol>"},{"location":"architecture/#data-pipeline","title":"Data Pipeline","text":"<p>The key functionality of this solution is to build a data pipeline to collect, process, and analyze their clickstream data. The data pipeline consists of four modules: </p> <ul> <li>ingestion module </li> <li>data processing module </li> <li>data modeling module </li> <li>reporting module </li> </ul> <p>The following introduces the architecture diagram for each module.</p>"},{"location":"architecture/#ingestion-module","title":"Ingestion module","text":"Figure 2: Ingestion module architecture <p>Suppose you create a data pipeline in the solution. This solution deploys the Amazon CloudFormation template in your AWS account and completes the following settings.</p> <p>Note</p> <p>The ingestion module supports three types of data sinks.</p> <ol> <li>(Optional) The ingestion module creates an AWS global accelerator endpoint to reduce the latency of sending events from your clients (web applications or mobile applications).</li> <li>Elastic Load Balancing (ELB) is used for load balancing ingestion web servers.</li> <li>(Optional) If you enable the authenticating feature, the ALB will communicate with the OIDC provider to authenticate the requests.</li> <li>ALB forwards all authenticated and valid requests to the ingestion servers.</li> <li>Amazon ECS cluster is hosting the ingestion fleet servers. Each server consists of a proxy and a worker service. The proxy is a facade of the HTTP protocol, and the worker will send the events to a data sink based on your choice.</li> <li>Amazon Kinesis Data Streams is used as a buffer. AWS Lambda consumes the events in Kinesis Data Streams and then sinks them to Amazon S3 in batches.</li> <li>Amazon MSK or self-built Kafka is used as a buffer. MSK Connector is provisioned with an S3 connector plugin that sinks the events to Amazon S3 in batches.</li> <li>The ingestion server will buffer a batch of events and sink them to Amazon S3.</li> </ol>"},{"location":"architecture/#data-processing-module","title":"Data processing module","text":"Figure 3: Data processing module architecture <p>Suppose you create a data pipeline in the solution and enable ETL. This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li>Amazon EventBridge is used to trigger the ETL jobs periodically.</li> <li>The configurable time-based scheduler invokes an AWS Lambda function.</li> <li>The Lambda function kicks off an EMR Serverless application based on Spark to process a batch of clickstream events.</li> <li>The EMR Serverless application uses the configurable transformer and enrichment plug-ins to process the clickstream events from the source S3 bucket.</li> <li>After processing the clickstream events, the EMR Serverless application sinks the processed events to the sink S3 bucket.</li> </ol>"},{"location":"architecture/#data-modeling-module","title":"Data modeling module","text":"Figure 4: Data modeling in Redshift architecture <p>Suppose you create a data pipeline in the solution and enable data modeling in Amazon Redshift. This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li>After the processed clickstream events data is written in the Amazon S3 bucket, the <code>Object Created Event</code> is emitted.</li> <li>An Amazon EventBridge rule is created for the event emitted in step 1, and an AWS Lambda function is invoked when the event happens.</li> <li>The Lambda function persists the source event to be loaded in an Amazon DynamoDB table.</li> <li>When data processing job is done, an event is emitted to Amazon EventBridge.</li> <li>The pre-defined event rule of Amazon EventBridge processes the <code>EMR job success event</code>.</li> <li>The rule invokes the AWS Step Functions workflow.</li> <li>The workflow invokes the <code>list objects</code> Lambda function that queries the DynamoDB table to find out the data to be loaded, then creates a manifest file for a batch of event data to optimize the load performance.</li> <li>After a few seconds, the <code>check status</code> Lambda function starts to check the status of loading job.</li> <li>If the load is still in progress, the <code>check status</code> Lambda function waits a few more seconds.</li> <li>After all objects are loaded, the workflow ends.</li> </ol> <p> </p> Figure 5: Scan metadata architecture <ol> <li>Once the load data workflow is completed, the scan metadata workflow will be triggered.</li> <li>The Lambda function checks whether the workflow should be started or not. If the interval since the last workflow initiation is less than one day or if the previous workflow has not yet finished, the current workflow is skipped.</li> <li>If it is necessary to start the current workflow, the <code>submit job</code> Lambda function is triggered.</li> <li>The Lambda function submits the stored procedure of scan metadata job, initiating the metadata scanning process.</li> <li>After a few seconds, the <code>check status</code> Lambda function starts to check the status of the scan job.</li> <li>If the scan is still in progress, the <code>check status</code> Lambda function waits for a few more seconds.</li> <li>Once the data scanning is completed, the <code>store metadata</code> Lambda function is triggered.</li> <li>The Lambda function saves the metadata to the DynamoDB table, the workflow ends.</li> </ol> <p> </p> Figure 6: Data modeling in Athena architecture <p>Suppose you create a data pipeline in the solution and enable data modeling in Amazon Athena. This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li>Amazon EventBridge initiates the data load into Amazon Athena periodically.</li> <li>The configurable time-based scheduler invokes an AWS Lambda function.</li> <li>The AWS Lambda function creates the partitions of the AWS Glue table for the processed clickstream data.</li> <li>Amazon Athena is used for interactive querying of clickstream events.</li> <li>The processed clickstream data is scanned via the Glue table.</li> </ol>"},{"location":"architecture/#reporting-module","title":"Reporting module","text":"Figure 7: Reporting module architecture <p>Suppose you create a data pipeline in the solution, enable data modeling in Amazon Redshift, and enable reporting in Amazon QuickSight. This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li>VPC connection in Amazon QuickSight is used for securely connecting your Redshift within VPC.</li> <li>The data source, data sets, template, analysis, and dashboard are created in Amazon QuickSight for out-of-the-box analysis and visualization.</li> </ol>"},{"location":"architecture/#analytics-studio","title":"Analytics Studio","text":"<p>Analytics Studio is a unified web interface for business analysts or data analysts to view and create dashboards, query and explore clickstream data, and manage metadata.</p> <p> </p> Figure 8: Analytics studio architecture <ol> <li>When analysts access Analytics Studio, requests are sent to Amazon CloudFront, which distributes the web application.</li> <li>When the analysts log in to Analytics Studio, the requests are redirected to the Amazon Cognito user pool or OpenID Connect (OIDC) for authentication.</li> <li>Amazon API Gateway hosts the backend API requests and uses the custom Lambda authorizer to authorize the requests with the public key of OIDC.</li> <li>API Gateway integrates with AWS Lambda to serve the API requests.</li> <li>The Lambda function uses Amazon DynamoDB to retrieve and persist the data.</li> <li>When analysts create analyses, the Lambda function requests Amazon QuickSight to create assets and get the embed URL in the data pipeline region.</li> <li>The browser of analysts access the QuickSight embed URL to view the QuickSight dashboards and visuals.</li> </ol>"},{"location":"aws-services/","title":"AWS services in this solution","text":"<p>This section describes the components and AWS services that make up this solution and the architecture details on how these components work together.</p>"},{"location":"aws-services/#aws-services-in-this-solution","title":"AWS services in this solution","text":"<p>The following AWS services are included in this solution:</p> AWS service Description Amazon Elastic Load Balancing Core.  To distribute network traffic to ingestion fleet. Amazon ECS Core.  To run the ingestion module fleet. Amazon EC2 Core. To provide the underlying computing resources for ingestion fleet. Amazon ECR Core. To host the container images used by ingestion fleet. Amazon S3 Core. To store the ingested and processed Clickstream data. And it also stores the service logs and static web assets (frontend user interface). AWS Global Accelerator Supporting. To improve the availability, performance, and security of the ingestion service in AWS Regions. AWS CloudWatch Supporting. To monitor the metrics, logs and trace of data pipeline. Amazon SNS Supporting. To provide topic and email subscription notifications for the alarms of data pipeline. Amazon Kinesis Data Streams Supporting. To provide the ingestion buffer. AWS Lambda Supporting. To integrate with kinds of AWS services. For example, sink ingestion data to S3, manage the lifecycle of AWS resources. Amazon Managed Streaming for Apache Kafka (MSK) Supporting. To provide the ingestion buffer with Apache Kafka. Amazon EMR Serverless Supporting. To process the ingested data. Amazon Glue Supporting. To manage the data catalog of ingested data. Amazon EventBridge Supporting. To integrate with AWS services with events or schedule. Amazon Redshift Supporting. To analyze your Clickstream data in data warehouse. Amazon Athena Supporting. To analyze your Clickstream data in data lake. AWS Step Functions Supporting. To orchestrate the lifecycle management of project's pipeline. Also it manages the workflow to load data into data warehouse. AWS Secrets Manager Supporting. To store the credential for OIDC credentials and BI user in Redshift. Amazon QuickSight Supporting. Visual your analysis reporting of your Clickstream data. Amazon CloudFront Supporting. To made available the static web assets (frontend user interface) and proxy the backend in the same origin. Amazon Cognito Supporting. To authenticate users (in AWS Regions). Amazon API Gateway Supporting. To provide the backend APIs. Amazon DynamoDB Supporting.  To store projects data. AWS CloudFormation Supporting.  To provision the AWS resources for the modules of data pipeline."},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general","title":"General","text":"<p>Q:  What is Clickstream Analytics on AWS? An AWS Solution that enables customers to build clickstream analytic system on AWS easily. This solution automates the data pipeline creation per customers\u2019 configurations with a visual pipeline builder, and provides SDKs for web and mobiles apps (including iOS, Android, and Web JS) to help customers to collect and ingest client-side data into the data pipeline on AWS. After data ingestion, the solution allows customers to further enrich and model the event data for business users to query, and provides built-in visualizations (e.g., acquisition, engagement, retention) to help them generate insights faster.</p>"},{"location":"faq/#sdk","title":"SDK","text":"<p>Q: Can I use other SDK to send data to the pipeline created by this solution Yes, you can. The solution support users using third-party SDK to send data to the pipeline. Note that, if you want to enable data processing and modeling module when using a third-party SDK to send data, you will need to provide an transformation plugin to map third-party SDK's data structure to solution data schema. Please refer to Custom plugin for more details.</p>"},{"location":"faq/#analytics-studio","title":"Analytics Studio","text":"<p>Q: Why is the Analytics Studio is not available? The reason for this prompt is due to the following two situations:</p> <ul> <li>The version of the pipeline is not v1.1 or higher. You can try upgrading the pipeline and wait for the upgrade to complete before trying again.</li> <li>The reporting module is not enabled on the pipeline.</li> </ul>"},{"location":"faq/#pricing","title":"Pricing","text":"<p>Q: How will I be charged and billed for the use of this solution? The solution is free to use, and you are responsible for the cost of AWS services used while running this solution.  You pay only for what you use, and there are no minimum or setup fees. Refer to the Cost section for detailed cost estimation. </p>"},{"location":"identity-mgmt/","title":"Identity Management","text":""},{"location":"identity-mgmt/#users-introduction","title":"Users Introduction","text":"<p>Clickstream Analytics on AWS supports a built-in Cognito user pool or third-party OpenID Connect (OIDC) for user management based on your deployment type.</p>"},{"location":"identity-mgmt/#user-management","title":"User management","text":"<p>If you use built-in Cognito for user management, you can find the Cognito user pool starting with <code>userPoolDC9497E0</code> in your deployment region. When you deploy the web console of the solution, a user with the required email address will be created as the first user with administrator permission. Following this documentation, you can manage other users. You can also follow this documentation to add federated third-party providers, such as SAML and OIDC.</p> <p>If you are using an OIDC provider for user management, you need to follow the documentation of the OIDC provider to manage the users.</p>"},{"location":"identity-mgmt/#user-roles","title":"User roles","text":"<p>There are four different types of Roles that you can assign to Users:</p> Role Description Administrator Full access to the solution, including identity management Operator Manage projects, alarms, and plug-ins Analyst View and update in Analytics Studio Analyst Reader View in Analytics Studio <p>The specific features for roles are shown in the following table:</p> Feature Administrator Operator Analyst Analyst Reader Project management Yes Yes No No Operation and Alarm Yes Yes No No Plugin Management Yes Yes No No Identity Management Yes No No No Analytics Studio - Dashboards Yes No Yes Yes Analytics Studio - Explore Yes No Yes Yes Analytics Studio - Analyzes Yes No Yes No Analytics Studio - Data Management Yes No Yes Yes"},{"location":"identity-mgmt/#user-roles-management","title":"User roles management","text":"<p>By default, the authenticated users do not have a role in the solution. You have below two ways to manage the user roles in the solution:</p> <ol> <li>Choose System - Users in the web console of the solution as <code>Administrator</code> user, add, update, or remove the user roles. This setting has precedence over other settings.</li> <li>Click the Setting in System - Users in the web console of the solution as <code>Administrator</code> user. Configure the roles of the solution mapping to the groups or roles in your OIDC provider.</li> </ol> <p>By default, the solution supports mapping group information from the Cognito user pool to multiple roles in the solution with the following rules:</p> Group name in Cognito Solution roles ClickstreamAdmin Administrator ClickstreamOperator Operator ClickstreamAnalyst Analyst ClickstreamAnalystReader Analyst Reader <p>For example, you create a group named <code>ClickstreamAnalyst</code>, then add users in the user pool to that group. After those users log in to the solution, the user has an analyst role to access Analyst Studio.</p> <p>Support mapping multiple groups to a single system role, with various group names separated by commas. For example, by modifying the Operator Role Name: <code>Group1,Group2</code>, both user groups can be mapped to the Operator role of the system.</p> <p>If you need to support other OIDC providers, modify User Role Json Path.</p> <p>Example: Modify User Role Json Path to <code>$.payload.realm_access.roles</code>, It can support the mapping of Keycloak roles to solution roles, where the token format of Keycloak is as follows:</p> <pre><code>{\n  \"exp\": 1701070445,\n  \"iat\": 1701063245,\n  \"auth_time\": 1701062050,\n  \"jti\": \"4a892061-56e1-4997-a5f3-84a5d38215f0\",\n  \"iss\": \"https://keycloak.xxxx.cn/auth/realms/xxx\",\n  \"aud\": \"P****************Y\",\n  \"sub\": \"29563a2d-****-43bb-b861-c163da7fe984\",\n  \"typ\": \"ID\",\n  \"azp\": \"P****************Y\",\n  \"session_state\": \"4df36df4-****-4e53-9c1a-43e6d27ffbb9\",\n  \"at_hash\": \"P****************Y\",\n  \"acr\": \"0\",\n  \"sid\": \"4df36df4-****-4e53-9c1a-43e6d27ffbb9\",\n  \"email_verified\": false,\n  \"realm_access\": {\n    \"roles\": [\n      \"role1\",\n      \"role2\",\n      \"role3\",\n    ]\n  },\n  \"preferred_username\": \"your name\",\n  \"email\": \"your-name@example.com\"\n}\n</code></pre>"},{"location":"notices/","title":"Notices","text":"<p>Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents Amazon Web Services current product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from Amazon Web Services and its affiliates, suppliers or licensors. Amazon Web Services products or services are provided \u201cas is\u201d without warranties, representations, or conditions of any kind, whether express or implied. Amazon Web Services responsibilities and liabilities to its customers are controlled by Amazon Web Services agreements, and this document is not part of, nor does it modify, any agreement between Amazon Web Services and its customers.</p> <p>Clickstream Analytics on AWS is licensed under the terms of the Apache License Version 2.0 available at The Apache Software Foundation.</p>"},{"location":"revisions/","title":"Revisions","text":"Date Change July 2023 Initial release Sep 2023 Add Web SDK"},{"location":"source/","title":"Source code","text":"<p>Visit our GitHub repository to download the source code for this solution. The Clickstream Analytics on AWS template is generated using the AWS Cloud Development Kit (CDK). Refer to the README.md file for additional information.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>The following help you to fix errors or problems that you might encounter when using Clickstream Analytics on AWS.</p>"},{"location":"troubleshooting/#problem-deployment-failure-due-to-invalid-logging-configuration-the-cloudwatch-logs-resource-policy-size-was-exceeded","title":"Problem: Deployment failure due to \"Invalid Logging Configuration: The CloudWatch Logs Resource Policy size was exceeded\"","text":"<p>If you encounter a deployment failure due to creating CloudWatch log group with an error message like the one below,</p> <p>Cannot enable logging. Policy document length breaking Cloudwatch Logs Constraints, either &lt; 1 or &gt; 5120 (Service: AmazonApiGatewayV2; Status Code: 400; Error Code: BadRequestException; Request ID: xxx-yyy-zzz; Proxy: null)</p> <p>Resolution:</p> <p>CloudWatch Logs resource policies are limited to 5120 characters. The remediation is merging or removing useless policies, then updating the resource policies of CloudWatch logs to reduce the number of policies.</p> <p>Below is a sample command to reset resource policy of CloudWatch logs:</p> <pre><code>aws logs put-resource-policy --policy-name AWSLogDeliveryWrite20150319 \\\n--policy-document '\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AWSLogDeliveryWrite2\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"delivery.logs.amazonaws.com\"\n      },\n      \"Action\": [\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n      ],\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:SourceAccount\": \"&lt;your AWS account id&gt;\"\n        },\n        \"ArnLike\": {\n          \"aws:SourceArn\": \"arn:aws:logs:&lt;AWS region&gt;:&lt;your AWS account id&gt;:*\"\n        }\n      }\n    }\n  ]\n}\n'\n</code></pre>"},{"location":"troubleshooting/#problem-can-not-delete-the-cloudformation-stacks-created-for-the-clickstream-pipeline","title":"Problem: Can not delete the CloudFormation stacks created for the Clickstream pipeline","text":"<p>If you encounter a failure with an error message like the one below when deleting the CloudFormation stacks created for the Clickstream pipeline,</p> <p>Role arn:aws:iam::&lt;your AWS account id&gt;:role/&lt;stack nam&gt;-ClickStreamApiStackActionSta-&lt;random suffix&gt; is invalid or cannot be assumed</p> <p>Resolution:</p> <p>It results from deleting the web console stack for this solution before the CloudFormation stacks are made for the Clickstream pipeline.</p> <p>Please create a new IAM role with the identical name mentioned in the above error message and trust the CloudFormation service with sufficient permission to delete those stacks.</p> <p>Tip</p> <p>You can delete the IAM role after successfully removing those CloudFormation stacks.</p>"},{"location":"troubleshooting/#problem-can-not-sink-data-to-msk-cluster-got-invalidreplicationfactor-broker-invalid-replication-factor-log-in-ingestion-server","title":"Problem: Can not sink data to MSK cluster, got \"InvalidReplicationFactor (Broker: Invalid replication factor)\" log in Ingestion Server","text":"<p>If you notice that data can not be sunk into S3 through MSK cluster, and the error message in log of Ingestion Server (ECS) worker task is as below:</p> <p>Message production error: InvalidReplicationFactor (Broker: Invalid replication factor)</p> <p>Resolution:</p> <p>This is caused by replication factor larger than available brokers, please edit the MSK cluster configuration, set default.replication.factor not larger than the total number of brokers.</p>"},{"location":"troubleshooting/#problem-data-processing-job-failure","title":"Problem: data processing job failure","text":"<p>If the data processing job implemented by EMR serverless fails with the below errors:</p> <ul> <li> <p>IOException: No space left on device</p> <p>Job failed, please check complete logs in configured logging destination. ExitCode: 1. Last few exceptions: Caused by: java.io.IOException: No space left on device Exception in thread \"main\" org.apache.spark.SparkException:</p> </li> <li> <p>ExecutorDeadException</p> <p>Job failed, please check complete logs in configured logging destination. ExitCode: 1. Last few exceptions: Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 34), which maintains the block data to fetch is dead. org.apache.spark.shuffle.FetchFailedException Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage</p> </li> <li> <p>Could not find CoarseGrainedScheduler</p> <p>Job failed, please check complete logs in configured logging destination. ExitCode: 1. Last few exceptions: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.</p> </li> </ul> <p>You need to tune the EMR job default configuration, and please refer to the configure execution parameters.</p>"},{"location":"troubleshooting/#problem-reporting-stackclickstream-reporting-xxx-deployment-fail","title":"Problem: Reporting stack(Clickstream-Reporting-xxx) deployment fail","text":"<p>Reporting stack deployment failed with message like </p> <p>Connection attempt timed out</p> <p>And it happened when creating DataSource(AWS::QuickSight::DataSource).</p> <p>Resolution:</p> <p>Login solution web console and click \"Retry\" button in pipeline detail information page.</p>"},{"location":"uninstall/","title":"Uninstall the Clickstream Analytics on AWS","text":"<p>Warning</p> <p>You will encounter an IAM role missing error if you delete Clickstream Analytics on AWS main stack before you delete the stacks created for Clickstream projects. Clickstream Analytics on AWS console launches additional CloudFormation stacks for the Clickstream pipelines. We recommend you delete projects before uninstalling the solution.</p>"},{"location":"uninstall/#step-1-delete-projects","title":"Step 1. Delete projects","text":"<ol> <li>Go to the Clickstream Analytics on AWS console. </li> <li>In the left sidebar, choose Projects.</li> <li>Select the project to be deleted.</li> <li>Choose the Delete button in the upper right corner.</li> <li>Repeat steps 3 and 4 to delete all your projects.</li> </ol>"},{"location":"uninstall/#step-2-delete-clickstream-analytics-on-aws-stack","title":"Step 2. Delete Clickstream Analytics on AWS stack","text":"<ol> <li>Go to the CloudFormation console.</li> <li>Find the CloudFormation stack of the solution.</li> <li>Delete the CloudFormation Stack of the solution.</li> <li>(Optional) Delete the S3 bucket created by the solution.<ol> <li>Choose the CloudFormation stack of the solution, and select the Resources tab.</li> <li>In the search bar, enter <code>DataBucket</code>. It shows all resources with the name <code>DataBucket</code> created by the solution. You can find the resource type AWS::S3::Bucket, and the Physical ID field is the S3 bucket name.</li> <li>Go to the S3 console, and find the S3 bucket with the bucket name. Empty and Delete the S3 bucket.</li> </ol> </li> </ol>"},{"location":"upgrade/","title":"Upgrade the solution","text":""},{"location":"upgrade/#planning-and-preparation","title":"Planning and Preparation","text":"<ol> <li>Data Processing interval: The pipeline upgrade will take about 20 minutes; ensure no data processing job is running while upgrading the existing pipeline. You can update the existing pipeline to increase the interval and view whether there are running jobs of the EMR Serverless application in the console.</li> <li>Backup modified QuickSight Analysis and Dashboard: The solution upgrade might update the out-of-box analysis and dashboard. If you changed it, please follow this documentation to back them up.</li> </ol>"},{"location":"upgrade/#upgrade-process","title":"Upgrade Process","text":""},{"location":"upgrade/#upgrade-web-console-stack","title":"Upgrade web console stack","text":"<ol> <li>Log in to AWS CloudFormation console, select your existing web console stack, and choose Update.</li> <li>Select Replace current template.</li> <li> <p>Under Specify template:</p> <ul> <li>Select Amazon S3 URL.</li> <li>Refer to the table below to find the link for your deployment type.</li> <li>Paste the link in the Amazon S3 URL box.</li> <li>Choose Next again.</li> </ul> Template Description Use Cognito for authentication Deploy as public service in AWS regions Use Cognito for authentication with custom domain Deploy as public service with custom domain in AWS regions Use OIDC for authentication Deploy as public service in AWS regions Use OIDC for authentication with custom domain Deploy as public service with custom domain in AWS regions Use OIDC for authentication within VPC Deploy as private service within VPC in AWS regions Use OIDC for authentication with custom domain in AWS China Deploy as public service with custom domain in AWS China regions Use OIDC for authentication within VPC in AWS China Deploy as private service within VPC in AWS China regions </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. Refer to Deployment for details about the parameters.</p> </li> <li>Choose Next.</li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Be sure to check the box acknowledging that the template might create (IAM) resources.</li> <li>Choose View change set and verify the changes.</li> <li>Choose Execute change set to deploy the stack.</li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive an <code>UPDATE_COMPLETE</code> status after a few minutes.</p>"},{"location":"upgrade/#upgrade-the-pipeline-of-project","title":"Upgrade the pipeline of project","text":"<ol> <li>Log in to the web console of the solution.</li> <li>Verify the solution version at the right-bottom of the page starting with <code>v1.1.0</code>. If not, you can force reload the page to recheck it.</li> <li>Go to Projects, and choose the project to be upgraded.</li> <li>Click on <code>project id</code> or View Details button, which will direct to the pipeline detail page.</li> <li>In the project details page, click on the Upgrade button</li> <li>You will be prompted to confirm the upgrade action.</li> <li>Click on Confirm, the pipeline will be in <code>Updating</code> status.</li> </ol> <p>You can view the status of the pipeline in the solution console in the Status column. You should receive an <code>Active</code> status after a few minutes.</p>"},{"location":"upgrade/#post-upgrade-actions","title":"Post-Upgrade Actions","text":""},{"location":"upgrade/#migrate-the-existing-data-after-upgrading-from-10x","title":"Migrate the existing data after upgrading from 1.0.x","text":"<p>When you upgraded the pipeline from v1.0.x, you need to perform the below actions to migrate data from old table <code>ods_events</code> to new tables <code>event</code>, <code>event_parameter</code>, <code>user</code>, and <code>item</code> in the Redshift:</p> <ol> <li> <p>Open Redshift query editor v2. You can refer to AWS doc Working with query editor v2 to log in and query data using Redshift query editor v2.</p> <p>Note</p> <p>You must use the <code>admin</code> user or the user with schema (known as the app ID) ownership permission.</p> </li> <li> <p>Select the Serverless workgroup or provisioned cluster, <code>&lt;project-id&gt;</code>-&gt;<code>&lt;app-id&gt;</code>-&gt;Tables, and make sure tables for the appId are listed there.</p> </li> <li> <p>Create a new SQL Editor\u3002</p> </li> <li> <p>Execute below SQL in editor.</p> <pre><code>-- please replace `&lt;app-id&gt;` with your actual app id\nCALL \"&lt;app-id&gt;\".sp_migrate_ods_events_1_0_to_1_1();\n</code></pre> </li> <li> <p>Wait for the SQL to complete. The execution time depends on the volume of data in table <code>ods_events</code>.</p> </li> <li> <p>Execute the below SQL to check the stored procedure execution log; make sure there are no errors there.</p> <pre><code>-- please replace `&lt;app-id&gt;` with your actual app id\nSELECT * FROM  \"&lt;app-id&gt;\".\"clickstream_log\" where log_name = 'sp_migrate_ods_events' order by log_date desc;\n</code></pre> </li> <li> <p>If you don't have other applications using the legacy tables and views, you could run the SQLs below to clean the legacy views and tables to save the storage of Redshift.</p> <pre><code>-- please replace `&lt;app-id&gt;` with your actual app id\nDROP TABLE \"&lt;app-id&gt;\".dim_users CASCADE;\nDROP TABLE \"&lt;app-id&gt;\".ods_events CASCADE;\n\nDROP PROCEDURE  \"&lt;app-id&gt;\".sp_clear_expired_events(retention_range_days integer);\nDROP PROCEDURE  \"&lt;app-id&gt;\".sp_upsert_users();\nDROP PROCEDURE  \"&lt;app-id&gt;\".sp_migrate_ods_events_1_0_to_1_1();\n</code></pre> </li> </ol>"},{"location":"well-architected-pillars/","title":"AWS Well-Architected pillars","text":"<p>This solution was designed with best practices from the AWS Well-Architected Framework which helps customers design and operate reliable, secure, efficient, and cost-effective workloads in the cloud.</p> <p>This section describes how the design principles and best practices of the Well-Architected Framework were applied when building this solution.</p>"},{"location":"well-architected-pillars/#operational-excellence","title":"Operational excellence","text":"<p>This section describes how the principles and best practices of the operational excellence pillar were applied when designing this solution.</p> <p>The Clickstream Analytics on AWS solution pushes metrics, logs and traces to Amazon CloudWatch at various stages to provide observability into the infrastructure, Elastic load balancer, Amazon ECS cluster, Lambda functions, EMR serverless application, Step Function workflow and the rest of the solution components. This solution also creates the CloudWatch dashboard for each data pipeline.</p>"},{"location":"well-architected-pillars/#security","title":"Security","text":"<p>This section describes how the principles and best practices of the security pillar were applied when designing this solution.</p> <ul> <li>Clickstream Analytics on AWS web console users are authenticated and authorized with Amazon Cognito or OpenID Connect.</li> <li>All inter-service communications use AWS IAM roles.</li> <li>All roles used by the solution follows least-privilege access. That is, it only contains minimum permissions required so the service can function properly.</li> </ul>"},{"location":"well-architected-pillars/#reliability","title":"Reliability","text":"<p>This section describes how the principles and best practices of the reliability pillar were applied when designing this solution.</p> <ul> <li>Using AWS serverless services wherever possible (for example, EMR Serverless, Redshift Serverless, Lambda, Step Functions, Amazon S3, and Amazon SQS) to ensure high availability and recovery from service failure.</li> <li>Data ingested by data pipeline is stored in Amazon S3 and Amazon Redshift, so it persists in multiple Availability Zones (AZs) by default.</li> </ul>"},{"location":"well-architected-pillars/#performance-efficiency","title":"Performance efficiency","text":"<p>This section describes how the principles and best practices of the performance efficiency pillar were applied when designing this solution.</p> <ul> <li>The ability to launch this solution in any Region that supports AWS services in this solution such as: Amazon S3, Amazon ECS, Elastic load balancer.</li> <li>Using serverless architecture removes the need for you to run and maintain physical servers for traditional compute activities.</li> <li>Automatically testing and deploying this solution daily. Reviewing this solution by solution architects and subject matter experts for areas to experiment and improve.</li> </ul>"},{"location":"well-architected-pillars/#cost-optimization","title":"Cost optimization","text":"<p>This section describes how the principles and best practices of the cost optimization pillar were applied when designing this solution.</p> <ul> <li>Use Autoscaling Group so that the compute costs are only related to how much data is ingested and processed.</li> <li>Using serverless services such as Amazon S3, Amazon Kinesis Data Streams, Amazon EMR Serverless and Amazon Redshift Serverless so that customers only get charged for what they use.</li> </ul>"},{"location":"well-architected-pillars/#sustainability","title":"Sustainability","text":"<p>This section describes how the principles and best practices of the sustainability pillar were applied when designing this solution.</p> <ul> <li>The solution\u2018s serverless design (using Amazon Kinesis Data Streams, Amazon EMR Serverless, Amazon Redshift Serverless and Amazon QuickSight) and the use of managed services (such as Amazon ECS, Amazon MSK) are aimed at reducing carbon footprint compared to the footprint of continually operating on-premises servers.</li> </ul>"},{"location":"analytics/","title":"Analytics Studio","text":"<p>Analytics Studio is a unified web interface for business analysts or data analysts to view and create dashboard, query and explore clickstream data, and manage metadata. </p>"},{"location":"analytics/#modules","title":"Modules","text":"<p>Below are the modules included in the Analytics Studio\uff1a - Dashboard. View out-of-the-box dashboard and custom dashboards. - Explorations. Advance analytics models for user to uncover insights beyond the out-of-the-box dashboard. - Analyses. Author interface for creating and modifying dashboard, as well as managing datasets. - Data management. View and manage the metadata for clickstream data.  </p>"},{"location":"analytics/#glossary","title":"Glossary","text":"<p>Below are the terms and concepts are used in Analytics Studios, it would be helpful for you to get familiar with those terms when using the Analytics Studio.</p> <ul> <li>Event. Event refers to the clickstream data generated by a user action within an app or website (such as clicking, visiting a page, etc.). </li> <li>Preset event. Events that are automatically collected by clickstream SDK, usually with a name start with an underscore '_';</li> <li>Custom event. Events that are defined and collected by app owner, and the collection timing and business purpose of the event varies by apps.</li> <li>Event parameter. 'Event parameter' are used to describe the various dimensions of information at the time the event occurred. Event parameters are divided into two categories: public parameter, namely the parameters that all events include, such as user information (user_id), device information (app version, device model, etc.); private parameter, namely parameters that are unique to certain events, such as user-customized parameters.</li> <li>Public parameter. The parameters that all events include, such as user information (user_id), device information (app version, device model, etc.)</li> <li>Private parameter. The parameters that are unique to certain events, such as user-customized parameters.</li> <li>User attribute. User attribute is used to record the property for a users. User attributes are divided into two categories: preset attributes, that is, attributes collected by SDK presets, such as '_first_visit_date'; custom attributes, that is, user attributes reported by the user themselves, such as 'email_address'.</li> </ul>"},{"location":"analytics/analyzes/","title":"Analyzes","text":"<p>Analyzes module allows user to create and modify dashboards based on the clickstream datasets in a drag-and-drop approach. It provides greater flexibility for users to create business-specific metrics and visualizations. Use the modules when you want to:</p> <ul> <li>create dashboard that are not provided in preset dashboard or not supported by explorations.</li> <li>make changes to the custom dashboard saved from exploration analysis, such as adding calculation fields to calculate custom metrics, adjust visual types etc.</li> <li>join clickstream data with external datasets, such as adding item master data to enrich clickstream datasets.</li> </ul>"},{"location":"analytics/analyzes/#access-analyzes","title":"Access Analyzes","text":"<p>To access Analyzes, follow below steps:</p> <ol> <li>Go to Clickstream Analytics on AWS Console, in the Navigation Bar, click on \"Analytics Studio\", a new tab will be opened in your browser.</li> <li>In the Analytics Studio page, click the Analyzes icon in the left navigation panel.</li> </ol>"},{"location":"analytics/analyzes/#how-it-works","title":"How it works","text":"<p>Analyzes module is essentially the author interface of QuickSight, in which you have the admin access to all the QuickSight functionalities, e.g., create analysis, add or manage datasets, publish and share dashboards. </p> <p>Note</p> <p>Only the user with <code>Administrator</code> or <code>Analyst</code> role can access this module.</p> <p>The solution automatically added the following datasets for each project and app:</p> Dataset name What it is Event_View_<code>app_name</code>_<code>project_name</code> Event data that includes all public event parameters Event_Parameter_View_<code>app_name</code>_<code>project_name</code> Events data that includes all private event parameters User_Dim_View_<code>app_name</code>_<code>project_name</code> User data that includes all public attributes User_Attr_View_<code>app_name</code>_<code>project_name</code> User data that includes all private(custom) attributes Session_View_<code>app_name</code>_<code>project_name</code> Data contains measures and dimension about session Device_View_<code>app_name</code>_<code>project_name</code> Data contains information about user device Retention_View_<code>app_name</code>_<code>project_name</code> Data provides metrics on total users and returned user for each date Lifecycle_Weekly_View_<code>app_name</code>_<code>project_name</code> User lifecycle metrics for every week Lifecycle_Daily_View_<code>app_name</code>_<code>project_name</code> User lifecycle metrics for every date <p>To create a custom analysis, you can follow below QuickSight documentation to prepare data and create visualization:</p> <ol> <li>Connecting to data</li> <li>Preparing data</li> <li>Visualizing data</li> </ol>"},{"location":"analytics/dashboard/","title":"Dashboard","text":"<p>Clickstream Analytics on AWS solution collects data from your websites and apps to create dashboards that derive insights. You can use dashboards to monitor traffic, investigate data, and understand your users and their activities.</p> <p>Once the data are processed by the data pipeline, the data appears in the QuickSight dashboards. Depends on your pipeline configuration, the time for data to be available in your dashboard varies. For example, if you set the data processing interval to be 1 day, the dashboard will show data at T+1 day (T as reporting date).</p>"},{"location":"analytics/dashboard/#view-dashboards","title":"View dashboards","text":"<p>You can find the dashboards for each application by following below steps:</p> <ol> <li>Go to Clickstream Analytics on AWS Console, in the Navigation Bar, click on \"Analytics Studio\", a new tab will be opened in your browser.</li> <li>In the Analytics Studio page, select the project and app you just created in the drop-down list in the top of the web page.</li> <li>Click the User life cycle - default dashboard</li> </ol>"},{"location":"analytics/dashboard/#default-dashboard","title":"Default Dashboard","text":"<p>The dashboards contains a set of reports covering the user life cycle, helps you understand how people use your website or app, from acquisition to retention.</p> Report name What it is Acquisition Summarizes key metrics about new users, provides detail view of user profile Engagement Summarizes key metrics about user engagements and sessions Activity Summarizes key metrics about events user generates in the app, provide detail view of event attributes Retention Summarizes key metrics about active users and user retentions Device Summarizes key metrics about the devices users are using to access your apps and websites, provides detail view of each device User This report allows you to query and view individual user' attributes and the events the user performed. Crash This report provides metric and information about the crash events in your app."},{"location":"analytics/dashboard/#custom-report","title":"Custom report","text":"<p>When you want to investigate certain pieces of data further, you can write SQL to create views in Redshift or Athena, then add dataset into QuickSight to create visualization. Here is an example, Custom report, to demonstrate how to create a customize report with Redshift. </p>"},{"location":"analytics/dashboard/#create-dashboard","title":"Create dashboard","text":"<p>You can create a custom dashboard to save the result of exploration query. Below are the steps:</p> <ol> <li>Click Create Dashboard button on the top-left.</li> <li>Fill in a name as Dashboard name.</li> <li>Fill in a description for the dashboard.</li> <li>Enter a sheet name, then click on the <code>+</code> button on the right. You can add multiple sheets in one dashboard.</li> <li>You can remove a sheet by click on the <code>X</code> button on the sheet name.</li> <li>Click Create button.</li> <li>After the dashboard was created, you can select the dashboard to save query results.</li> </ol>"},{"location":"analytics/dashboard/acquisition/","title":"Acquisition report","text":"<p>You can use the User acquisition report to get insights into how new users find your website or app for the first time. This report also allows you view the detail user profile.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"analytics/dashboard/acquisition/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Acquisition</code>.</li> </ol>"},{"location":"analytics/dashboard/acquisition/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Acquisition report are created based on the QuickSight dataset of <code>User_Dim_View-&lt;app&gt;-&lt;project&gt;</code>, which connects to the <code>clickstream_user_dim_view_v1</code> view in analytics engine (i.e., Redshift). Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena clickstream-user-dim-view_v1.sql<pre><code>    user_id,\n    _first_visit_date AS first_visit_date,\n    _first_referer AS first_referer,\n    CASE\n      WHEN NULLIF(_first_traffic_source, '') IS NULL THEN '(Direct)'\n      ELSE _first_traffic_source\n    END AS first_traffic_source_source,\n    _first_traffic_medium AS first_traffic_source_medium,\n    _first_traffic_source_type AS first_traffic_source_name,\n    CASE\n      WHEN user_id IS NOT NULL THEN 'Registered'\n      ELSE 'Non-registered'\n    END AS registration_status\n  FROM\n    {{schema}}.user_m_view\n), device_id AS (\n  SELECT\n    user_pseudo_id,\n    LISTAGG(d_id, ' | ') WITHIN GROUP (ORDER BY user_pseudo_id) AS device_id\n  FROM (\n    SELECT\n      user_pseudo_id,\n      d_id::VARCHAR\n    FROM\n      {{schema}}.user_m_view u, u.device_id_list d_id\n  )\n  GROUP BY\n    user_pseudo_id\n)\nSELECT\n  u.*,\n  f.first_visit_install_source,\n  f.first_visit_device_language,\n  f.first_platform,\n  f.first_visit_country,\n  f.first_visit_city,\n  d.device_id\nFROM\n  user_base u\nLEFT JOIN\n  {{schema}}.clickstream_user_first_attr_view_v1 f ON u.user_pseudo_id = f.user_pseudo_id\nLEFT JOIN\n  device_id d ON u.user_pseudo_id = d.user_pseudo_id\n;\n</code></pre> clickstream-user-dim-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n\nwith base as (\n  select \n    *\n  from {{database}}.{{eventTable}}\n  where partition_app = ? \n    and partition_year &gt;= ?\n    and partition_month &gt;= ?\n    and partition_day &gt;= ?\n),\nclickstream_user_dim_mv_1 as (\n  SELECT\n      user_pseudo_id\n    , event_date as first_visit_date\n    , app_info.install_source as first_visit_install_source\n    , device.system_language as first_visit_device_language\n    , platform as first_platform\n    , geo.country as first_visit_country\n    , geo.city as first_visit_city\n    , (case when nullif(traffic_source.source,'') is null then '(direct)' else traffic_source.source end) as first_traffic_source_source\n    , traffic_source.medium as first_traffic_source_medium\n    , traffic_source.name as first_traffic_source_name\n  from base\n  where event_name in ('_first_open','_first_visit')\n),\n\nclickstream_user_dim_mv_2 AS (\n  select user_pseudo_id,\n    count\n    (\n        distinct user_id\n    ) as user_id_count\n  from base ods\n  where event_name not in \n    (\n        '_first_open',\n        '_first_visit'\n    ) group by 1\n)\n\nSELECT upid.*,\n  (\n    case when uid.user_id_count&gt;0 then 'Registered' else 'Non-registered' end\n  ) as is_registered\nfrom clickstream_user_dim_mv_1 as upid left outer join \nclickstream_user_dim_mv_2 as uid on upid.user_pseudo_id=uid.user_pseudo_id\n</code></pre>"},{"location":"analytics/dashboard/acquisition/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>user_id</code> Dimension The user ID set via the setUserId API in SDK Query from analytics engine <code>device_id</code> Dimension The unique ID for the device, please refer to SDK Manual for how the device id was obtained Query from analytics engine <code>first_visit_date</code> Dimension The date that the user first visited your website or first opened the app Query from analytics engine <code>first_visit_install_source</code> Dimension The installation source when user first opened your app. Blank for web Query from analytics engine <code>first_traffic_source_source</code> Dimension The traffic source for the user when first visit the app or web Query from analytics engine <code>first_traffic_source_medium</code> Dimension The traffic medium for the user when first visit the app or web Query from analytics engine <code>first_traffic_source_name</code> Dimension The traffic campaign name for the user when first visit the app or web Query from analytics engine <code>first_visit_device_language</code> Dimension The system language of the device user used when they first opened your app or first visited your website. Query from analytics engine <code>first_visit_device_language</code> Dimension The system language of the device user used when they first opened your app or first visited your website. Query from analytics engine <code>first_platform</code> Dimension The platform when user first visited your website or first opened your app Query from analytics engine <code>first_referer</code> Dimension The referer when user first visited your website Query from analytics engine <code>first_visit_country</code> Dimension The country where user first visited your website or first opened your app. Query from analytics engine <code>first_visit_city</code> Dimension The city where user first visited your website or first opened your app. Query from analytics engine <code>custom_attr_key</code> Dimension The name of the custom attribute key of the user. Query from analytics engine <code>custom_attr_value</code> Dimension The value of the custom attribute key of the user. Query from analytics engine <code>registration_status</code> Dimension If user had registered or not Query from analytics engine <code>Logged-in Rate</code> Metric Number of distinct user_id divide by number of distinct user_pseudo_id Calculated field in QuickSight"},{"location":"analytics/dashboard/acquisition/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"analytics/dashboard/activity/","title":"Activity report","text":"<p>You can use the Activity report to get insights into the activities the users performed when using your websites and apps. This report measures user activity by the events that users triggered, and let you view the detail attributes of the events.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"analytics/dashboard/activity/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Activity</code>.</li> </ol>"},{"location":"analytics/dashboard/activity/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Activity report are created based on the following QuickSight datasets:</p> <ul> <li><code>Events_View-&lt;app id&gt;-&lt;project id&gt;</code> that connects to the <code>clickstream_event_view_v1</code> view in analytics engines (i.e., Redshift)</li> <li><code>Events_Parameter_View-&lt;app id&gt;-&lt;project id&gt;</code> that connects to the <code>clickstream_events_parameter_view_v1</code> view in analytics engines  </li> </ul> <p>Below is the SQL command that generates the related views.</p> SQL Commands RedshiftAthena clickstream_event_view_v1.sql<pre><code>SORTKEY(event_date)\nAUTO REFRESH YES\nAS\nselect \nevent_date\n, event_name\n, event_id\n, event_bundle_sequence_id::bigint as event_bundle_sequence_id\n, event_previous_timestamp::bigint as event_previous_timestamp\n, event_timestamp\n, event_value_in_usd\n, app_info.app_id::varchar as app_info_app_id\n, app_info.id::varchar as app_info_package_id\n, app_info.install_source::varchar as app_info_install_source\n, app_info.version::varchar as app_info_version\n, app_info.sdk_name::varchar as app_info_sdk_name\n, app_info.sdk_version::varchar as app_info_sdk_version\n, device.vendor_id::varchar as device_id\n, device.mobile_brand_name::varchar as device_mobile_brand_name\n, device.mobile_model_name::varchar as device_mobile_model_name\n, device.manufacturer::varchar as device_manufacturer\n, device.screen_width::bigint as device_screen_width\n, device.screen_height::bigint as device_screen_height\n, device.carrier::varchar as device_carrier\n, device.network_type::varchar as device_network_type\n, device.operating_system::varchar as device_operating_system\n, device.operating_system_version::varchar as device_operating_system_version\n, device.host_name::varchar\n, device.ua_browser::varchar \n, device.ua_browser_version::varchar\n, device.ua_os::varchar\n, device.ua_os_version::varchar\n, device.ua_device::varchar\n, device.ua_device_category::varchar\n, device.system_language::varchar as device_system_language\n, device.time_zone_offset_seconds::bigint as device_time_zone_offset_seconds\n, geo.continent::varchar as geo_continent\n, geo.country::varchar as geo_country\n, geo.city::varchar as geo_city\n, geo.metro::varchar as geo_metro\n, geo.region::varchar as geo_region\n, geo.sub_continent::varchar as geo_sub_continent\n, geo.locale::varchar as geo_locale\n, platform\n, project_id\n, traffic_source.name::varchar as traffic_source_name\n, traffic_source.medium::varchar as traffic_source_medium\n, traffic_source.source::varchar as traffic_source_source\n, event.user_id\n, event.user_pseudo_id\n, u.user_first_touch_timestamp\nfrom {{schema}}.event\nleft join (\n    select\n      *\n    from (\n    select  \n      user_pseudo_id,\n          user_first_touch_timestamp\n        ,ROW_NUMBER() over (partition by user_pseudo_id ORDER BY event_timestamp desc) AS et_rank\n      from {{schema}}.user\n  )\n    where et_rank = 1\n) as u on event.user_pseudo_id = u.user_pseudo_id\n;\n</code></pre> clickstream-ods-events-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n-- msck repair table {{database}}.{{userTable}};\n\nselect \n   event_date\n  ,event_name\n  ,event_id\n  ,event_bundle_sequence_id as event_bundle_sequence_id\n  ,event_previous_timestamp as event_previous_timestamp\n  ,event_timestamp\n  ,event_value_in_usd\n  ,app_info.app_id as app_info_app_id\n  ,app_info.id as app_info_package_id\n  ,app_info.install_source as app_info_install_source\n  ,app_info.version as app_info_version\n  ,device.vendor_id as device_id\n  ,device.mobile_brand_name as device_mobile_brand_name\n  ,device.mobile_model_name as device_mobile_model_name\n  ,device.manufacturer as device_manufacturer\n  ,device.screen_width as device_screen_width\n  ,device.screen_height as device_screen_height\n  ,device.carrier as device_carrier\n  ,device.network_type as device_network_type\n  ,device.operating_system as device_operating_system\n  ,device.operating_system_version as device_operating_system_version\n  ,device.ua_browser \n  ,device.ua_browser_version\n  ,device.ua_os\n  ,device.ua_os_version\n  ,device.ua_device\n  ,device.ua_device_category\n  ,device.system_language as device_system_language\n  ,device.time_zone_offset_seconds as device_time_zone_offset_seconds\n  ,geo.continent as geo_continent\n  ,geo.country as geo_country\n  ,geo.city as geo_city\n  ,geo.metro as geo_metro\n  ,geo.region as geo_region\n  ,geo.sub_continent as geo_sub_continent\n  ,geo.locale as geo_locale\n  ,platform\n  ,project_id\n  ,traffic_source.name as traffic_source_name\n  ,traffic_source.medium as traffic_source_medium\n  ,traffic_source.source as traffic_source_source\n  ,event.user_id\n  ,event.user_pseudo_id\n  ,u.user_first_touch_timestamp\nfrom {{database}}.{{eventTable}} as event\nleft join (\n    select\n      *\n    from (\n    select  \n      user_pseudo_id,\n          user_first_touch_timestamp\n        ,ROW_NUMBER() over (partition by user_pseudo_id ORDER BY event_timestamp desc) AS et_rank\n      from {{database}}.{{userTable}}\n  )\n    where et_rank = 1\n) as u on event.user_pseudo_id = u.user_pseudo_id\nwhere event.partition_app = ? \n  and event.partition_year &gt;= ?\n  and event.partition_month &gt;= ?\n  and event.partition_day &gt;= ?\n</code></pre>"},{"location":"analytics/dashboard/activity/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>event_id</code> Dimension A SDK-generated unique id for the event user triggered when using your websites and apps Query from analytics engine <code>event_name</code> Dimension The name of the event Query from analytics engine <code>platform</code> Dimension The platform user used during the session Query from analytics engine <code>Event User Type</code> Dimension The type of user performed the event, i.e., new user or existing user Calculated field in QuickSight <code>event_date</code> Metric The date when the event was logged (YYYYMMDD format in UTC). Query from analytics engine <code>event_timestamp</code> Dimension The time (in microseconds, UTC) when the event was logged on the client. Query from analytics engine <code>app_info_version</code> Dimension The version of the app or website when event was logged Query from analytics engine <code>event_parameter_key</code> Dimension The key of the event parameter Query from analytics engine <code>event_parameter_key</code> Dimension The value of the event parameter Query from analytics engine <code>User activity number in last 7 days</code> Metrics Number of events logged in last 7 days Calculated field in QuickSight <code>User activity number in last 30 days</code> Metrics Number of events logged in last 30 days Calculated field in QuickSight <code>Views</code> Metrics Number of events that are <code>_screen_view</code> or <code>_page_view</code> Calculated field in QuickSight <code>Screen Time</code> Metrics Engagement time (in minute) on a screen or web page Calculated field in QuickSight"},{"location":"analytics/dashboard/activity/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"analytics/dashboard/crash/","title":"Crash report","text":"<p>Crash report provides metric and information about the crash events in your app.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"analytics/dashboard/crash/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Crash</code>.</li> </ol>"},{"location":"analytics/dashboard/crash/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Crash report are created based on the following QuickSight datasets:</p> <ul> <li><code>clickstream_user_dim_view_v1</code> - <code>Events_View-&lt;app id&gt;-&lt;project id&gt;</code> that connects to the <code>clickstream_event_view_v1</code> view in analytics engines (i.e., Redshift)</li> <li><code>Events_Parameter_View-&lt;app id&gt;-&lt;project id&gt;</code> that connects to the <code>clickstream_events_parameter_view_v1</code> view in analytics engines  </li> </ul> SQL Commands RedshiftAthena clickstream_event_view_v1.sql<pre><code>SORTKEY(event_date)\nAUTO REFRESH YES\nAS\nselect \nevent_date\n, event_name\n, event_id\n, event_bundle_sequence_id::bigint as event_bundle_sequence_id\n, event_previous_timestamp::bigint as event_previous_timestamp\n, event_timestamp\n, event_value_in_usd\n, app_info.app_id::varchar as app_info_app_id\n, app_info.id::varchar as app_info_package_id\n, app_info.install_source::varchar as app_info_install_source\n, app_info.version::varchar as app_info_version\n, app_info.sdk_name::varchar as app_info_sdk_name\n, app_info.sdk_version::varchar as app_info_sdk_version\n, device.vendor_id::varchar as device_id\n, device.mobile_brand_name::varchar as device_mobile_brand_name\n, device.mobile_model_name::varchar as device_mobile_model_name\n, device.manufacturer::varchar as device_manufacturer\n, device.screen_width::bigint as device_screen_width\n, device.screen_height::bigint as device_screen_height\n, device.carrier::varchar as device_carrier\n, device.network_type::varchar as device_network_type\n, device.operating_system::varchar as device_operating_system\n, device.operating_system_version::varchar as device_operating_system_version\n, device.host_name::varchar\n, device.ua_browser::varchar \n, device.ua_browser_version::varchar\n, device.ua_os::varchar\n, device.ua_os_version::varchar\n, device.ua_device::varchar\n, device.ua_device_category::varchar\n, device.system_language::varchar as device_system_language\n, device.time_zone_offset_seconds::bigint as device_time_zone_offset_seconds\n, geo.continent::varchar as geo_continent\n, geo.country::varchar as geo_country\n, geo.city::varchar as geo_city\n, geo.metro::varchar as geo_metro\n, geo.region::varchar as geo_region\n, geo.sub_continent::varchar as geo_sub_continent\n, geo.locale::varchar as geo_locale\n, platform\n, project_id\n, traffic_source.name::varchar as traffic_source_name\n, traffic_source.medium::varchar as traffic_source_medium\n, traffic_source.source::varchar as traffic_source_source\n, event.user_id\n, event.user_pseudo_id\n, u.user_first_touch_timestamp\nfrom {{schema}}.event\nleft join (\n    select\n      *\n    from (\n    select  \n      user_pseudo_id,\n          user_first_touch_timestamp\n        ,ROW_NUMBER() over (partition by user_pseudo_id ORDER BY event_timestamp desc) AS et_rank\n      from {{schema}}.user\n  )\n    where et_rank = 1\n) as u on event.user_pseudo_id = u.user_pseudo_id\n;\n</code></pre> clickstream-event-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n-- msck repair table {{database}}.{{userTable}};\n\nselect \n   event_date\n  ,event_name\n  ,event_id\n  ,event_bundle_sequence_id as event_bundle_sequence_id\n  ,event_previous_timestamp as event_previous_timestamp\n  ,event_timestamp\n  ,event_value_in_usd\n  ,app_info.app_id as app_info_app_id\n  ,app_info.id as app_info_package_id\n  ,app_info.install_source as app_info_install_source\n  ,app_info.version as app_info_version\n  ,device.vendor_id as device_id\n  ,device.mobile_brand_name as device_mobile_brand_name\n  ,device.mobile_model_name as device_mobile_model_name\n  ,device.manufacturer as device_manufacturer\n  ,device.screen_width as device_screen_width\n  ,device.screen_height as device_screen_height\n  ,device.carrier as device_carrier\n  ,device.network_type as device_network_type\n  ,device.operating_system as device_operating_system\n  ,device.operating_system_version as device_operating_system_version\n  ,device.ua_browser \n  ,device.ua_browser_version\n  ,device.ua_os\n  ,device.ua_os_version\n  ,device.ua_device\n  ,device.ua_device_category\n  ,device.system_language as device_system_language\n  ,device.time_zone_offset_seconds as device_time_zone_offset_seconds\n  ,geo.continent as geo_continent\n  ,geo.country as geo_country\n  ,geo.city as geo_city\n  ,geo.metro as geo_metro\n  ,geo.region as geo_region\n  ,geo.sub_continent as geo_sub_continent\n  ,geo.locale as geo_locale\n  ,platform\n  ,project_id\n  ,traffic_source.name as traffic_source_name\n  ,traffic_source.medium as traffic_source_medium\n  ,traffic_source.source as traffic_source_source\n  ,event.user_id\n  ,event.user_pseudo_id\n  ,u.user_first_touch_timestamp\nfrom {{database}}.{{eventTable}} as event\nleft join (\n    select\n      *\n    from (\n    select  \n      user_pseudo_id,\n          user_first_touch_timestamp\n        ,ROW_NUMBER() over (partition by user_pseudo_id ORDER BY event_timestamp desc) AS et_rank\n      from {{database}}.{{userTable}}\n  )\n    where et_rank = 1\n) as u on event.user_pseudo_id = u.user_pseudo_id\nwhere event.partition_app = ? \n  and event.partition_year &gt;= ?\n  and event.partition_month &gt;= ?\n  and event.partition_day &gt;= ?\n</code></pre>"},{"location":"analytics/dashboard/crash/#dimensions-and-metrics","title":"Dimensions and Metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating \"calculated fields\" in the QuickSight dataset. Learn more.</p> Field Type What Is It How to Populate <code>user_pseudo_id</code> Dimension Unique ID generated by the user's SDK Query from the analytics engine <code>user_id</code> Dimension User ID set through the setUserId API in the SDK Query from the analytics engine <code>device_id</code> Dimension Unique ID of the device, refer to SDK manual for how to obtain the device ID Query from the analytics engine <code>Event Time (HH:MM:SS)</code> Dimension Time of the event recorded on the client in MMDDYYYY HH:MM:SS format Calculated field in QuickSight <code>event_id</code> Dimension Unique ID generated by the SDK for events triggered by the user on your website and application Query from the analytics engine <code>event_name</code> Dimension Name of the event Query from the analytics engine <code>platform</code> Dimension Platform used by the user during the session Query from the analytics engine <code>Crash Rate (by device)</code> Metric Percentage of crash events by device Calculated field in QuickSight <code>app_info_version</code> Dimension Application version associated with the event Query from the analytics engine <code>geo_locale</code> Dimension Geographic location and region information associated with the event Query from the analytics engine <code>event_parameter_key</code> Dimension Key of the event parameter Query from the analytics engine <code>event_parameter_key</code> Dimension Value of the event parameter Query from the analytics engine <code>event_date</code> Metric Date of recording the event (in UTC format YYYYMMDD) Query from the analytics engine <code>event_timestamp</code> Dimension Time of the event recorded on the client (in microseconds, UTC) Query from the analytics engine <code>app_info_version</code> Dimension Version of the application or website when the event was recorded Query from the analytics engine <code>app_info_package_id</code> Dimension Package ID of the application or website when the event was recorded Query from the analytics engine <code>app_info_sdk_name</code> Dimension SDK name at the time of recording the event Query from the analytics engine <code>app_info_sdk_version</code> Dimension SDK version at the time of recording the event Query from the analytics engine <code>app_info_package_id</code> Dimension Package ID of the application or website when the event was recorded Query from the analytics engine <code>device_mobile_model_name</code> Dimension Model name of the device Query from the analytics engine <code>device_network_type</code> Dimension Network type when the user recorded the event Query from the analytics engine <code>device_operating_system</code> Dimension Operating system of the device Query from the analytics engine <code>device_operating_system_version</code> Dimension Operating system version of the device Query from the analytics engine"},{"location":"analytics/dashboard/crash/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"analytics/dashboard/custom-analysis/","title":"Custom report","text":"<p>One of the key benefits of this solution is that you have 100% control over the clickstream data collected from your apps and websites. You have complete flexibility to analyze the data for your specific business needs.  This article illustrates the steps of creating a custom report with an example of creating funnel analysis by using Redshift Serverless as analytics engine and QuickSight as reporting tools.</p>"},{"location":"analytics/dashboard/custom-analysis/#steps","title":"Steps","text":"<p>Creating a custom report mainly consists of two parts, the first part is to prepare the dataset in your analytics engine, the second part is to create visualization in QuickSight.</p>"},{"location":"analytics/dashboard/custom-analysis/#part-1-dataset-preparation","title":"Part 1 - Dataset preparation","text":"<ol> <li>Open Redshift Serverless dashboard</li> <li>Click the workgroup starting with <code>clickstream-&lt;project-id&gt;</code> created by the solution.</li> <li>Click on the <code>Query data</code> button, you will be directed to the Redshift Query Editor.</li> <li> <p>In the <code>Editor</code> view on the Redshift Query Editor, right click on the workgroup with name of <code>clickstream-&lt;project-id&gt;</code>. In the prompted drop-down, select <code>Edit connection</code>, you will be asked to provide connection parameters. Follow this guide to use an appropriate method to connect.</p> <p>Important</p> <p>You will need read and write permissions for the database (with name as <code>&lt;project-id&gt;</code>) to create custom view or table. For example, you can use Admin user to connect to the cluster or workgroup. If you don't know the password for the Admin user, you can reset the admin password in the Redshift Console (Learn more). </p> </li> <li> <p>If it is the first time you access the query editor, you will be prompted to configure the account, please click Config account button to open query editor.</p> </li> <li>Add a new SQL editor, and make sure you selected the correct workgroup and schema.</li> <li> <p>Create a new view for funnel analysis. In this example, we used below SQL.</p> SQL Commands <pre><code>CREATE OR REPLACE VIEW notepad.clickstream_funnel_view as\nSELECT\nplatform,\nCOUNT(DISTINCT step1_id) AS login_users,\nCOUNT(DISTINCT step2_id) AS add_button_click_users,\nCOUNT(DISTINCT step3_id) AS note_create_users\nFROM (\nSELECT\n    platform,\n    user_pseudo_id AS step1_id,\n    event_timestamp AS step1_timestamp, \n    step2_id,\n    step2_timestamp,\n    step3_id,\n    step3_timestamp\nFROM\n    notepad.ods_events\nLEFT JOIN (\nSELECT\n    user_pseudo_id AS step2_id,\n    event_timestamp AS step2_timestamp\nFROM\n    notepad.ods_events\nWHERE\n    event_name = 'add_button_click' )\nON\n    user_pseudo_id = step2_id\n    AND event_timestamp &lt; step2_timestamp\nLEFT JOIN (\nSELECT\n    user_pseudo_id AS step3_id,\n    event_timestamp AS step3_timestamp\nFROM\n    notepad.ods_events\nWHERE\n    event_name= 'note_create' )\nON\n    step3_id  = step2_id\n    AND step2_timestamp &lt; step3_timestamp\nWHERE\nevent_name = 'user_login' )\ngroup by\nplatform\n</code></pre> </li> <li> <p>Go to QuickSight console, click 'Dataset', and then click 'New dataset'.</p> </li> <li> <p>In the New Dataset page, click Redshift Manual connect to add dataset, fill in the prompted form with the following parameters. </p> <ul> <li>Data source name: <code>clickstream-funnel-view-&lt;project-id&gt;</code></li> <li>Connection type: select <code>VPC connections</code> / <code>VPC Connection for Clickstream pipeline &lt;project-id&gt;</code></li> <li>Database server: input the endpoint url of the serverless workgroup, which you can find on the workgroup console.</li> <li>Port: <code>5439</code></li> <li>Database name: <code>&lt;project-id&gt;</code></li> <li>User name: name of the user you used to created the custom view in previous steps</li> <li>Password: password of the user you used to created the custom view in previous steps</li> </ul> </li> <li>Validated the connection, if ok, click Create data source button.</li> <li> <p>Choose the view from Redshift as data source - \"clickstream_funnel_view\", then</p> <ul> <li>Schema: select <code>notepad</code> </li> <li>Tables: <code>clickstream_funnel_view</code></li> </ul> <p>Tip</p> <p>You will be prompt to select <code>Import to SPICE</code> or <code>Directly query your data</code>, please select <code>Directly query your data</code>.</p> <ul> <li>Click Edit/Preview data to preview the data, once you're familiar with the data, click  PUBLISH &amp; VISUALIZE at the top-right.</li> </ul> </li> </ol>"},{"location":"analytics/dashboard/custom-analysis/#part-2-create-visualization-in-quicksight","title":"Part 2 - Create visualization in QuickSight","text":"<ol> <li>You will be prompt to select a layout for your visualization, select one per your need.</li> <li>Click \"+Add\" at the top-left of the screen then click \"Add visual\" button.</li> <li>Select a Visual type at the bottom-left of the screen, in this example, select Vertical bar chart</li> <li>In the Field wells, select <code>platform</code> as X axis, <code>login_user</code>, <code>add_button_click_users</code>, and <code>note_create_users</code> as Value.</li> <li>You now can publish this analysis as dashboard or continue to format it. Learn more about QuickSight visualization in this link</li> </ol>"},{"location":"analytics/dashboard/device/","title":"Device report","text":"<p>You can use the Device report to get insights into the devices that users used when using your apps or websites. The report provides more information for your user profile.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"analytics/dashboard/device/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Device</code>.</li> </ol>"},{"location":"analytics/dashboard/device/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Device report are created based on the following QuickSight dataset:</p> <ul> <li><code>Device_View-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_device_view_v1</code> view in analytics engines (i.e., Redshift or Athena). </li> </ul> <p>Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena clickstream_device_view_v1.sql<pre><code>select\ndevice.vendor_id::varchar as device_id\n, event_date \n, device.mobile_brand_name::varchar\n, device.mobile_model_name::varchar\n, device.manufacturer::varchar\n, device.screen_width::int\n, device.screen_height::int\n, device.carrier::varchar\n, device.network_type::varchar\n, device.operating_system::varchar\n, device.operating_system_version::varchar\n, device.ua_browser::varchar\n, device.ua_browser_version::varchar\n, device.ua_os::varchar\n, device.ua_os_version::varchar\n, device.ua_device::varchar\n, device.ua_device_category::varchar\n, device.system_language::varchar\n, device.time_zone_offset_seconds::int\n, device.advertising_id::varchar\n, device.host_name::varchar\n, user_pseudo_id\n, user_id\n, count(event_id) as usage_num\n--please update the following schema name with your schema name\nfrom {{schema}}.event \ngroup by\ndevice_id\n, event_date\n, device.mobile_brand_name\n, device.mobile_model_name\n, device.manufacturer\n, device.screen_width\n, device.screen_height\n, device.carrier\n, device.network_type\n, device.operating_system\n, device.operating_system_version\n, device.ua_browser\n, device.ua_browser_version\n, device.ua_os \n, device.ua_os_version\n, device.ua_device\n, device.ua_device_category\n, device.system_language\n, device.time_zone_offset_seconds\n, device.advertising_id\n, device.host_name\n, user_pseudo_id\n, user_id;\n</code></pre> clickstream-device-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n\nselect\n   device.vendor_id as device_id\n  ,event_date \n  ,device.mobile_brand_name\n  ,device.mobile_model_name\n  ,device.manufacturer\n  ,device.screen_width\n  ,device.screen_height\n  ,device.carrier\n  ,device.network_type\n  ,device.operating_system\n  ,device.operating_system_version\n  ,device.ua_browser\n  ,device.ua_browser_version\n  ,device.ua_os\n  ,device.ua_os_version\n  ,device.ua_device\n  ,device.ua_device_category\n  ,device.system_language\n  ,device.time_zone_offset_seconds\n  ,device.advertising_id\n  ,device.host_name\n  ,user_pseudo_id\n  ,user_id\n  ,count(event_id) as usage_num\nfrom {{database}}.{{eventTable}} \nwhere partition_app = ? \n  and partition_year &gt;= ?\n  and partition_month &gt;= ?\n  and partition_day &gt;= ?\ngroup by\n  device.vendor_id\n  ,event_date\n  ,device.mobile_brand_name\n  ,device.mobile_model_name\n  ,device.manufacturer\n  ,device.screen_width\n  ,device.screen_height\n  ,device.carrier\n  ,device.network_type\n  ,device.operating_system\n  ,device.operating_system_version\n  ,device.ua_browser\n  ,device.ua_browser_version\n  ,device.ua_os \n  ,device.ua_os_version\n  ,device.ua_device\n  ,device.ua_device_category\n  ,device.system_language\n  ,device.time_zone_offset_seconds\n  ,device.advertising_id\n  ,device.host_name\n  ,user_pseudo_id\n  ,user_id\n</code></pre>"},{"location":"analytics/dashboard/device/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>device_id</code> Dimension The unique ID for the device, please refer to SDK Manual for how the device id was obtained Query from analytics engine <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>user_id</code> Dimension The user ID set via the setUserId API in SDK Query from analytics engine <code>event_date</code> Dimension The event data of when the device information was logged Query from analytics engine <code>mobile_brand_name</code> Dimension The brand name for the device Query from analytics engine <code>mobile_model_name</code> Dimension The model name for the device Query from analytics engine <code>manufacturer</code> Dimension The manufacturer for the device Query from analytics engine <code>network_type</code> Dimension The network type when user logged the events Query from analytics engine <code>operating_system</code> Dimension The operating system of the device Query from analytics engine <code>operating_system_version</code> Dimension The operating system version of the device Query from analytics engine <code>screen_height</code> Dimension The screen height of the device Query from analytics engine <code>screen_width</code> Dimension The screen width of the device Query from analytics engine <code>Screen Resolution</code> Dimension The screen resolution (i.e., screen height x screen width) of the device Calculated field in QuickSight <code>system_language</code> Dimension The system language of the solution Query from analytics engine <code>us_browser</code> Dimension The browser derived from user agent Query from analytics engine <code>us_browser_version</code> Dimension The browser version derived from user agent Query from analytics engine <code>us_os</code> Dimension The operating system derived from user agent Query from analytics engine <code>us_device</code> Dimension The device derived from user agent Query from analytics engine <code>us_device_category</code> Dimension The device category derived from user agent Query from analytics engine <code>usage_num</code> Metric Number of event that logged for the device ID Query from analytics engine"},{"location":"analytics/dashboard/device/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"analytics/dashboard/engagement/","title":"Engagement report","text":"<p>You can use the Engagement report to get insights into the engagement level of the users when using your websites and apps.  This report measures user engagement by the sessions that users trigger and the web pages and app screens that users visit.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"analytics/dashboard/engagement/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard.</li> <li>In the dashboard, click on the sheet with name of <code>Engagement</code>.</li> </ol>"},{"location":"analytics/dashboard/engagement/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Engagement report are created based on the QuickSight dataset of <code>Session_View-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_session_view_v1</code> view in analytics engines (i.e., Redshift or Athena). Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena clickstream-session-view.sql<pre><code>  SELECT \n    event.event_id,\n    event.event_name,\n    event.event_date,\n    event.platform,\n    event.user_id,\n    event.user_pseudo_id,\n    event.event_timestamp,\n    event_parameter.event_param_key,\n    event_parameter.event_param_double_value,\n    event_parameter.event_param_float_value,\n    event_parameter.event_param_int_value,\n    event_parameter.event_param_string_value\n  FROM {{schema}}.event\n  JOIN {{schema}}.event_parameter ON event.event_timestamp = event_parameter.event_timestamp AND event.event_id = event_parameter.event_id\n),\n\nsession_part_1 AS (\n  SELECT \n    es.session_id::VARCHAR,\n    user_pseudo_id,\n    platform,\n    MAX(session_duration) AS session_duration,\n    (CASE WHEN (MAX(session_duration) &gt;= 10000 OR SUM(view) &gt;= 1) THEN 1 ELSE 0 END) AS engaged_session,\n    (CASE WHEN (MAX(session_duration) &gt;= 10000 OR SUM(view) &gt;= 1) THEN 0 ELSE 1 END) AS bounced_session,\n    MIN(session_st) AS session_start_timestamp,\n    SUM(view) AS session_views,\n    SUM(engagement_time) AS session_engagement_time\n  FROM\n  (\n    SELECT \n      user_pseudo_id,\n      event_id,\n      platform,\n      MAX(CASE WHEN event_param_key = '_session_id' THEN event_param_string_value ELSE NULL END) AS session_id,\n      MAX(CASE WHEN event_param_key = '_session_duration' THEN event_param_int_value ELSE NULL END) AS session_duration,\n      MAX(CASE WHEN event_param_key = '_session_start_timestamp' THEN event_param_int_value ELSE NULL END) AS session_st,\n      MAX(CASE WHEN (event_param_key = '_engagement_time_msec' AND event_name = '_user_engagement') THEN event_param_int_value ELSE NULL END) AS engagement_time,\n      (CASE WHEN MAX(event_name) IN ('_screen_view', '_page_view') THEN 1 ELSE 0 END) AS view\n    FROM base_data\n    GROUP BY 1,2,3\n  ) AS es\n  GROUP BY 1,2,3\n),\n\nsession_part_2 AS (\n  SELECT session_id, first_sv_event_id, last_sv_event_id, COUNT(event_id) FROM (\n    SELECT \n      session_id::VARCHAR,\n      event_id,\n      FIRST_VALUE(event_id) OVER(PARTITION BY session_id ORDER BY event_timestamp ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS first_sv_event_id,\n      LAST_VALUE(event_id) OVER(PARTITION BY session_id ORDER BY event_timestamp ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_sv_event_id\n    FROM (\n      SELECT \n        event_name,\n        event_id,\n        event_timestamp,\n        MAX(CASE WHEN event_param_key = '_session_id' THEN event_param_string_value ELSE NULL END) AS session_id\n      FROM base_data WHERE event_name IN ('_screen_view','_page_view')\n      GROUP BY 1,2,3\n    ) \n  )\n  GROUP BY 1,2,3\n),\n\ntmp_data AS (\n  SELECT event_id, MAX(\n    CASE \n      WHEN (event_param_key = '_screen_name' OR event_param_key = '_page_title') THEN event_param_string_value \n      ELSE NULL \n    END) AS view\n  FROM base_data\n  GROUP BY 1\n),\n\nsession_f_sv_view AS (\n  SELECT session_f_l_sv.*, t.view AS first_sv_view\n  FROM session_part_2 AS session_f_l_sv \n  LEFT OUTER JOIN tmp_data t ON session_f_l_sv.first_sv_event_id = t.event_id\n), \n\nsession_f_l_sv_view AS (\n  SELECT session_f_sv_view.*, t.view AS last_sv_view\n  FROM session_f_sv_view \n  LEFT OUTER JOIN tmp_data t ON session_f_sv_view.last_sv_event_id = t.event_id\n)\n\nSELECT \n  CASE\n    WHEN session.session_id IS NULL THEN CAST('#' AS VARCHAR)\n    WHEN session.session_id = '' THEN CAST('#' AS VARCHAR)\n    ELSE session.session_id \n  END AS session_id,\n  user_pseudo_id,\n  platform,\n  session_duration::BIGINT,\n  session_views::BIGINT,\n  engaged_session::BIGINT,\n  bounced_session,\n  session_start_timestamp,\n  CASE\n    WHEN session.session_engagement_time IS NULL THEN CAST(0 AS BIGINT)\n    ELSE session.session_engagement_time \n  END::BIGINT AS session_engagement_time,\n  DATE_TRUNC('day', TIMESTAMP 'epoch' + session_start_timestamp/1000 * INTERVAL '1 second') AS session_date,\n  DATE_TRUNC('hour', TIMESTAMP 'epoch' + session_start_timestamp/1000 * INTERVAL '1 second') AS session_date_hour,\n  first_sv_view::VARCHAR AS entry_view,\n  last_sv_view::VARCHAR AS exit_view\nFROM session_part_1 AS session \nLEFT OUTER JOIN session_f_l_sv_view ON session.session_id = session_f_l_sv_view.session_id\n;\n</code></pre> clickstream-session-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n-- msck repair table {{database}}.{{eventParamTable}};\n\nwith temp_1 as (\n  select \n    event.event_id,\n    event.event_name,\n    event.event_date,\n    event.platform,\n    event.user_id,\n    event.user_pseudo_id,\n    event.event_timestamp,\n    event_parameter.event_param_key,\n    event_parameter.event_param_double_value,\n    event_parameter.event_param_float_value,\n    event_parameter.event_param_int_value,\n    event_parameter.event_param_string_value\n  from {{database}}.{{eventTable}} as event\n  join {{database}}.{{eventParamTable}} as event_parameter \n  on event.event_timestamp = event_parameter.event_timestamp and event.event_id = event_parameter.event_id\n  where event.partition_app = ? \n  and event.partition_year &gt;= ?\n  and event.partition_month &gt;= ?\n  and event.partition_day &gt;= ?\n),\ntemp_2 as \n(\n  SELECT \n     user_pseudo_id\n    ,event_id\n    ,platform\n    ,max(case when event_param_key = '_session_id' then event_param_string_value else null end) as session_id\n    ,max(case when event_param_key = '_session_duration' then event_param_int_value else null end) as session_duration\n    ,max(case when event_param_key = '_session_start_timestamp' then event_param_int_value else null end) as session_st\n    ,max(case when event_param_key = '_engagement_time_msec' then event_param_int_value else null end) as engagement_time\n    ,(case when max(event_name) in ('_screen_view', '_page_view') then 1 else 0 end) as view\n  FROM temp_1\n  group by 1,2,3\n),\ntemp_3 as (\n  select \n     event_name\n    ,event_id\n    ,event_timestamp\n    ,max(case when event_param_key = '_session_id' then event_param_string_value else null end) as session_id\n    ,(case when max(event_name) in ('_screen_view', '_page_view') then 1 else 0 end) as view\n    from temp_1 where event_name in ('_screen_view','_page_view')\n    group by 1,2,3\n),\nsession_part_1 as (\n  SELECT\n     session_id \n    ,user_pseudo_id\n    ,platform\n    ,max(session_duration) as session_duration\n    ,(case when (max(session_duration)&gt;10000 or sum(view) &gt;1) then 1 else 0 end) as engaged_session\n    ,(case when (max(session_duration)&gt;10000 or sum(view) &gt;1) then 0 else 1 end) as bounced_session\n    ,min(session_st) as session_start_timestamp\n    ,sum(view) as session_views\n    ,sum(engagement_time) as session_engagement_time\n  FROM temp_2\n  GROUP BY 1,2,3\n),\nsession_part_2 as (\n  select session_id, first_sv_event_id, last_sv_event_id, count(event_id) from (\n    select \n      session_id\n      ,event_id\n      ,first_value(event_id) over(partition by session_id order by event_timestamp asc rows between unbounded preceding and unbounded following) as first_sv_event_id,\n      last_value(event_id) over(partition by session_id order by event_timestamp asc rows between unbounded preceding and unbounded following) as last_sv_event_id\n    from temp_3 \n  ) group by 1,2,3\n  ),\n  session_f_sv_view as (\n    select \n      session_f_l_sv.*,\n      t.view as first_sv_view\n    from session_part_2 as session_f_l_sv left outer join\n    temp_3 as t on session_f_l_sv.first_sv_event_id=t.event_id\n), \nsession_f_l_sv_view as (\n    select \n      session_f_sv_view.*,\n      t.view as last_sv_view\n    from session_f_sv_view left outer join\n    temp_3 as t on session_f_sv_view.last_sv_event_id=t.event_id\n)\nselect \n    CASE\n      WHEN session.session_id IS NULL THEN CAST('#' AS VARCHAR)\n      WHEN session.session_id = '' THEN CAST('#' AS VARCHAR)\n      ELSE session.session_id \n    END AS session_id\n    ,user_pseudo_id\n    ,platform\n    ,cast(session_duration as bigint) as session_duration\n    ,cast(session_views as bigint) as session_duration\n    ,engaged_session\n    ,bounced_session\n    ,session_start_timestamp\n    ,session_engagement_time\n    ,CASE\n       WHEN session.session_engagement_time IS NULL THEN CAST(0 AS BIGINT)\n       ELSE session.session_engagement_time \n     END AS session_engagement_time\n    ,DATE_TRUNC('day', from_unixtime(session_start_timestamp/1000)) as session_date\n    ,DATE_TRUNC('hour', from_unixtime(session_start_timestamp/1000)) as session_date_hour\n    ,first_sv_view as entry_view\n    ,last_sv_view as exit_view\nfrom session_part_1 as session left outer join \nsession_f_l_sv_view on session.session_id = session_f_l_sv_view.session_id\n</code></pre>"},{"location":"analytics/dashboard/engagement/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>session_id</code> Dimension A SDK-generated unique id for the session user triggered when using your websites and apps Query from analytics engine <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>platform</code> Dimension The platform user used during the session Query from analytics engine <code>session_duration</code> Dimension The length of the session in millisecond Query from analytics engine <code>session_views</code> Metric Number of screen view or page view within the session Query from analytics engine <code>engaged_session</code> Dimension Whether the session is engaged or not. <code>Engaged session is defined as if the session last more than 10 seconds or have two or more screen views page views</code> Query from analytics engine <code>session_start_timestamp</code> Dimension The start timestamp of the session Query from analytics engine <code>session_engagement_time</code> Dimension The total engagement time of the session in millisecond Query from analytics engine <code>entry_view</code> Dimension The screen name or page title of the first screen or page user viewed in the session Query from analytics engine <code>exit_view</code> Dimension The screen name or page title of the last screen or page user viewed in the session Query from analytics engine <code>Average engaged session per user</code> Metric Average number of session per user in the selected time period Calculated field in QuickSight <code>Average engagement time per session</code> Metric Average engagement time per session in the selected time period Calculated field in QuickSight <code>Average engagement time per user</code> Metric Average engagement time per user in the selected time period Calculated field in QuickSight <code>Average screen view per user</code> Metric Average number of screen views per user in the selected time period Calculated field in QuickSight"},{"location":"analytics/dashboard/engagement/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"analytics/dashboard/retention/","title":"Retention report","text":"<p>You can use the Retention report to get insights into how frequently and for how long users engage with your website or mobile app after their first visit. The report helps you understand how well your app is doing in terms of attracting users back after their first visit.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"analytics/dashboard/retention/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Retention</code>.</li> </ol>"},{"location":"analytics/dashboard/retention/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Retention report are created based on the following QuickSight dataset:</p> <ul> <li><code>lifecycle_weekly_view-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_lifecycle_weekly_view_v1</code> view in analytics engines (i.e., Redshift or Athena). </li> <li><code>lifecycle_daily_view-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_lifecycle_daily_view_v1</code> view in analytics engines (i.e., Redshift or Athena). </li> <li><code>retention_view-&lt;app id&gt;-&lt;project id&gt;</code> that connects to the <code>clickstream_retention_view_v1</code> view in analytics engines</li> </ul> <p>Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena <p>clickstream_lifecycle_weekly_view_v1.sql<pre><code>  select \n    user_pseudo_id, \n    DATE_TRUNC('week', dateadd(ms,event_timestamp, '1970-01-01')) as time_period\n  from {{schema}}.event\n  where event_name = '_session_start' group by 1,2 order by 1,2),\n-- detect if lag and lead exists\nlag_lead as (\n  select user_pseudo_id, time_period,\n    lag(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period),\n    lead(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period)\n  from weekly_usage),\n-- calculate lag and lead size\nlag_lead_with_diffs as (\n  select user_pseudo_id, time_period, lag, lead, \n    datediff(week,lag,time_period) lag_size,\n    datediff(week,time_period,lead) lead_size\n  from lag_lead),\n-- case to lifecycle stage\ncalculated as (select time_period,\n  case when lag is null then '1-NEW'\n     when lag_size = 1 then '2-ACTIVE'\n     when lag_size &gt; 1 then '3-RETURN'\n  end as this_week_value,\n\n  case when (lead_size &gt; 1 OR lead_size IS NULL) then '0-CHURN'\n     else NULL\n  end as next_week_churn,\n  count(distinct user_pseudo_id)\n   from lag_lead_with_diffs\n  group by 1,2,3)\nselect time_period, this_week_value, sum(count) \n  from calculated group by 1,2\nunion\nselect time_period+7, '0-CHURN', -1*sum(count) \n  from calculated where next_week_churn is not null group by 1,2;\n</code></pre> clickstream_lifecycle_daily_view_v1.sql<pre><code>  select \n    user_pseudo_id, \n    DATE_TRUNC('day', dateadd(ms,event_timestamp, '1970-01-01')) as time_period\n  from {{schema}}.event\n  where event_name = '_session_start' group by 1,2 order by 1,2),\n-- detect if lag and lead exists\nlag_lead as (\n  select user_pseudo_id, time_period,\n    lag(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period),\n    lead(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period)\n  from daily_usage),\n-- calculate lag and lead size\nlag_lead_with_diffs as (\n  select user_pseudo_id, time_period, lag, lead, \n    datediff(day,lag,time_period) lag_size,\n    datediff(day,time_period,lead) lead_size\n  from lag_lead),\n-- case to lifecycle stage\ncalculated as (select time_period,\n  case when lag is null then '1-NEW'\n     when lag_size = 1 then '2-ACTIVE'\n     when lag_size &gt; 1 then '3-RETURN'\n  end as this_day_value,\n\n  case when (lead_size &gt; 1 OR lead_size IS NULL) then '0-CHURN'\n     else NULL\n  end as next_day_churn,\n  count(distinct user_pseudo_id)\n   from lag_lead_with_diffs\n  group by 1,2,3)\nselect time_period, this_day_value, sum(count) \n  from calculated group by 1,2\nunion\nselect time_period+1, '0-CHURN', -1*sum(count) \n  from calculated where next_day_churn is not null group by 1,2;\n</code></pre> clickstream_retention_view_v1.sql<pre><code>WITH user_first_date AS (\n  SELECT\n    user_pseudo_id,\n    min(event_date) as first_date\n  FROM {{schema}}.event\n  GROUP BY user_pseudo_id\n),\n\nretention_data AS (\nSELECT\n    user_pseudo_id,\n    first_date,\n    DATE_DIFF('day', first_date, event_date) AS day_diff\n  FROM {{schema}}.event\n  JOIN user_first_date USING (user_pseudo_id)\n),\n\nretention_counts AS (\n  SELECT\n    first_date,\n    day_diff,\n    COUNT(DISTINCT user_pseudo_id) AS returned_user_count\n  FROM retention_data\n  WHERE day_diff &lt;= 42 -- Calculate retention rate for the last 42 days\n  GROUP BY first_date, day_diff\n),\n\ntotal_users AS (\n  SELECT\n    first_date,\n    COUNT(DISTINCT user_pseudo_id) AS total_users\n  FROM user_first_date\n  group by 1\n),\n\nretention_rate AS (\n  SELECT\n    first_date,\n    day_diff,\n    returned_user_count,\n    total_users\n  FROM retention_counts join total_users using(first_date)\n)\n\nSELECT\n*\nFROM retention_rate;\n</code></pre></p> <p>clickstream-lifecycle-weekly-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n\nwith weekly_usage as (\n  select \n    user_pseudo_id, \n    DATE_TRUNC('week', event_date) as time_period\n  from {{database}}.{{eventTable}}\n  where partition_app = ? \n    and partition_year &gt;= ?\n    and partition_month &gt;= ?\n    and partition_day &gt;= ?\n    and event_name = '_session_start' group by 1,2 order by 1,2\n),\nlag_lead as (\n  select user_pseudo_id, time_period,\n    lag(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period) as lag,\n    lead(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period) as lead\n  from weekly_usage\n),\nlag_lead_with_diffs as (\n  select user_pseudo_id, time_period, lag, lead, \n    date_diff('week',lag,time_period) lag_size,\n    date_diff('week',time_period,lead) lead_size\n  from lag_lead\n),\ncalculated as (\n  select time_period,\n    case when lag is null then '1-NEW'\n      when lag_size = 1 then '2-ACTIVE'\n      when lag_size &gt; 1 then '3-RETURN'\n    end as this_week_value,\n    case when (lead_size &gt; 1 OR lead_size IS NULL) then '0-CHURN'\n      else NULL\n    end as next_week_churn,\n    count(distinct user_pseudo_id) as cnt\n  from lag_lead_with_diffs\n  group by 1,2,3\n)\nselect time_period, this_week_value, sum(cnt) as cnt from calculated group by 1,2\nunion\nselect date_add('day', 7, time_period), '0-CHURN', -1*sum(cnt) as cnt\n  from calculated \nwhere next_week_churn is not null \ngroup by 1,2\n</code></pre> clickstream-lifecycle-daily-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n\nwith daily_usage as (\n  select \n    user_pseudo_id, \n    DATE_TRUNC('day', event_date) as time_period\n  from {{database}}.{{eventTable}} \n  where partition_app = ? \n    and partition_year &gt;= ?\n    and partition_month &gt;= ?\n    and partition_day &gt;= ?\n    and event_name = '_session_start' group by 1,2 order by 1,2\n),\nlag_lead as (\n  select user_pseudo_id, time_period,\n    lag(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period) as lag,\n    lead(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period) as lead\n  from daily_usage\n),\nlag_lead_with_diffs as (\n  select user_pseudo_id, time_period, lag, lead, \n    date_diff('day',lag,time_period) lag_size,\n    date_diff('day',time_period,lead) lead_size\n  from lag_lead\n),\ncalculated as (\n  select time_period,\n    case when lag is null then '1-NEW'\n      when lag_size = 1 then '2-ACTIVE'\n      when lag_size &gt; 1 then '3-RETURN'\n    end as this_day_value,\n\n    case when (lead_size &gt; 1 OR lead_size IS NULL) then '0-CHURN'\n      else NULL\n    end as next_day_churn,\n    count(distinct user_pseudo_id) as cnt\n  from lag_lead_with_diffs\n  group by 1,2,3\n)\nselect time_period, this_day_value, sum(cnt) as cnt\n  from calculated group by 1,2\nunion\nselect date_add('day', 1, time_period) as time_period, '0-CHURN', -1*sum(cnt) as cnt \n  from calculated \n  where next_day_churn is not null \n  group by 1,2;\n</code></pre> clickstream-lifecycle-daily-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n\nwith base as (\n  select \n    *\n  from {{database}}.{{eventTable}}\n  where partition_app = ? \n    and partition_year &gt;= ?\n    and partition_month &gt;= ?\n    and partition_day &gt;= ?\n),\nuser_first_date AS (\n  SELECT\n    user_pseudo_id,\n    min(event_date) as first_date\n  FROM base\n  GROUP BY user_pseudo_id\n),\n\nretention_data AS (\nSELECT\n    user_pseudo_id,\n    first_date,\n    DATE_DIFF('day', first_date, event_date) AS day_diff\n  FROM base\n  JOIN user_first_date USING (user_pseudo_id)\n),\n\nretention_counts AS (\n  SELECT\n    first_date,\n    day_diff,\n    COUNT(DISTINCT user_pseudo_id) AS returned_user_count\n  FROM retention_data\n  WHERE day_diff &lt;= 42 -- Calculate retention rate for the last 42 days\n  GROUP BY first_date, day_diff\n),\n\ntotal_users AS (\n  SELECT\n    first_date,\n    COUNT(DISTINCT user_pseudo_id) AS total_users\n  FROM user_first_date\n  group by 1\n),\n\nretention_rate AS (\n  SELECT\n    first_date,\n    day_diff,\n    returned_user_count,\n    total_users\n  FROM retention_counts join total_users using(first_date)\n)\n\nSELECT\n*\nFROM retention_rate\n</code></pre></p>"},{"location":"analytics/dashboard/retention/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>Daily Active User (DAU)</code> Metric Number of active users per date QuickSight aggregation <code>Weekly Active User (WAU)</code> Metric Number of active users in last 7 days Calculated field in QuickSight <code>Monthly Active User (MAU)</code> Metric Number of active users in last 30 days Calculated field in QuickSight <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>user_id</code> Dimension The user ID set via the setUserId API in SDK Query from analytics engine <code>DAU/WAU</code> Metric DAU/WAU % for user stickiness Calculated field in QuickSight <code>WAU/MAU</code> Metric WAU/MAU % for user stickiness Calculated field in QuickSight <code>DAU/MAU</code> Metric DAU/MAU % for user stickiness Calculated field in QuickSight <code>Event User Type</code> Dimension The type of user performed the event, i.e., new user or existing user Calculated field in QuickSight <code>User first touch date</code> Metric The first date that a user use your websites or apps Calculated field in QuickSight <code>Retention rate</code> Metric Distinct active users number / Distinct active user number by User first touch date Calculated field in QuickSight <code>time_period</code> Dimension The week or day for the user lifecycle Query from analytics engine <code>this_week_value</code> Dimension The user lifecycle stage, i.e., New, Active, Return, and Churn Query from analytics engine <code>this_day_value</code> Dimension The user lifecycle stage, i.e., New, Active, Return, and Churn Query from analytics engine"},{"location":"analytics/dashboard/retention/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"analytics/dashboard/user/","title":"user report","text":"<p>You can use the User report to query and view individual user' attributes and the events the user performed.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"analytics/dashboard/user/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>User</code>.</li> </ol>"},{"location":"analytics/dashboard/user/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>User report are created based on the QuickSight dataset of:</p> <ul> <li><code>User_Dim_View-&lt;app&gt;-&lt;project&gt;</code> which connects to the <code>clickstream_user_dim_view_v1</code> view in analytics engine (i.e., Redshift)</li> <li><code>User_Attr_View-&lt;app&gt;-&lt;project&gt;</code> which connects to <code>clickstream_user_attr_view_v1</code>. Below is the SQL command that generates the view.</li> </ul> SQL Commands RedshiftAthena clickstream_user_dim_view_v1.sql<pre><code>    user_id,\n    _first_visit_date AS first_visit_date,\n    _first_referer AS first_referer,\n    CASE\n      WHEN NULLIF(_first_traffic_source, '') IS NULL THEN '(Direct)'\n      ELSE _first_traffic_source\n    END AS first_traffic_source_source,\n    _first_traffic_medium AS first_traffic_source_medium,\n    _first_traffic_source_type AS first_traffic_source_name,\n    CASE\n      WHEN user_id IS NOT NULL THEN 'Registered'\n      ELSE 'Non-registered'\n    END AS registration_status\n  FROM\n    {{schema}}.user_m_view\n), device_id AS (\n  SELECT\n    user_pseudo_id,\n    LISTAGG(d_id, ' | ') WITHIN GROUP (ORDER BY user_pseudo_id) AS device_id\n  FROM (\n    SELECT\n      user_pseudo_id,\n      d_id::VARCHAR\n    FROM\n      {{schema}}.user_m_view u, u.device_id_list d_id\n  )\n  GROUP BY\n    user_pseudo_id\n)\nSELECT\n  u.*,\n  f.first_visit_install_source,\n  f.first_visit_device_language,\n  f.first_platform,\n  f.first_visit_country,\n  f.first_visit_city,\n  d.device_id\nFROM\n  user_base u\nLEFT JOIN\n  {{schema}}.clickstream_user_first_attr_view_v1 f ON u.user_pseudo_id = f.user_pseudo_id\nLEFT JOIN\n  device_id d ON u.user_pseudo_id = d.user_pseudo_id\n;\n</code></pre> clickstream-user-dim-query.sql<pre><code>-- run following command to load latest partition\n-- msck repair table {{database}}.{{eventTable}};\n\nwith base as (\n  select \n    *\n  from {{database}}.{{eventTable}}\n  where partition_app = ? \n    and partition_year &gt;= ?\n    and partition_month &gt;= ?\n    and partition_day &gt;= ?\n),\nclickstream_user_dim_mv_1 as (\n  SELECT\n      user_pseudo_id\n    , event_date as first_visit_date\n    , app_info.install_source as first_visit_install_source\n    , device.system_language as first_visit_device_language\n    , platform as first_platform\n    , geo.country as first_visit_country\n    , geo.city as first_visit_city\n    , (case when nullif(traffic_source.source,'') is null then '(direct)' else traffic_source.source end) as first_traffic_source_source\n    , traffic_source.medium as first_traffic_source_medium\n    , traffic_source.name as first_traffic_source_name\n  from base\n  where event_name in ('_first_open','_first_visit')\n),\n\nclickstream_user_dim_mv_2 AS (\n  select user_pseudo_id,\n    count\n    (\n        distinct user_id\n    ) as user_id_count\n  from base ods\n  where event_name not in \n    (\n        '_first_open',\n        '_first_visit'\n    ) group by 1\n)\n\nSELECT upid.*,\n  (\n    case when uid.user_id_count&gt;0 then 'Registered' else 'Non-registered' end\n  ) as is_registered\nfrom clickstream_user_dim_mv_1 as upid left outer join \nclickstream_user_dim_mv_2 as uid on upid.user_pseudo_id=uid.user_pseudo_id\n</code></pre>"},{"location":"analytics/dashboard/user/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>user_id</code> Dimension The user ID set via the setUserId API in SDK Query from analytics engine <code>device_id</code> Dimension The unique ID for the device, please refer to SDK Manual for how the device id was obtained Query from analytics engine <code>first_visit_date</code> Dimension The date that the user first visited your website or first opened the app Query from analytics engine <code>first_visit_install_source</code> Dimension The installation source when user first opened your app. Blank for web Query from analytics engine <code>first_traffic_source_source</code> Dimension The traffic source for the user when first visit the app or web Query from analytics engine <code>first_traffic_source_medium</code> Dimension The traffic medium for the user when first visit the app or web Query from analytics engine <code>first_traffic_source_name</code> Dimension The traffic campaign name for the user when first visit the app or web Query from analytics engine <code>first_visit_device_language</code> Dimension The system language of the device user used when they first opened your app or first visited your website. Query from analytics engine <code>first_visit_device_language</code> Dimension The system language of the device user used when they first opened your app or first visited your website. Query from analytics engine <code>first_platform</code> Dimension The platform when user first visited your website or first opened your app Query from analytics engine <code>first_referer</code> Dimension The referer when user first visited your website Query from analytics engine <code>first_visit_country</code> Dimension The country where user first visited your website or first opened your app. Query from analytics engine <code>first_visit_city</code> Dimension The city where user first visited your website or first opened your app. Query from analytics engine <code>custom_attr_key</code> Dimension The name of the custom attribute key of the user. Query from analytics engine <code>custom_attr_value</code> Dimension The value of the custom attribute key of the user. Query from analytics engine <code>registration_status</code> Dimension If user had registered or not Query from analytics engine <code>Event Time (HH:MM:SS)</code> Dimension The time in MMDDYYYY HH:MM:SS format when the event was recorded in the client Calculated field in QuickSight <code>event_id</code> Dimension A SDK-generated unique id for the event user triggered when using your websites and apps Query from analytics engine <code>event_name</code> Dimension The name of the event Query from analytics engine <code>platform</code> Dimension The platform user used during the session Query from analytics engine <code>event_value_in_usd</code> Metric The value in USD associated with the event Query from analytics engine <code>app_info_version</code> Dimension The app version associated with the event Query from analytics engine <code>geo_locale</code> Dimension The geo and locale information associated with the event Query from analytics engine <code>event_parameter_key</code> Dimension The key of the event parameter Query from analytics engine <code>event_parameter_key</code> Dimension The value of the event parameter Query from analytics engine <code>event_date</code> Metric The date when the event was logged (YYYYMMDD format in UTC). Query from analytics engine <code>event_timestamp</code> Dimension The time (in microseconds, UTC) when the event was logged on the client. Query from analytics engine <code>app_info_version</code> Dimension The version of the app or website when event was logged Query from analytics engine"},{"location":"analytics/dashboard/user/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"analytics/data-mgmt/","title":"Data management","text":"<p>The Data Management module automatically scans the data in your clickstream to generate metadata, making it convenient for you to view and manage all events, event properties, and user properties. You can also modify the display names, descriptions, and data dictionaries of events and properties, making it easy for you to use them in the Exploration Analysis module.</p>"},{"location":"analytics/data-mgmt/#accessing-data-management","title":"Accessing Data Management","text":"<p>To access Explorations, follow below steps:</p> <ol> <li>Go to Clickstream Analytics on AWS Console, in the Navigation Bar, click on \"Analytics Studio\", a new tab will be opened in your browser.</li> <li>In the Analytics Studio page, click the Data Management icon in the left navigation panel.</li> </ol> <p>Note</p> <p>Only the user with <code>Administrator</code> or <code>Analyst</code> role can modify the metadata, such as display name, description.</p>"},{"location":"analytics/data-mgmt/#how-it-works","title":"How it works","text":"<p>The solution automatically scans clickstream data to generate metadata and then stored them in Redshift on a daily basis. There are three type of metadata:</p> <ol> <li>Event: metadata describes clickstream events, which are stored in <code>event_metadata</code> table in Redshift.</li> <li>Event Parameter: metadata describes clickstream event parameters, which are stored in <code>event_parameter_metadata</code> table in Redshift.</li> <li>User Attribute: metadata describes user attributes, which are stored in <code>user_attribute_metadata</code> table in Redshift.</li> </ol>"},{"location":"analytics/data-mgmt/#metadata-dimensions","title":"Metadata dimensions","text":"<p>Below tables list all the dimensions included in each type of the metadata.</p>"},{"location":"analytics/data-mgmt/#event","title":"Event","text":"Dimension name What it is Event name The name of the event reported from SDK Display name The display name of the event. By default, it is the same as Event name, user can customize the display name. Description The description name of the event reported from SDK. User can customize the display name. For the event automatically collected by the clickstream SDK, the solution has pre-populated description Source Describe how the event was collected, <code>Preset</code> indicates the event is automatically collected by SDK, <code>Custom</code> indicates the event is defined and collected by app owner Platform Describe which platform the event was collected from, i.e., from Android, Web or iOS Data volume last day Describe how much data was collected in last day (in UTC timezone) SDK version Describe the version of the SDK that collected the event Associate preset parameters The preset event parameters associated with the event Associate custom parameters The custom event parameters associated with the event"},{"location":"analytics/data-mgmt/#event-parameters","title":"Event Parameters","text":"Dimension name What it is Parameter name The name of the event event parameter reported from SDK Display name The display name of the event parameter. By default, it is the same as Parameter name, user can customize the display name. Description The description name of the event parameter reported from SDK. User can customize the display name. For the event automatically collected by the clickstream SDK, the solution has pre-populated description Source Describe how the event parameter was collected, <code>Preset</code> indicates the event parameter is automatically collected by SDK, <code>Custom</code> indicates the event parameter is defined and collected by app owner Data type Describe the data type of the event parameter value, e.g., int, string, Associate event The event that the event parameters associated with. Dictionary The unique value for the event parameters, user can customize the display value."},{"location":"analytics/data-mgmt/#user-attribute","title":"User Attribute","text":"Dimension name What it is Attribute name The name of the user attribute reported from SDK Display name The display name of the user attribute. By default, it is the same as Attribute name, user can customize the display name. Description The description name of the user attribute reported from SDK. User can customize the display name. For the user attribute automatically collected by the clickstream SDK, the solution has pre-populated description Source Describe how the event parameter was collected, <code>Preset</code> indicates the user attribute is automatically collected by SDK, <code>Custom</code> indicates the user attribute is defined and collected by app owner Data type Describe the data type of the user attribute value, e.g., int, string,"},{"location":"analytics/data-mgmt/#update-event-display-name-and-description","title":"Update event display name and description","text":"<ol> <li>Click on the Events page, select any event, e.g., <code>view_item</code>.</li> <li>Click on the column labeled Display Name, enter a name, such as <code>View Product Details</code>, and then click confirm.</li> <li>Go back to Exploration, and in the filter dropdown, you can see <code>view_item</code> now displayed as <code>View Product Details</code>.</li> </ol> <p>Follow the same steps to update display name and description for Event Parameters and User Attributes.</p>"},{"location":"analytics/data-mgmt/#customize-data-dictionary-for-event-parameter-values","title":"Customize data dictionary for Event Parameter values","text":"<ol> <li>Click on the Event Properties page, and in the search box, select an event parameter, such as <code>_entrances</code>.</li> <li>The detailed information for the property should open automatically, choose the Dictionary page.</li> <li>Click on the column labeled Display Value, enter a new value, such as <code>Non-First Entry</code> for <code>0</code>, and enter <code>First Entry</code> for <code>1</code>, then click confirm.</li> <li>Go back to Exploration, and in the filter dropdown, you can see <code>_entrances</code> displayed as <code>Non-First Entry</code> and <code>First Entry</code>.</li> </ol> <p>Follow the same steps to customize data dictionary for User Attributes values.</p>"},{"location":"analytics/explore/","title":"Exploration","text":"<p>Explorations is a collection of advanced analytics models that provides flexible and easy-to-use query method to analyze clickstream, allowing you to uncover deeper insights from your clickstream data.</p> <p>When you want to explore the clickstream data in more detail, you can use explorations to:</p> <ul> <li>perform ad hoc queries </li> <li>focus on the most relevant data by using filters on event parameters and user attributes</li> <li>group metrics by dimension to make it easy to compare</li> <li>easily switch visualization type and drill down or up into data</li> <li>view and export summary data into the excel or csv</li> <li>save the exploration analytics results into a dashboard for share or regular use</li> </ul> <p>s</p>"},{"location":"analytics/explore/#access-explorations","title":"Access Explorations","text":"<p>To access Explorations, follow below steps:</p> <ol> <li>Go to Clickstream Analytics on AWS Console, in the Navigation Bar, click on \"Analytics Studio\", a new tab will be opened in your browser.</li> <li>In the Analytics Studio page, click the Explorations icon in the left navigation panel.</li> </ol>"},{"location":"analytics/explore/#how-explorations-works","title":"How Explorations works","text":"<p>Explorations currently provides 4 analytics models:</p> Report name What it is Event Analysis Event analysis is used to study the frequency of certain behavioral events. You can conduct multi-dimensional analysis of user behavior through custom metrics, groupings, filters, and various visual charts. Funnel Analysis Funnel analysis, or conversion analysis, is mainly used to analyze the conversion status of users in a specified process. The model first breaks down the entire process into steps and then counts the conversion rate from each step to the next. It can be used to measure the performance of each step. Common usage scenarios include analyzing registration conversion rates, purchase conversion rates, etc. Path Analysis Funnel analysis, or conversion analysis, is mainly used to analyze the conversion status of users in a specified process. The model first breaks down the entire process into steps and then counts the conversion rate from each step to the next. It can be used to measure the performance of each step. Common usage scenarios include analyzing registration conversion rates, purchase conversion rates, etc. Retention Analysis Retention analysis support configuring initial event and returning event to calculate the retention or attrition rate of target user groups. It also supports setting associated attributes for initial event and return event."},{"location":"analytics/explore/#how-to-use-explorations","title":"How to use Explorations","text":"<p>Explorations interface consists of the following components:</p> <p></p> <ol> <li>Analytics Model. A drop-down list to select or switch analytics model.</li> <li>Model Configuration. Specifies the configurations for the analytics model, such select event, adding filters. Each model might have different configuration.</li> <li>Global Filters. Filters that apply to all the metrics that defined in the Model Configuration. You can add multiple filters and adjust the filter relationship (i.e., 'And' or 'Or').</li> <li>Grouping. Group the analytics result by specified parameter's value, make it easy for you can compare the metrics at a dimension.</li> <li>Analysis Time Range. Specifies the time range for the analysis.</li> <li>Aggregation Granularity. Specifies the level of granularity to display metrics, such by day, week, or month.</li> <li>Visual Type. Specifies the chart type of the visualization.</li> <li>Save to Dashboard. Save the exploration analysis </li> <li>Result Display Area. Show visualization and detail data.</li> <li>Help Panel. Display additional helpful info when click \"Info\" icon.</li> </ol>"},{"location":"analytics/explore/event/","title":"Event Analysis","text":"<p>Event analysis is used to study the frequency of certain behavioral events. You can conduct multi-dimensional analysis of user behavior through custom metrics, groupings, filters, and various visual charts.</p>"},{"location":"analytics/explore/event/#use-cases","title":"Use cases","text":"<p>Event analysis are commonly used when analyzing user behaviors, for example:</p> <ul> <li>Query on user usage of certain product functions (such as adding favorite, video playback, view live stream, etc.);</li> <li>Compare different group of users's behaviors, such as number of login per different country</li> <li>Compare different channel's effectiveness, such as sign-up rate per traffic source.</li> </ul>"},{"location":"analytics/explore/event/#key-concept","title":"Key concept:","text":"<ul> <li>Metric: perform aggregation on a selected event, such as count number of event, or count number of distinct users perform the event.</li> </ul>"},{"location":"analytics/explore/event/#how-to-use","title":"How to use","text":"<ol> <li>Select an event, and select the aggregation method for the metric.</li> <li>Add filter to the event by clicking the  icon next to the metric.</li> <li>Select an event parameter or user attribute as filter. You can add multiple filters by clicking on the  icon. You can also configure the filter relationship by clicking on <code>AND</code> or <code>OR</code>.</li> <li>Repeat above step to add more metric if needed.</li> <li>If needed, configure global filter by selecting event parameter or user attributes. Similar to event filter, you can add multiple global filters and configure the filter relationship.</li> <li>If needed, configure grouping by selecting an event parameter or user attribute</li> <li>Click on Query button to start the analysis.</li> <li>Adjust the data granularity, such as Daily, Weekly, Monthly, if needed.</li> <li>Adjust query time range if needed.</li> <li>Click on Save to Dashboard to save the analysis to a Dashboard, input a name, description, and select a dashboard and sheet.</li> </ol>"},{"location":"analytics/explore/event/#example","title":"Example:","text":"<p>Calculate the daily page views (PV - Page View) and active user count (UV - Unique Visitor) on the web from different countries over the past month, requiring active users to have a session duration of at least 30000 millisecond.</p>"},{"location":"analytics/explore/event/#steps","title":"Steps","text":"<ol> <li>Select the Event Analysis model.</li> <li>In the left Define Metrics area, choose <code>_page_view</code> as the metric for calculating events and select <code>Event number</code> as the metric type.</li> <li>Click the + Add Event button to add another metric. Choose <code>_app_end</code> as the metric for calculating events and select <code>User number</code> as the metric type.</li> <li>Click the filter icon to the right of <code>_app_end</code> to add a event filter condition:<ul> <li>Filter property: <code>event._session_duration</code></li> <li>Operation: <code>&gt;=</code></li> <li>Value: <code>30000</code> (the unit of <code>event._session_duration</code> is millisecond)</li> </ul> </li> <li>Configure a global filter in the right Filters area:<ul> <li>Choose <code>other.platform</code> as the filter property.</li> <li>Operation: <code>=</code></li> <li>Value: <code>Web</code></li> </ul> </li> <li>In the right Attribute Grouping area, configure grouping by selecting <code>geo.country</code>.</li> <li>In the time selector at the bottom, choose <code>Past Month</code> and click OK.</li> <li>Click the Save to Dashboard button in the top right corner. In the pop-up dialog, enter:<ul> <li>Chart Name: <code>PV and UV</code></li> <li>Chart Description: <code>PV and UV on the web over the past month (at least 30 seconds)</code></li> <li>Choose a Dashboard: Select a dashboard. (You need to create a dashboard first, follow the Create dashboard,)</li> <li>Choose a Worksheet: Select a worksheet.</li> <li>Click OK.</li> </ul> </li> </ol> <p>All configurations are as shown in the image below: </p>"},{"location":"analytics/explore/funnel/","title":"Funnel Analysis","text":"<p>Funnel analysis, or conversion analysis, is mainly used to analyze the conversion status of users in a specified process. The model first breaks down the entire process into steps and then counts the conversion rate from each step to the next. It can be used to measure the performance of each step. </p>"},{"location":"analytics/explore/funnel/#use-cases","title":"Use cases","text":"<p>Funnel analysis are commonly used when analyzing user behaviors, for example:</p> <ul> <li>Analysis of the conversion rate of the key process in a product: such as order to purchase rate, registration completion rate;</li> <li>Analysis of the conversion rate of promotion: for example, conversion rate of different in-app promotion spot;</li> <li>Analysis of marketing channels's effectiveness: for example, purchase rate of new users brought by different ad campaigns.</li> </ul>"},{"location":"analytics/explore/funnel/#key-concepts","title":"Key concepts","text":"<ul> <li>Metric: the entity used for funnel analysis, such as event number or user number.</li> <li>Funnel: a funnel is a sequence of events that represents a process, it contains at least two events, each event represents a step in the funnel.</li> <li>Funnel window: he funnel Window refers to the time for the user to complete the entire process. Only when the user completes all the selected steps within the set window period is considered a successful conversion.</li> </ul>"},{"location":"analytics/explore/funnel/#how-to-use","title":"How to use","text":"<ol> <li>Select a metric type. <ol> <li>User number: calculate the number of distinct users pass through the entire funnel.</li> <li>Event number: calculate the number of completion of the entire funnel.</li> </ol> </li> <li>Configure the funnel window.<ol> <li>Custom: you can define any duration as the funnel window.</li> <li>The day: complete the funnel within the same date of the first step.</li> </ol> </li> <li>Select event for as the step, click the <code>+Add Step</code> button to add more steps. You can add up to 10 steps.</li> <li>Click on the  to filter the event. Only the event meet with the filter criterial that will be counted as pass through the funnel. You can add multiple filters to one event.</li> <li>If needed, configure global filter by selecting event parameter or user attributes. Similar to event filter, you can add multiple global filters and configure the filter relationship.</li> <li> <p>If needed, configure grouping by selecting an event parameter or an user attribute.</p> <p>Note</p> <p>The Funnel visualization does not support grouping, if you need to group funnel result, please select bar chart.</p> </li> <li> <p>If you want to only apply the grouping on the first event, toggle on <code>Apply grouping to first step only</code>. If this option is not selected, the grouping will apply to all the steps in the funnel, which means all the events should have parameter or attributes that used to group.</p> </li> <li>Click on <code>Query</code> button to start the analysis. </li> <li>Adjust the data granularity, such as <code>Daily</code>, <code>Weekly</code>, <code>Monthly</code>, if needed.</li> <li>Adjust query time range if needed.</li> <li>Click on <code>Save to Dashboard</code> to save the analysis to a Dashboard, input a name, description, and select a dashboard and sheet.</li> </ol>"},{"location":"analytics/explore/funnel/#example","title":"Example","text":"<p>Calculate the conversion rate of users on the web from opening the website -&gt; viewing the product details page -&gt; adding to the shopping cart -&gt; making a payment over the past week.</p> <ol> <li>Select the Funnel Analysis model.</li> <li>Choose <code>User number</code> as the metric.</li> <li>In the left Define Funnel area, choose <code>The Day</code> as the funnel window.</li> <li>Choose <code>_session_start</code>, <code>view_item</code>, <code>add_to_cart</code>, <code>purchase</code> as funnel events.</li> <li>Configure a global filter in the right Filters area:<ul> <li>Choose <code>other.platform</code> as the filter property.</li> <li>Operation: <code>=</code></li> <li>Value: <code>Web</code></li> </ul> </li> <li>Click Query.</li> </ol> <p>All configurations are as shown in the image below: </p>"},{"location":"analytics/explore/path/","title":"Path Analysis","text":"<p>Path analysis refers to the distribution of the behavior of a group of users after sorting them in sequential order. It is mainly used to analyze and record the distribution ratio of users between pages or events. For example, check how many customers clicked on the product list after opening the app, how many customers viewed the product detail page after visiting landing page, how many customers searched for the product, etc. </p>"},{"location":"analytics/explore/path/#use-cases","title":"Use cases","text":"<p>Path analysis are commonly used when analyzing user navigation pattern, for example:</p> <ul> <li>Analyze the behavioral path distribution of users after entering the product or arriving at landing page;</li> <li>Analyze the transition steps or screen within a specific processes (such as registration, login, payment, etc.) or product modules;</li> </ul>"},{"location":"analytics/explore/path/#key-concepts","title":"Key concepts","text":"<ul> <li>Metric: the entity used for path analysis, such as event number or user number.</li> <li>Session: the length of the path analysis session, only the events happened within the session can form a path, events happened in different path will not be counted in the same path.</li> <li>Node: a node in the path can be an event, screen, or page.</li> <li>Starting node: the starting point of the path analysis.</li> </ul>"},{"location":"analytics/explore/path/#how-to-use","title":"How to use","text":"<ol> <li>Select a metric type. <ol> <li>User number: calculate the number of distinct users pass through the entire path.</li> <li>Event number: calculate the number of completion of the entire path.</li> </ol> </li> <li>Configure the session.<ol> <li>Session ID: Use the session generated by the SDK as the session for path analysis, only the events happened with the same session ID could be counted as a path.</li> <li>Custom: you can define any duration as the funnel window.</li> </ol> </li> <li>Specify the type of node for the path.<ol> <li>Event name: use event as a node, e.g., _page_view, screen_view.</li> <li>Screen name: use the name of a screen in the App as a node.</li> <li>Screen ID: use the class ID of a screen in the App as a node.</li> <li>Page title: use the page title as a node</li> <li>Page url: use the page url as a node</li> </ol> </li> <li>Set a starting node. The start node will be the first node of the path, only event happen after the node will be counted into the path.</li> <li>Select nodes to participate in the analysis. Only node selected will be highlighted as nodes in the path. You can add up to 10 nodes.</li> <li>Specify if you want to include other nodes in the path, if this option is toggled on, all the nodes that are not selected will be counted and display as \"Other\" in the path.</li> <li>Specify if you want to merge consecutive nodes, if this option is toggled on, nodes that are repeated continuously in the same session will be merged into a single node.</li> <li>If needed, configure global filter by selecting event parameter or user attributes. Similar to event filter, you can add multiple global filters and configure the filter relationship.</li> <li>Click on <code>Query</code> button to start the analysis. </li> <li>Adjust query time range if needed.</li> <li>Click on <code>Save to Dashboard</code> to save the analysis to a Dashboard, input a name, description, and select a dashboard and sheet.</li> </ol>"},{"location":"analytics/explore/path/#example","title":"Example","text":"<p>Calculate the distribution of events triggered by customers on the web after the session starts, focusing on login, registration, product exposure, search, viewing products, adding to cart, starting checkout, and placing an order, as well as all other events.</p> <ol> <li>Select the Path Analysis model.</li> <li>Choose <code>User number</code> as the metric.</li> <li>Choose <code>Session ID</code> as the session definition.</li> <li>Choose <code>Event</code> as the node type.</li> <li>In the left Select Nodes area, choose <code>_session_start</code> as the starting node.</li> <li>Choose <code>sign_up</code>, <code>login</code>, <code>product_exposure</code>, <code>search</code>, <code>view_item</code>, <code>add_to_cart</code>, <code>begin_checkout</code>, <code>purchase</code> in sequence as path nodes.</li> <li>Enable Include Other Events.</li> <li>Enable Merge Consecutive Events.</li> <li>Configure a global filter in the right Filters area:<ul> <li>Choose <code>other.platform</code> as the filter property.</li> <li>Operation: <code>=</code></li> <li>Value: <code>Web</code></li> </ul> </li> <li>Click Query.</li> </ol> <p>All configurations are as shown in the image below: </p>"},{"location":"analytics/explore/retention/","title":"Retention Analysis","text":"<p>Retention rate is a common metric used to assess the user stickiness for an app or website. Retention means that the user returns to your app or website again some time after they used your app. In addition to the standard retention metrics in the default dashboard, Retention Analysis module allow you to select a start event and returning event to customize a retention or attrition rate of a target user group. </p>"},{"location":"analytics/explore/retention/#use-cases","title":"Use cases","text":"<p>Retention analysis are commonly used to understand how well your app or website is doing in terms of retaining users, for example:</p> <ul> <li>Calculate new user retention rate to measure the effectiveness of traffic channel;</li> <li>Calculate the active users retention rate to measure teh effectiveness for a promotion campaign</li> <li>Compare the repurchase rate for different groups of users to identify the most-value customers.</li> </ul>"},{"location":"analytics/explore/retention/#key-concept","title":"Key concept:","text":"<ul> <li>Start: the event indicates that users start using the app or website. </li> <li>Revisit: the event indicates that users returning to the app or website.</li> <li> <p>Associated parameter: Associated parameter are used to keep the value of a parameter consistent between the starting event and the return event. For example, promotion campaign name, page title, or product titles must be the same values for both starting event and return event.</p> <p>Note</p> <p>The two associated parameter must both have value, and the value types must be consistent.</p> </li> <li> <p>Retention Rate:  retention rate % = the number of users performed the specified starting event on the start date (or week, or month depends on the granularity selection) / the number of the same users performed the specified returning event on the the return date (or week, or month) </p> </li> </ul>"},{"location":"analytics/explore/retention/#how-to-use","title":"How to use","text":"<ol> <li>Select a Start event, you can add filter to the event by clicking the  icon.</li> <li>Select a Revisit event, you can add filter to the event by clicking the  icon.</li> <li>If need to associate parameter, you can toggle on Associate parameter, then select parameter for both starting event and return event.</li> <li>Repeat above step to add more metric if needed.</li> <li>If needed, configure global filter by selecting event parameter or user attributes. Similar to event filter, you can add multiple global filters and configure the filter relationship.</li> <li>If needed, configure grouping by selecting an event parameter or user attribute</li> <li>Click on Query button to start the analysis.</li> <li>Adjust the data granularity, such as Daily, Weekly, Monthly, if needed.</li> <li> <p>Specify query time range. </p> <p>Note</p> <p>The start time will be the starting point (i.e., day 0) for the retention analysis, retention rate % will be calculated against the number of users performed the specified starting event on the start date (or week, or month depends on the granularity selection).</p> </li> <li> <p>Click on Save to Dashboard to save the analysis to a Dashboard, input a name, description, and select a dashboard and sheet.</p> </li> </ol>"},{"location":"analytics/explore/retention/#example","title":"Example:","text":"<p>Calculate the retention rate of new customers who downloaded from different app markets on Android one week ago.</p> <ol> <li>Select the Retention Analysis model.</li> <li>Choose <code>_first_open</code> as the start event.</li> <li>Choose <code>_app_start</code> as the return event.</li> <li>Configure a global filter in the right Filters area:<ul> <li>Choose <code>other.platform</code> as the filter property.</li> <li>Operation: <code>=</code></li> <li>Value: <code>Android</code></li> </ul> </li> <li>In the right Attribute Grouping area, configure grouping by selecting <code>app_info.instal_source</code>.</li> <li>Click Query.</li> </ol> <p>All configurations are as shown in the image below: </p>"},{"location":"deployment/","title":"Overview","text":"<p>Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<p>Review all the considerations and make sure you have the following in the target Region you want to deploy the solution:</p> <ul> <li>At least two vacant S3 buckets.</li> </ul>"},{"location":"deployment/#deployment-in-aws-regions","title":"Deployment in AWS Regions","text":"<p>Clickstream Analytics on AWS provides two ways to authenticate and log into the solution web console. For some AWS Regions where Cognito User Pool is unavailable (for example, Hong Kong), you must launch the solution with OpenID Connect.</p> <ul> <li>Launch with Cognito User Pool. (Fastest way to get started, suitable for most AWS regions)</li> <li>Launch with OpenID Connect</li> </ul> <p>For more information about supported regions, see Regional deployments.</p>"},{"location":"deployment/#deployment-in-aws-china-regions","title":"Deployment in AWS China Regions","text":"<p>AWS China Regions do not have Cognito User Pool. You must launch the solution with OpenID Connect.</p> <ul> <li>Launch with OpenID Connect</li> </ul>"},{"location":"deployment/#deployment-within-amazon-vpc","title":"Deployment within Amazon VPC","text":"<p>Clickstream Analytics on AWS supports being deployed into an Amazon VPC, allowing access to the web console without leaving your VPC network.</p> <ul> <li>Launch within VPC</li> </ul>"},{"location":"deployment/tls-note/","title":"Tls note","text":"<p>By default, this deployment uses TLSv1.0 and TLSv1.1 in CloudFront. However, we recommend that you manually configure CloudFront to use the more secure TLSv1.2/TLSv1.3 and apply for a certificate and custom domain to enable this. We highly recommend that you update your TLS configuration and cipher suite selection according to the following recommendations:</p> <ul> <li>Transport Layer Security Protocol: Upgrade to TLSv1.2 or higher</li> <li>Key Exchange: ECDHE</li> <li>Block Cipher Mode of Operation: GCM</li> <li>Authentication: ECDSA</li> <li>Encryption Cipher: AES256</li> <li>Message Authentication: SHA(256/384/any hash function except for SHA1)</li> </ul> <p>Such as TLSv1.2_2021 can meet the above recommendations.</p>"},{"location":"deployment/with-cognito/","title":"Launch with Cognito User Pool","text":"<p>Time to deploy: Approximately 15 minutes</p>"},{"location":"deployment/with-cognito/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Launch the stack</p> <p>Step 2. Launch the web console</p>"},{"location":"deployment/with-cognito/#step-1-launch-the-stack","title":"Step 1. Launch the stack","text":"<p>This AWS CloudFormation template automatically deploys the Clickstream Analytics on AWS solution on AWS.</p> <ol> <li> <p>Sign in to the AWS Management Console and select the button to launch the AWS CloudFormation template.</p> Launch in AWS Console Launch stack Launch stack with custom domain </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Clickstream Analytics on AWS solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and AWS STS quotas in the AWS Identity and Access Management User Guide.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li> <p>This solution uses the following parameters:</p> Parameter Default Description Email <code>&lt;Requires input&gt;</code> Specify the email of the Administrator. This email address will receive a temporary password to access the Clickstream Analytics on AWS web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. <p>Important</p> <p> By default, this deployment uses TLSv1.0 and TLSv1.1 in CloudFront. However, we recommend that you manually configure CloudFront to use the more secure TLSv1.2/TLSv1.3 and apply for a certificate and custom domain to enable this. We highly recommend that you update your TLS configuration and cipher suite selection according to the following recommendations:</p> <ul> <li>Transport Layer Security Protocol: Upgrade to TLSv1.2 or higher</li> <li>Key Exchange: ECDHE</li> <li>Block Cipher Mode of Operation: GCM</li> <li>Authentication: ECDSA</li> <li>Encryption Cipher: AES256</li> <li>Message Authentication: SHA(256/384/any hash function except for SHA1)</li> </ul> <p>Such as TLSv1.2_2021 can meet the above recommendations. </p> </li> <li> <p>If you are launching the solution with custom domain in AWS regions, this solution uses the additional following parameters:</p> Parameter Default Description Hosted Zone ID <code>&lt;Requires input&gt;</code> Choose the public hosted zone ID of Amazon Route 53. Hosted Zone Name <code>&lt;Requires input&gt;</code> The domain name of the public hosted zone, for example, <code>example.com</code>. Record Name <code>&lt;Requires input&gt;</code> The sub name (as known as record name in R53) of the domain name of console. For example, enter <code>clickstream</code>, if you want to use custom domain <code>clickstream.example.com</code> for the console. </li> </ul> </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Select the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"deployment/with-cognito/#step-2-launch-the-web-console","title":"Step 2. Launch the web Console","text":"<p>After the stack is successfully created, this solution generates a CloudFront domain name that gives you access to the Clickstream Analytics on AWS web console. Meanwhile, an auto-generated temporary password will be sent to your email address.</p> <ol> <li> <p>Sign in to the AWS CloudFormation console.</p> </li> <li> <p>On the Stacks page, select the solution\u2019s stack.</p> </li> <li> <p>Choose the Outputs tab and record the domain name.</p> </li> <li> <p>Open the ControlPlaneURL using a web browser, and navigate to a sign-in page.</p> </li> <li> <p>Enter the Email and the temporary password.</p> <p>a. Set a new account password.</p> <p>b. (Optional) Verify your email address for account recovery.</p> </li> <li> <p>After the verification is complete, the system opens the Clickstream Analytics on AWS web console.</p> </li> </ol> <p>Once you have logged into the Clickstream Analytics on AWS console, you can start to create a project for your applications.</p>"},{"location":"deployment/with-oidc/","title":"Launch with OpenID Connect (OIDC)","text":"<p>Time to deploy: Approximately 30 minutes</p>"},{"location":"deployment/with-oidc/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>The Clickstream Analytics on AWS console is served via CloudFront distribution which is considered as an Internet information service. If you are deploying the solution in AWS China Regions, the domain must have a valid ICP Recordal.</p> <ul> <li>A domain. You will use this domain to access the Clickstream Analytics on AWS console. This is required for AWS China Regions, and is optional for AWS Regions.</li> <li>An SSL certificate in AWS IAM. The SSL must be associated with the given domain. Follow this guide to upload SSL certificate to IAM. This is required for AWS China Regions only.</li> </ul>"},{"location":"deployment/with-oidc/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Create OIDC client</p> <p>Step 2. Launch the stack</p> <p>Step 3. Update the callback URL of OIDC client</p> <p>Step 4. Set up DNS Resolver</p> <p>Step 5. Launch the web console</p>"},{"location":"deployment/with-oidc/#step-1-create-oidc-client","title":"Step 1. Create OIDC client","text":"<p>You can use different kinds of OpenID Connect providers. This section introduces Option 1 to Option 4.</p> <ul> <li>(Option 1) Using Amazon Cognito from another region as OIDC provider.</li> <li>(Option 2) Authing, which is an example of a third-party authentication provider.</li> <li>(Option 3) Keycloak, which is a solution maintained by AWS and can serve as an authentication identity provider.</li> <li>(Option 4) ADFS, which is a service offered by Microsoft.</li> <li>(Option 5) Other third-party authentication platforms such as Auth0.</li> </ul> <p>Follow the steps below to create an OIDC client, and obtain the <code>client_id</code> and <code>issuer</code>.</p>"},{"location":"deployment/with-oidc/#option-1-using-cognito-user-pool-from-another-region","title":"(Option 1) Using Cognito User Pool from another region","text":"<p>You can leverage the Cognito User Pool in a supported AWS Region as the OIDC provider.</p> <ol> <li>Go to the Amazon Cognito console in an AWS Region.</li> <li>Set up the hosted UI with the Amazon Cognito console based on this guide. Please pay attentions to below two configurations<ul> <li>Choose Public client when selecting the App type. Make sure don't change the selection Don't generate a client secret for Client secret.</li> <li>Add Profile in OpenID Connect scopes.</li> </ul> </li> <li> <p>Enter the Callback URL and Sign out URL using your domain name for Clickstream Analytics on AWS console as the following:</p> <ul> <li>Callback URL: <code>http[s]://&lt;domain-name&gt;/signin</code></li> <li>Sign out URL: <code>http[s]://&lt;domain-name&gt;</code></li> </ul> <p>Note</p> <p>If you're not using custom domain for the console, you don't know the domain name of console. You can input a fake one, for example, <code>clickstream.example.com</code>. Then update it following guidelines in Step 3.</p> </li> <li> <p>If your hosted UI is set up, you should be able to see something like below.</p> <p></p> </li> <li> <p>Save the App client ID, User pool ID and the AWS Region to a file, which will be used later.</p> <p> </p> </li> </ol> <p>In Step 2. Launch the stack, enter the parameters below from your Cognito User Pool.</p> <ul> <li>OIDCClientId: <code>App client ID</code></li> <li>OIDCProvider: <code>https://cognito-idp.${REGION}.amazonaws.com/${USER_POOL_ID}</code></li> </ul>"},{"location":"deployment/with-oidc/#option-2-authingcn-oidc-client","title":"(Option 2) Authing.cn OIDC client","text":"<ol> <li>Go to the Authing console.</li> <li>Create a user pool if you don't have one.</li> <li>Select the user pool.</li> <li>On the left navigation bar, select Self-built App under Applications.</li> <li>Click the Create button.</li> <li>Enter the Application Name, and Subdomain.</li> <li> <p>Save the <code>App ID</code> (that is, <code>client_id</code>) and <code>Issuer</code> to a text file from Endpoint Information, which will be used later.</p> <p></p> </li> <li> <p>Update the <code>Login Callback URL</code> and <code>Logout Callback URL</code>, note that you need to add <code>/signin</code> to your domain name for <code>Login Callback URL</code> as follow:</p> <ul> <li>Callback URL: <code>http[s]://&lt;domain-name&gt;/signin</code></li> <li>Sign out URL: <code>http[s]://&lt;domain-name&gt;</code></li> </ul> </li> <li> <p>Set the Authorization Configuration.</p> <p></p> </li> </ol> <p>You have successfully created an authing self-built application.</p> <p>In Step 2. Launch the stack, enter the parameters below from your Authing user pool.</p> <ul> <li>OIDCClientId: <code>client id</code></li> <li>OIDCProvider: <code>Issuer</code></li> </ul>"},{"location":"deployment/with-oidc/#option-3-keycloak-oidc-client","title":"(Option 3) Keycloak OIDC client","text":"<ol> <li> <p>Deploy the Keycloak solution in AWS China Regions following this guide.</p> </li> <li> <p>Sign in to the Keycloak console.</p> </li> <li> <p>On the left navigation bar, select Add realm. Skip this step if you already have a realm.</p> </li> <li> <p>Go to the realm setting page. Choose Endpoints, and then OpenID Endpoint Configuration from the list.</p> <p></p> </li> <li> <p>In the JSON file that opens up in your browser, record the issuer value which will be used later.</p> <p></p> </li> <li> <p>Go back to Keycloak console and select Clients on the left navigation bar, and choose Create.</p> </li> <li>Enter a Client ID, which must contain letters (case-insensitive) or numbers. Record the Client ID which will be used later.</li> <li> <p>Change client settings. Enter <code>http[s]://&lt;Clickstream Analytics on AWS Console domain&gt;/signin</code> in Valid Redirect URIs\uff0cand enter <code>&lt;console domain&gt;</code> and <code>+</code> in Web Origins.</p> <p>Tip</p> <p>If you're not using custom domain for the console, the domain name of console is not available yet. You can enter a fake one, for example, <code>clickstream.example.com</code>, and then update it following guidelines in Step 3.</p> </li> <li> <p>In the Advanced Settings, set the Access Token Lifespan to at least 5 minutes.</p> </li> <li>Select Users on the left navigation bar.</li> <li>Click Add user and enter Username.</li> <li>After the user is created, select Credentials, and enter Password.</li> </ol> <p>In Step 2. Launch the stack, enter the parameters below from your Keycloak realm.</p> <ul> <li>OIDCClientId: <code>client id</code></li> <li>OIDCProvider: <code>https://&lt;KEYCLOAK_DOMAIN_NAME&gt;/auth/realms/&lt;REALM_NAME&gt;</code></li> </ul>"},{"location":"deployment/with-oidc/#option-4-adfs-openid-connect-client","title":"(Option 4) ADFS OpenID Connect Client","text":"<ol> <li>Make sure your ADFS is installed. For information about how to install ADFS, refer to this guide.</li> <li>Make sure you can log in to the ADFS Sign On page. The URL should be <code>https://adfs.domain.com/adfs/ls/idpinitiatedSignOn.aspx</code>, and you need to replace adfs.domain.com with your real ADFS domain.</li> <li>Log on your Domain Controller, and open Active Directory Users and Computers.</li> <li> <p>Create a Security Group for Clickstream Analytics on AWS Users, and add your planned Clickstream Analytics on AWS users to this Security Group.</p> </li> <li> <p>Log on to ADFS server, and open ADFS Management.</p> </li> <li> <p>Right click Application Groups, choose Application Group, and enter the name for the Application Group. Select Web browser accessing a web application option under Client-Server Applications, and choose Next.</p> </li> <li> <p>Record the Client Identifier (<code>client_id</code>) under Redirect URI, enter your Clickstream Analytics on AWS domain (for example, <code>xx.example.com</code>), and choose Add, and then choose Next.</p> </li> <li> <p>In the Choose Access Control Policy window, select Permit specific group, choose parameters under Policy part, add the created Security Group in Step 4, then click Next. You can configure other access control policy based on your requirements.</p> </li> <li> <p>Under Summary window, choose Next, and choose Close.</p> </li> <li> <p>Open the Windows PowerShell on ADFS Server, and run the following commands to configure ADFS to allow CORS for your planned URL.</p> <pre><code>Set-AdfsResponseHeaders -EnableCORS $true\nSet-AdfsResponseHeaders -CORSTrustedOrigins https://&lt;your-clickstream-analytics-on-aws-domain&gt;\n</code></pre> </li> <li> <p>Under Windows PowerShell on ADFS server, run the following command to get the Issuer (<code>issuer</code>) of ADFS, which is similar to <code>https://adfs.example.com/adfs</code>.</p> <pre><code>Get-ADFSProperties | Select IdTokenIssuer\n</code></pre> <p></p> </li> </ol> <p>In Step 2. Launch the stack, enter the parameters below from your ADFS server.</p> <ul> <li>OIDCClientId: <code>client id</code></li> <li>OIDCProvider: Get the server of the issuer from above step 11</li> </ul>"},{"location":"deployment/with-oidc/#step-2-launch-the-stack","title":"Step 2. Launch the stack","text":"<ol> <li> <p>Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template.</p> Launch in AWS Console Launch in AWS Regions Launch with custom domain in AWS Regions Launch in AWS China Regions </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Clickstream Analytics on AWS solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and AWS STS quotas in the AWS Identity and Access Management User Guide.</li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li> <p>This solution uses the following parameters:</p> Parameter Default Description OIDCClientId <code>&lt;Requires input&gt;</code> OpenID Connect client Id. OIDCProvider <code>&lt;Requires input&gt;</code> OpenID Connect provider issuer. The issuer must begin with <code>https://</code> Email <code>&lt;Requires input&gt;</code> Specify the email of the Administrator. <p>Important</p> <p> By default, this deployment uses TLSv1.0 and TLSv1.1 in CloudFront. However, we recommend that you manually configure CloudFront to use the more secure TLSv1.2/TLSv1.3 and apply for a certificate and custom domain to enable this. We highly recommend that you update your TLS configuration and cipher suite selection according to the following recommendations:</p> <ul> <li>Transport Layer Security Protocol: Upgrade to TLSv1.2 or higher</li> <li>Key Exchange: ECDHE</li> <li>Block Cipher Mode of Operation: GCM</li> <li>Authentication: ECDSA</li> <li>Encryption Cipher: AES256</li> <li>Message Authentication: SHA(256/384/any hash function except for SHA1)</li> </ul> <p>Such as TLSv1.2_2021 can meet the above recommendations. </p> </li> <li> <p>If you are launching the solution with custom domain in AWS Regions, this solution has the following additional parameters:</p> Parameter Default Description Hosted Zone ID <code>&lt;Requires input&gt;</code> Choose the public hosted zone ID of Amazon Route 53. Hosted Zone Name <code>&lt;Requires input&gt;</code> The domain name of the public hosted zone, for example, <code>example.com</code>. Record Name <code>&lt;Requires input&gt;</code> The sub name (as known as record name in R53) of the domain name of console. For example, enter <code>clickstream</code> if you want to use custom domain <code>clickstream.example.com</code> for the console. </li> <li> <p>If you are launching the solution in AWS China Regions, this solution has the following additional parameters:</p> Parameter Default Description Domain <code>&lt;Requires input&gt;</code> Custom domain for Clickstream Analytics on AWS console. Do NOT add <code>http(s)</code> prefix. IamCertificateID <code>&lt;Requires input&gt;</code> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the <code>list-server-certificates</code> command to retrieve the ID. </li> </ul> </li> <li> <p>Choose Next.</p> </li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</li> <li>Choose Create stack  to deploy the stack.</li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"deployment/with-oidc/#step-3-update-the-callback-url-of-oidc-client","title":"Step 3. Update the callback URL of OIDC client","text":"<p>Important</p> <p>If you don't deploy stack with custom domain, you must complete below steps.</p> <ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select the solution's stack.</li> <li>Choose the Outputs tab.</li> <li>Obtain the ControlPlaneURL as the endpoint.</li> <li>Update or add the callback URL to your OIDC.<ol> <li>For Cognito, add or update the url in Allowed callback URL of your client with value <code>${ControlPlaneURL}/signin</code>. NOTE: The url must start with <code>https://</code>.</li> <li>For Keycloak, add or update the url in Valid Redirect URIs of your client with value <code>${ControlPlaneURL}/signin</code>.</li> <li>For Authing.cn, add or update the url in Login Callback URL of Authentication Configuration.</li> </ol> </li> </ol>"},{"location":"deployment/with-oidc/#step-4-setup-dns-resolver","title":"Step 4. Setup DNS Resolver","text":"<p>Important</p> <p>If you deploy stack in AWS Regions, you can skip this step.</p> <p>This solution provisions a CloudFront distribution that gives you access to the Clickstream Analytics on AWS console.</p> <ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select the solution's stack.</li> <li>Choose the Outputs tab.</li> <li>Obtain the ControlPlaneURL and CloudFrontDomainName.</li> <li>Create a CNAME record for ControlPlaneURL in DNS resolver, which points to the domain CloudFrontDomainName obtained in previous step.</li> </ol>"},{"location":"deployment/with-oidc/#step-5-launch-the-web-console","title":"Step 5. Launch the web console","text":"<p>Important</p> <p>Your login credentials is managed by the OIDC provider. Before signing in to the Clickstream Analytics on AWS console, make sure you have created at least one user in the OIDC provider's user pool.</p> <ol> <li>Use the previously assigned domain name or the generated ControlPlaneURL in a web browser.</li> <li>Choose Sign In, and navigate to OIDC provider.</li> <li>Enter sign-in credentials. You may be asked to change your default password for first-time login, which depends on your OIDC provider's policy.</li> <li>After the verification is complete, the system opens the Clickstream Analytics on AWS web console.</li> </ol> <p>Once you have logged into the Clickstream Analytics on AWS console, you can start to create a project for your applications.</p>"},{"location":"deployment/within-vpc/","title":"Launch within VPC","text":"<p>Time to deploy: Approximately 30 minutes</p>"},{"location":"deployment/within-vpc/#prerequisites","title":"Prerequisites","text":"<p>Review all the considerations and make sure you have the following in the target region you want to deploy the solution:</p> <ul> <li>At least one Amazon VPC.</li> <li>At least two private (with NAT gateways or instances) subnets across two AZs.</li> </ul>"},{"location":"deployment/within-vpc/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Create OIDC client</p> <p>Step 2. Launch the stack</p> <p>Step 3. Update the callback url of OIDC client</p> <p>Step 4. Launch the web console</p>"},{"location":"deployment/within-vpc/#step-1-create-oidc-client","title":"Step 1. Create OIDC client","text":"<p>You can use existing OpenID Connect (OIDC) provider or following this guide to create an OIDC client.</p> <p>Tip</p> <p>This solution deploys the console in VPC without requiring SSL certificate by default. You have to use an OIDC client to support callback url with <code>http</code> protocol.</p>"},{"location":"deployment/within-vpc/#step-2-launch-the-stack","title":"Step 2. Launch the stack","text":"<ol> <li> <p>Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template.</p> Launch in AWS Console Launch in AWS Regions Launch in AWS China Regions </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Clickstream Analytics on AWS solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and AWS STS quotas in the AWS Identity and Access Management User Guide.</li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li>This solution uses the following parameters:</li> </ul> Parameter Default Description VpcId <code>&lt;Requires input&gt;</code> Select the VPC in which the solution will be deployed. PrivateSubnets <code>&lt;Requires input&gt;</code> Select the subnets in which the solution will be deployed. Note: You must choose two subnets across two AZs at least. OIDCClientId <code>&lt;Requires input&gt;</code> OpenID Connect client Id. OIDCProvider <code>&lt;Requires input&gt;</code> OpenID Connect provider issuer. The issuer must begin with <code>https://</code> Email <code>&lt;Requires input&gt;</code> Specify the email of the Administrator. </li> <li> <p>Choose Next.</p> </li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</li> <li>Choose Create stack  to deploy the stack.</li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"deployment/within-vpc/#step-3-update-the-callback-url-of-oidc-client","title":"Step 3. Update the callback URL of OIDC client","text":"<ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select the solution's stack.</li> <li>Choose the Outputs tab.</li> <li>Obtain the ControlPlaneURL as the endpoint.</li> <li>Update or add the callback URL ${ControlPlaneURL}/signin to your OIDC client.<ol> <li>For Keycloak, add or update the url in Valid Redirect URIs.</li> <li>For Authing.cn, add or update the url in Login Callback URL of Authentication Configuration.</li> </ol> </li> </ol>"},{"location":"deployment/within-vpc/#step-4-launch-the-web-console","title":"Step 4. Launch the web console","text":"<p>Important</p> <p>Your login credentials is managed by the OIDC provider. Before signing in to the Clickstream Analytics on AWS console, make sure you have created at least one user in the OIDC provider's user pool.</p> <ol> <li>Because you deploy the solution console in your VPC without public access, you have to setup a network connection to the solution console serving by an internal application load balancer. There are some options for your reference.<ol> <li>(Option 1) Use bastion host, for example, Linux Bastion Hosts on AWS solution</li> <li>(Option 2) Use AWS Client VPN or AWS Site-to-Site VPN</li> <li>(Option 3) Use AWS Direct Connect</li> </ol> </li> <li>The application load balancer only allows the traffic from specified security group, you can find the security group id from the output named SourceSecurityGroup from the stack you deployed in step 2. Then attach the security group to your bastion host or other source to access the solution console.</li> <li>Use the previously assigned domain name or the generated ControlPlaneURL in a web browser.</li> <li>Choose Sign In, and navigate to OIDC provider.</li> <li>Enter sign-in credentials. You may be asked to change your default password for first-time login, which depends on your OIDC provider's policy.</li> <li>After the verification is complete, the system opens the Clickstream Analytics on AWS web console.</li> </ol> <p>Once you have logged into the Clickstream Analytics on AWS console, you can start to create a project for your applications.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>After deploying the solution, refer to this chapter to get started with the Clickstream Analytics on AWS to collect and analyze clickstream data from your app. This chapter show you how to create a serverless data pipeline to collect data from an application, and use the Analytics Studio to view out-of-the-box user life cycle dashboards and query the clickstream data with exploration analytics. </p>"},{"location":"getting-started/#steps","title":"Steps","text":"<ul> <li>Step 1: Create a project. Create a project to get started.</li> <li>Step 2: Configure a data pipeline. Configure a data pipeline with serverless infrastructure.</li> <li>Step 3: Integrate SDK. Integrate SDK into your application to automatically collect data and send data to the pipeline.</li> <li>Step 4: Access built-in dashboard. View the out-of-the-box dashboards based on the data automatically collected from your applications.  </li> </ul>"},{"location":"getting-started/1.create-project/","title":"Step 1 - Create a project","text":"<p>To get started with the Clickstream Analytics on AWS solution, you need to create a project in the solution console. A project is like a container that groups all the AWS resources provisioned for collecting and analyzing the clickstream data from your apps.</p>"},{"location":"getting-started/1.create-project/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have deployed the Clickstream Analytics on AWS solution. If you haven't, please refer to the deployment guide.</p>"},{"location":"getting-started/1.create-project/#steps","title":"Steps","text":"<p>Following below steps to create a project.</p> <ol> <li>Sign in to the Clickstream Analytics on AWS Console.</li> <li>On the Home page, choose Create Project.</li> <li>In the window that pops up, enter a project name, for example, <code>quickstart</code>.</li> <li>(Optional) Customize the project ID that was automatically created by solution. To do so, click the <code>edit</code> icon and update the project ID as per your need.</li> <li>Provide a description for your project, for example, <code>This is a demo project</code>.</li> <li>Choose Next.</li> <li>Provide an email address to receive notification regarding this project, for example, <code>email@example.com</code>, and choose Next.</li> <li>Specify an environment type for this project. In this example, select <code>Dev</code>.</li> <li>Choose Create. Wait until the project creation completed, and you will be directed to the Projects page.</li> </ol> <p>We have completed all the steps of creating a project.</p>"},{"location":"getting-started/1.create-project/#next","title":"Next","text":"<ul> <li>Configure data pipeline</li> </ul>"},{"location":"getting-started/2.config-pipeline/","title":"Step 2 - Configure pipeline","text":"<p>After you create a project, you need to configure the data pipeline for it. A data pipeline is a set of integrated modules that collect and process the clickstream data sent from your applications. A data pipeline contains four modules, namely data ingestion, data processing, data modeling and reporting. For more information, see pipeline management.</p> <p>Here we provide an example with steps to create a data pipeline with end-to-end serverless infrastructure.</p>"},{"location":"getting-started/2.config-pipeline/#steps","title":"Steps","text":"<ol> <li>Log into Clickstream Analytics on AWS Console.</li> <li>In the left navigation pane, choose Projects, then select the project you just created in Step 1, choose View Details in the top right corner to navigate to the project homepage.</li> <li>Choose Configure pipeline, and it will bring you to the wizard of creating data pipeline for your project.</li> <li> <p>On the Basic information page, fill in the form as follows:</p> <ul> <li>AWS Region: select an AWS region you want to deploy the data pipeline into, for example, <code>us-east-1</code>.</li> <li>VPC: select a VPC that meets the following requirements<ul> <li>At least two public subnets across two different AZs (Availability Zone)</li> <li>At least two private subnets across two different AZs</li> <li>One NAT Gateway or Instance</li> </ul> </li> <li>Data collection SDK: <code>Clickstream SDK</code></li> <li>Data location: select an S3 bucket. (You can create one bucket, and select it after clicking the Refresh button.)</li> </ul> <p>Tip</p> <p>Please refer Security best practices for Amazon S3 to create and configure Amazon S3 bucket. For example: Enable Amazon S3 server access logging, Enable S3 Versioning and so on.</p> <p>Tip</p> <p>If you don't have a VPC meet the criteria, you can create a VPC by using VPC creation wizard quickly. For more information, see Create a VPC. We also recommend that you refer Security best practices for your VPC to configure your vpc.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure ingestion page, fill in the information as follows:</p> <ol> <li>Fill in the form of Ingestion endpoint settings.<ul> <li>Public Subnets: Select two public subnets in two different AZs</li> <li>Private Subnets: Select two private subnets in the same AZs as public subnets</li> <li>Ingestion capacity: Keep the default values</li> <li>Enable HTTPS: Uncheck and then Acknowledge the security warning</li> <li>Cross-Origin Resource Sharing (CORS): leave it blank</li> <li>Additional settings: Keep the default values</li> </ul> </li> <li>Fill in the form of Data sink settings.<ul> <li>Sink type: <code>Amazon Kinesis Data Stream(KDS)</code></li> <li>Provision mode: <code>On-demand</code></li> <li>In Additional Settings, change Sink Maximum Interval to <code>60</code> and Batch Size to <code>1000</code> </li> </ul> </li> <li>Click Next to move to step 3.</li> </ol> <p>Important</p> <p>Using HTTP is NOT a recommended configuration for production workload. This example configuration is to help you get started quicker.</p> </li> <li> <p>On the Configure data processing information, fill in the information as follows:</p> <ul> <li>In the form of Enable data processing, toggle on the Enable data processing</li> <li>In the form of Execution parameters,<ul> <li>Data processing interval:<ul> <li>Select <code>Fixed Rate</code></li> <li>Enter <code>10</code></li> <li>Select <code>Minutes</code></li> </ul> </li> <li>Event freshness: <code>35</code> <code>Days</code></li> </ul> </li> </ul> <p>Important</p> <p>In this example, we set the Data processing interval to be 10 minutes so that you can view the data faster. You can change the interval to be less frequent later to save cost. Refer to Pipeline Management to make change to data pipeline.</p> <ul> <li>In the form of Enrichment plugins, make sure the two plugins of IP lookup and UA parser are selected.</li> <li>In the form of Analytics engine, fill in the form as follow:<ul> <li>Check the box for Redshift</li> <li>Select the Redshift Serverless</li> <li>Keep Base RPU as 8</li> <li>VPC: select the default VPC or the same one you selected previously in the last step</li> <li>Security group: select the <code>default</code> security group</li> <li>Subnet: select three subnets across three different AZs</li> <li>Keep Athena selection as default</li> </ul> </li> <li>Choose Next.</li> </ul> </li> <li> <p>On the Reporting page, fill in the form as follows:</p> <ul> <li>If your AWS account has not subscribed to QuickSight, please follow this guide to subscribe.</li> <li>Toggle on the option of Enable Analytics Studio</li> <li>Choose Next.</li> </ul> </li> <li> <p>On the Review and launch page, review your pipeline configuration details. If everything is configured properly, choose Create.</p> </li> </ol> <p>We have completed all the steps of configuring a pipeline for your project. This pipeline will take about 20 minutes to create, and please wait for the pipeline status change to be Active in pipeline detail page.</p>"},{"location":"getting-started/2.config-pipeline/#next","title":"Next","text":"<ul> <li>Integrate SDK</li> </ul>"},{"location":"getting-started/3.integrate-sdk/","title":"Step 3 - Integrate SDK","text":"<p>Once pipeline's status becomes <code>Active</code>, it is ready to receive clickstream data. Now you need to register an application to the pipeline, then you can integrate SDK into your application to enable it to send data to the pipeline.</p>"},{"location":"getting-started/3.integrate-sdk/#steps","title":"Steps","text":"<ol> <li>Log into Clickstream Analytics on AWS Console.</li> <li>In the left navigation pane, choose Projects, then select the project (<code>quickstart</code>) you just created in previous steps, click its title, and it will bring you to the project page.</li> <li>Choose + Add application to start adding application to the pipeline.</li> <li> <p>Fill in the form as follows:</p> <ul> <li>App name: <code>test-app</code></li> <li>App ID: The system will generate one ID based on the name, and you can customize it if needed.</li> <li>Description: <code>A test app for Clickstream Analytics on AWS solution</code></li> <li>Android package name: leave it blank</li> <li>App Bundle ID: leave it blank</li> </ul> </li> <li> <p>Choose Register App &amp; Generate SDK Instruction, and wait for the registration to be completed.</p> </li> <li> <p>Select the tab Android, and you will see the detailed instruction of adding SDK into your application. You can follow the steps to add SDK.</p> </li> <li> <p>Click the Download the config json file button to download the config file, and keep this file open, which will be used later.</p> </li> </ol> <p>It will take about 3 ~ 5 minutes to update the pipeline with the application you just add. When you see the pipeline status become Active again, it is ready to receive data from your application. </p> <p>We have completed all the steps of adding an application to a project.</p>"},{"location":"getting-started/3.integrate-sdk/#generate-sample-data","title":"Generate sample data","text":"<p>You might not have immediate access to integrate SDK with your app. In this case, we provide a Python script to generate sample data to the pipeline you just configured, so that you can view and experience the analytics dashboards.</p>"},{"location":"getting-started/3.integrate-sdk/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> </ul>"},{"location":"getting-started/3.integrate-sdk/#steps_1","title":"Steps","text":"<ol> <li>Clone the repository to your local environment.    <pre><code>git clone https://github.com/awslabs/clickstream-analytics-on-aws.git\n</code></pre></li> <li> <p>After you cloned the repository, change directory into the <code>examples/standalone-data-generator</code> project folder.</p> </li> <li> <p>Install the dependencies of the project.     <pre><code>pip3 install requests\n</code></pre></p> </li> <li> <p>Put <code>amplifyconfiguration.json</code> into the root of <code>examples/standalone-data-generator</code> which you downloaded in register an app step. See the <code>examples/standalone-data-generator/README.md</code> for more usages.</p> </li> <li> <p>Open an terminal at this project folder location. For example, if you are using Visual Studio Code IDE, at the top of Visual Studio Code, click Terminal -&gt; New Terminal to open a terminal.</p> </li> <li> <p>Copy the following command and paste it to the terminal:</p> <pre><code>python3 create_event.py\n</code></pre> </li> </ol> <p>Hit <code>Enter</code> key in terminal to execute the program. If you see the following output, this means that the program execution is complete.</p> <pre><code>job finished, upload 4360476 events, cost: 95100ms\n</code></pre> <p>This process will take about 10 minutes with default configuration. After job is finished, you can move to next step.</p>"},{"location":"getting-started/3.integrate-sdk/#next","title":"Next","text":"<ul> <li>Analyze data</li> </ul>"},{"location":"getting-started/4.view-dashboard/","title":"Step 4 - Analyze data","text":"<p>After your application sends data (or the sample data are sent) to the pipeline, you can go into the Analytic Studio to view dashboard and query data.</p>"},{"location":"getting-started/4.view-dashboard/#steps","title":"Steps","text":"<ol> <li>Log into Clickstream Analytics on AWS Console.</li> <li>In the left navigation pane, click on Analytics Studio, a new tab should be opened in your browser.</li> <li>In the Analytics Studio page, select the project and app you just created in the drop-down list in the top of the web page.</li> <li>By default, you should be on the Dashboard page, if not, click on the first icon (Dashboard) in the left navigation panel.    </li> <li>Click on the \"User life cycle dashboard - default\", you will be able to see the dashboard created by solution.</li> <li>Click the second icon (Explore) in the left navigation panel, you will be able to query the clickstream data by using the exploratory analytics models.</li> </ol> <p>Congratulations! You have completed the getting started tutorial. You can explore the Analytic Studio or continue to learn more about this solution later.</p>"},{"location":"getting-started/4.view-dashboard/#next","title":"Next","text":"<ul> <li>Pipeline management</li> </ul>"},{"location":"pipeline-mgmt/","title":"Data pipeline","text":"<p>Data pipeline is the core functionality of this solution. In the Clickstream Analytics on AWS solution, we define a data pipeline as a sequence of integrated AWS services that ingest, process, and model the clickstream data into a destination data warehouse for analytics and visualization. It is also designed to efficiently and reliably collect data from your websites and apps to a S3-based data lake, where it can be further processed, analyzed, and utilized for additional use cases (such as real-time monitoring, and personal recommendation).</p>"},{"location":"pipeline-mgmt/#concepts","title":"Concepts","text":"<p>Before creating a data pipeline, you can learn a few concepts in this solution so that you can configure the data pipeline to best fit your business goal.</p>"},{"location":"pipeline-mgmt/#project","title":"Project","text":"<p>A project in this solution is the top-level entity, like a container, that groups your apps and data pipeline for collecting and processing clickstream data. One project contains one data pipeline, and can have one or more apps registered to it.</p>"},{"location":"pipeline-mgmt/#data-pipeline_1","title":"Data pipeline","text":"<p>A data pipeline is deployed into one AWS region, which means all the underlining resources are created in one AWS region. A data pipeline in this solution contains four modules:</p> <ul> <li>Data ingestion: a web service that provides an endpoint to collect data through HTTP requests, and sink the data in a streaming service (e.g., Kafka, Kinesis) or S3.</li> <li>Data processing: a module that transforms raw data to the solution schema and enriches data with additional dimensions.</li> <li>Data modeling: a module that aggregates data to calculate metrics for business analytics.</li> <li>Reporting: a module that creates metrics and out-of-the-box visualizations in QuickSight, and allows users to view dashboards and query clickstream data in Analytics Studio.</li> </ul>"},{"location":"pipeline-mgmt/#app","title":"App","text":"<p>An app in this solution can represent an application in your business, which might be built on one or multiple platforms (for example, Android, iOS, and Web).</p>"},{"location":"pipeline-mgmt/#analytics-studio","title":"Analytics Studio","text":"<p>Analytics Studio is a web console for business or data analysts to view dashboards, query, and manage clickstream data. </p> <p>Below is a diagram to help you better understand those concepts and their relationship with each other in the AWS context.</p> <p></p>"},{"location":"pipeline-mgmt/#prerequisites","title":"Prerequisites","text":"<p>You can configure the pipeline in all AWS regions. For opt-in regions, you need to enable them firstly.</p> <p>Before you start to configure the pipeline in a specific region, make sure you have the following in the target region:</p> <ul> <li>At least one Amazon VPC.</li> <li>At least two public subnets across two AZs in the VPC.</li> <li>At least two private (with NAT gateways or instances) subnets across two AZs, or at least two isolated subnets across two AZs in the VPC. If you want to deploy the solution resources in the isolated subnets, you have to create VPC endpoints for below AWS services,<ul> <li><code>s3</code>, <code>logs</code>, <code>ecr.api</code>, <code>ecr.dkr</code>, <code>ecs</code>, <code>ecs-agent</code>, <code>ecs-telemetry</code>.</li> <li><code>kinesis-streams</code> if you use KDS as sink buffer in ingestion module.</li> <li><code>emr-serverless</code>, <code>glue</code> if you enable data processing module.</li> <li><code>redshift-data</code>, <code>sts</code>, <code>dynamodb</code>, <code>states</code> and <code>lambda</code> if you enable Redshift as analytics engine in data modeling module.</li> </ul> </li> <li>an Amazon S3 bucket located in the same Region.</li> <li>If you need to enable Redshift Serverless as analytics engine in data modeling module, you need have subnets across at least three AZs.</li> <li>QuickSight Enterprise edition subscription is required if the reporting is enable.</li> </ul>"},{"location":"pipeline-mgmt/pipe-mgmt/","title":"Data pipeline maintenance","text":"<p>This solution provides three features to help you manage and maintain the data pipeline after it gets created.</p>"},{"location":"pipeline-mgmt/pipe-mgmt/#monitoring-and-alarms","title":"Monitoring and Alarms","text":"<p>The solution collects metrics from each resource in the data pipeline and creates monitoring dashboards in CloudWatch, which provides you a comprehensive view into the pipeline status. It also provides a set of alarms that will notify project owner if anything goes abnormal. </p> <p>Following are steps to view monitoring dashboards and alarms.</p>"},{"location":"pipeline-mgmt/pipe-mgmt/#monitoring-dashboards","title":"Monitoring dashboards","text":"<p>To view monitoring dashboard for a data pipeline, follows below steps:</p> <ol> <li>Go to project detail page</li> <li>Click on <code>project id</code> or View Details button, which will direct to the pipeline detail page.</li> <li>Click the tab of \"Monitoring\"</li> <li>In the tab, click on the View in CloudWatch button, which will direct you to the monitoring dashboard.</li> </ol>"},{"location":"pipeline-mgmt/pipe-mgmt/#alarms","title":"Alarms","text":"<p>To view alarms for a data pipeline, follows below steps:</p> <ol> <li>Go to project detail page</li> <li>Click on <code>project id</code> or View Details button, which will direct to the pipeline detail page.</li> <li>Click the tab of \"Alarms\"</li> <li>In the tab, you can view all the alarms. You can also click on the View in CloudWatch button, which will direct you to CloudWatch alarm pages to view alarm details.</li> <li>You can also enable or disable an alarm by select the alarm then click on the Enable or Disable buttons.</li> </ol>"},{"location":"pipeline-mgmt/pipe-mgmt/#pipeline-modification","title":"Pipeline modification","text":"<p>You are able to modify some configuration the data pipeline after it created, follow below steps to update a pipeline.</p> <ol> <li>Go to project detail page</li> <li>Click on <code>project id</code> or View Details button, which will direct to the pipeline detail page.</li> <li>In the project details page, click on the Edit button, which will bring you to the pipeline creation wizard page.</li> <li>You will notice some configuration are in disable mode, which means they cannot be updated after creation.</li> <li>For those configuration options are editable, you can update them.</li> <li>After you edit the configuration, click Next until you reach last page, click Save.</li> <li>You will see pipeline is in <code>Updating</code> status.</li> </ol>"},{"location":"pipeline-mgmt/pipe-mgmt/#pipeline-upgrade","title":"Pipeline upgrade","text":"<p>See Upgrade the solution for the detailed procedures.</p>"},{"location":"pipeline-mgmt/data-modeling/configure-data-modeling/","title":"Data modeling settings","text":"<p>Once the data pipeline processes the event data, you can load the data into an analytics engine (i.e., Redshift) for data modeling, where data will be aggregated and organized into different views (such as event, device, session), as well as calculated metrics that are commonly used. Below are the preset data views this solution provides if you choose to enable data modeling module. </p>"},{"location":"pipeline-mgmt/data-modeling/configure-data-modeling/#preset-data-views","title":"Preset data views","text":"Data model name Redshift Description clickstream_device_view_v1 Materialized view A view contains all device dimensions. clickstream_event_view_v1 Materialized view A view contains all event dimensions clickstream_event_parameter_view_v1 Materialized view A view contains all event parameters. clickstream_user_dim_view_v1 Materialized view A view contains all user dimensions. clickstream_user_attr_view_v1 Materialized view A view contains all user custom attributes. clickstream_session_view_v1 Materialized view A view contains all session dimension and relevant metrics, e.g.,session duration, session views. clickstream_retention_view_v1 Materialized view A view contains metrics of retentions by dates and return days. clickstream_lifecycle_daily_view_v1 Materialized view A view contains metrics of user number by lifecycle stages by day, i.e., New, Active, Return, Churn. clickstream_lifecycle_weekly_view_v1 Materialized view A view contains metrics of user number by lifecycle stages by week, i.e., New, Active, Return, Churn. <p>You can choose to use Redshift or Athena, or both. </p> <p>Tip</p> <p>We recommended you select both, that is, using Redshift for hot data modeling and using Athena for all-time data analysis.</p> <p>You can set below configurations for Redshift.  </p> <ul> <li> <p>Redshift Mode: Select Redshift serverless or provisioned mode.</p> <ul> <li> <p>Serverless mode</p> <ul> <li> <p>Base RPU: RPU stands for Redshift Processing Unit. Amazon Redshift Serverless measures data warehouse capacity in RPUs, which are resources used to handle workloads. The base capacity specifies the base data warehouse capacity Amazon Redshift uses to serve queries and is specified in RPUs. Setting higher base capacity improves query performance, especially for data processing jobs that consume a lot of resources.</p> </li> <li> <p>VPC: A virtual private cloud (VPC) based on the Amazon VPC service is your private, logically isolated network in the AWS Cloud.</p> <p>Note: If you place the cluster within the isolated subnets, the VPC must have VPC endpoints for S3, Logs, Dynamodb, STS, States, Redshift and Redshift-data service.</p> </li> <li> <p>Security Group: This VPC security group defines which subnets and IP ranges can access the endpoint of Redshift cluster.</p> </li> <li> <p>Subnets: Select at least three existing VPC subnets.</p> <p>Note: We recommend using private subnets to deploy for following security best practices.</p> </li> </ul> </li> <li> <p>Provisioned mode</p> <ul> <li> <p>Redshift Cluster: With a provisioned Amazon Redshift cluster, you build a cluster with node types that meet your cost and performance specifications. You have to set up, tune, and manage Amazon Redshift provisioned clusters.</p> </li> <li> <p>Database user: The solution needs permissions to access and create database in Redshift cluster. By default, it grants Redshift Data API with the permissions of the admin user to execute the commands to create DB, tables, and views, as well as loading data.</p> </li> </ul> </li> <li> <p>Data range: Considering the cost performance issue of having Redshift to save all the data, we recommend that Redshift save hot data and that all data are stored in S3. It is necessary to delete expired data in Redshift on a regular basis.</p> </li> </ul> </li> <li> <p>Athena: Choose Athena to query all data on S3 using the table created in the Glue Data Catalog.</p> </li> </ul>"},{"location":"pipeline-mgmt/data-processing/","title":"Data Processing","text":"<p>Clickstream Analytics on AWS provides an inbuilt data schema to parse and model the raw event data sent from your web and mobile apps, which makes it easy for you to analyze the data in analytics engines (such as Redshift and Athena). </p> <p>Data Processing module includes two functionalities:</p> <ul> <li>Transformation: Extract the data from files sank by ingestion module, then parse each event data and transform them to solution data model.</li> <li>Enrichment: Add additional dimensions/fields to event data.</li> </ul> <p>This chapter includes:</p> <ul> <li>Data schema</li> <li>Configure execution parameters</li> <li>Configure ETL custom plugins</li> </ul>"},{"location":"pipeline-mgmt/data-processing/configure-execution-para/","title":"Configure Execution Parameters","text":"<p>Execution parameters control how the transformation and enrichment jobs are orchestrated.</p>"},{"location":"pipeline-mgmt/data-processing/configure-execution-para/#parameters","title":"Parameters","text":"<p>You can configure the following Execution parameters after you toggle on Enable data processing.</p> Parameter Description Values Data processing interval/Fixed Rate Specify the interval to batch the data for ETL processing by fixed rate 1 hour 12 hours1 day Data processing interval/Cron Expression Specify the interval to batch the data for ETL processing by cron expression <code>cron(0 * * ? *)</code> <code>cron(0 0,12 * ? *)</code><code>cron(0 0 * ? *)</code> Event freshness Specify the days after which the solution will ignore the event data. For example, if you specify 3 days for this parameter, the solution will ignore any event which arrived more than 3 days after the events are triggered 3 days 5 days 30 days"},{"location":"pipeline-mgmt/data-processing/configure-execution-para/#cron-expression-syntax","title":"Cron Expression Syntax","text":"<p>Syntax</p> <p><code>cron(minutes hours day-of-month month day-of-week year)</code></p> <p>For more information, refer to Cron-based schedules.</p>"},{"location":"pipeline-mgmt/data-processing/configure-execution-para/#config-spark-job-parameters","title":"Config Spark job parameters","text":"<p>By default, the Clickstream pipeline automatically adjusts EMR job parameters based on the dataset volume that requires processing. In most of time, you do not need to adjust the EMR job parameters, but if you want to override the EMR job parameters, you can put <code>spark-config.json</code> file in S3 bucket to set your job parameters.</p> <p>To add your customized the EMR job parameters, you can add a file <code>s3://{PipelineS3Bucket}/{PipelineS3Prefix}{ProjectId}/config/spark-config.json</code> in the S3 bucket.</p> <p>Please replace <code>{PipelineS3Bucket}</code>, <code>{PipelineS3Prefix}</code>, and <code>{ProjectId}</code> with the values of your data pipeline. These values are found in the <code>Clickstream-DataProcessing-&lt;uuid&gt;</code> stack's Parameters.</p> <p>Also, you can get these values by running the below commands,</p> <pre><code>stackNames=$(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --no-paginate  | jq -r '.StackSummaries[].StackName' | grep  Clickstream-DataProcessing  | grep -v Nested)\n\necho -e \"$stackNames\" | while read stackName; do\n    aws cloudformation describe-stacks --stack-name $stackName  | jq '.Stacks[].Parameters' | jq 'map(select(.ParameterKey == \"PipelineS3Bucket\" or .ParameterKey == \"PipelineS3Prefix\" or .ParameterKey == \"ProjectId\"))'\ndone\n</code></pre> <p>Here is an example of the file <code>spark-config.json</code>:</p> <pre><code>{\n   \"sparkConfig\": [\n        \"spark.emr-serverless.executor.disk=200g\",\n        \"spark.executor.instances=16\",\n        \"spark.dynamicAllocation.initialExecutors=16\",\n        \"spark.executor.memory=100g\",\n        \"spark.executor.cores=16\",\n        \"spark.network.timeout=10000000\",\n        \"spark.executor.heartbeatInterval=10000000\",\n        \"spark.shuffle.registration.timeout=120000\",\n        \"spark.shuffle.registration.maxAttempts=5\",\n        \"spark.shuffle.file.buffer=2m\",\n        \"spark.shuffle.unsafe.file.output.buffer=1m\"\n    ],\n    \"inputRePartitions\": 2000\n}\n</code></pre> <p>Please make sure your account has enough emr-serverless quotas, you can view the quotas via emr-serverless-quotas in region us-east-1. For more configurations, please refer to Spark job properties and application worker config.</p>"},{"location":"pipeline-mgmt/data-processing/configure-plugin/","title":"Configure transformation and enrichment plugins","text":"<p>There are two types of plugins: transformer or enrichment.  When choose plugins, you can only have one transformer and zero or multiple enrichment for a pipeline.</p>"},{"location":"pipeline-mgmt/data-processing/configure-plugin/#built-in-plugins","title":"Built-in Plugins","text":"<p>Below plugins are provided by Clickstream Analytics on AWS.</p> Plugin name Type Description UAEnrichment enrichment User-agent enrichment, use <code>ua_parser</code> Java library to enrich <code>User-Agent</code> in the HTTP header to <code>ua_browser</code>,<code>ua_browser_version</code>,<code>ua_os</code>,<code>ua_os_version</code>,<code>ua_device</code> IpEnrichment enrichment IP address enrichment, use GeoLite2 data by MaxMind to enrich <code>IP</code> to <code>city</code>, <code>continent</code>, <code>country</code> <p>The UAEnrichment uses UA Parser to parse user-agent in Http header.</p> <p>The IpEnrichment plugin uses GeoLite2-City data created by MaxMind, available from https://www.maxmind.com.</p>"},{"location":"pipeline-mgmt/data-processing/configure-plugin/#custom-plugins","title":"Custom Plugins","text":"<p>You can add custom plugins to transform raw event data or enrich the data for your need.</p> <p>Note</p> <p>To add custom plugins, you must develop your own plugins firstly, see Develop Custom Plugins</p> <p>You can add your plugins by click Add Plugin button, which will open a new window, in which you can upload your plugins.</p> <ol> <li>Give the plugin Name and Description.</li> <li>Chose Plugin Type,</li> <li>Enrichment: Plugin to add fields into event data collected by SDK (both Clickstream SDK or third-party SDK)</li> <li> <p>Transformation: A plugin used to transform a third-party SDK\u2019s raw data into solution built-in schema</p> </li> <li> <p>Upload plugin java JAR file.</p> </li> <li> <p>(Optional) Upload the dependency files if any.</p> </li> <li> <p>Main function class: fill the full class name of your plugin class name, e.g. <code>com.example.sol.CustomTransformer</code>.</p> </li> </ol>"},{"location":"pipeline-mgmt/data-processing/configure-plugin/#develop-custom-plugins","title":"Develop Custom Plugins","text":"<p>The simplest way to develop custom plugins is making changes based on our example project.</p> <ol> <li>Clone/Fork the example project.</li> </ol> <pre><code>git clone https://github.com/awslabs/clickstream-analytics-on-aws.git\n\ncd examples/custom-plugins\n</code></pre> <ul> <li>For enrichment plugin, please refer to the example: <code>custom-enrich/</code></li> <li> <p>For transformer plugin, please refer to the example: <code>custom-sdk-transformer/</code></p> </li> <li> <p>Change packages and classes name as your desired.</p> </li> <li> <p>Implement the method <code>public Dataset&lt;row&gt; transform(Dataset&lt;row&gt; dataset)</code> to do transformation or enrichment.</p> </li> <li> <p>(Optional) Write test code.</p> </li> <li> <p>Run gradle to package code to jar <code>./gradlew clean build</code>.</p> </li> <li> <p>Get the jar file in build output directory <code>./build/libs/</code>.</p> </li> </ul>"},{"location":"pipeline-mgmt/data-processing/data-schema/","title":"Data schema","text":"<p>This article explains the data schema and format in Clickstream Analytics on AWS. This solution uses an event-based data model to store and analyze clickstream data, every activity (e.g., click, view) on the clients is modeled as an event with multiple dimensions. Dimensions are common for all events, but customers have the flexibility to use JSON object to store values into some dimensions (e.g., event parameters, user attributes), which will cater for the need of collecting information that are specific for their business. Those JSON will be stored in special data types which allow customers to unnest the values in the analytics engines.</p>"},{"location":"pipeline-mgmt/data-processing/data-schema/#database-and-table","title":"Database and table","text":"<p>For each project, the solution creates a database with name of <code>&lt;project-id&gt;</code> in Redshift and Athena. Each App will have a schema with name of <code>app_id</code>, within which event-related data are stored in <code>event</code> and <code>event_parameter</code> tables, user-related data are stored in <code>user</code> table, item-related data are stored in <code>item</code> table. In Athena, all tables are added partitions of app_id, year, month, and day. Below diagram illustrates the table relationship.</p> <p></p>"},{"location":"pipeline-mgmt/data-processing/data-schema/#columns","title":"Columns","text":"<p>Each column in the tables represents a specific parameter for a event, user, or item. Note that some parameters are nested within a Super field in Redshift or Array in Athena, and those fields (e.g., items, user_properties, and item properties) contains parameters that are repeatable. Table columns are described below.</p>"},{"location":"pipeline-mgmt/data-processing/data-schema/#event-table-fields","title":"Event table fields","text":"Field Name Data Type - Redshift Data Type - Athena Description event_id VARCHAR STRING Unique ID for the event. event_date DATE DATE The date when the event was logged (YYYYMMDD format in UTC). event_timestamp BIGINT BIGINT The time (in microseconds, UTC) when the event was logged on the client. event_previous_timestamp BIGINT BIGINT The time (in microseconds, UTC) when the event was previously logged on the client. event_name VARCHAR STRING The name of the event. event_value_in_usd BIGINT BIGINT The currency-converted value (in USD) of the event's \"value\" parameter. event_bundle_sequence_id BIGINT BIGINT The sequential ID of the bundle in which these events were uploaded. ingest_timestamp BIGINT BIGINT Timestamp offset between collection time and upload time in micros. device.mobile_brand_name VARCHAR STRING The device brand name. device.mobile_model_name VARCHAR STRING The device model name. device.manufacturer VARCHAR STRING The device manufacturer name. device.carrier VARCHAR STRING The device network provider name. device.network_type VARCHAR STRING The network_type of the device, e.g., WIFI, 5G device.operating_system VARCHAR STRING The operating system of the device. device.operating_system_version VARCHAR STRING The OS version. device.vendor_id VARCHAR STRING IDFV (present only if IDFA is not collected). device.advertising_id VARCHAR STRING Advertising ID/IDFA. device.system_language VARCHAR STRING The OS language. device.time_zone_offset_seconds BIGINT BIGINT The offset from GMT in seconds. device.ua_browser VARCHAR STRING The browser in which the user viewed content, derived from User Agent string device.ua_browser_version VARCHAR STRING The version of the browser in which the user viewed content, derive from User Agent device.ua_device VARCHAR STRING The device in which user viewed content, derive from User Agent. device.ua_device_category VARCHAR STRING The device category in which user viewed content, derive from User Agent. device.screen_width VARCHAR STRING The screen width of the device. device.screen_height VARCHAR STRING The screen height of the device. geo.continent VARCHAR STRING The continent from which events were reported, based on IP address. geo.sub_continent VARCHAR STRING The subcontinent from which events were reported, based on IP address. geo.country VARCHAR STRING The country from which events were reported, based on IP address. geo.region VARCHAR STRING The region from which events were reported, based on IP address. geo.metro VARCHAR STRING The metro from which events were reported, based on IP address. geo.city VARCHAR STRING The city from which events were reported, based on IP address. geo.locale VARCHAR STRING The locale information obtained from device. traffic_source.name VARCHAR STRING Name of the marketing campaign that acquired the user when the events were reported. traffic_source.medium VARCHAR STRING Name of the medium (paid search, organic search, email, etc.) that  acquired the user when the events were reported. traffic_source.source VARCHAR STRING Name of the network source that acquired the user when the event were reported. app_info.id VARCHAR STRING The package name or bundle ID of the app. app_info.app_id VARCHAR STRING The App ID (created by this solution) associated with the app. app_info.install_source VARCHAR STRING The store that installed the app. app_info.version VARCHAR STRING The app's versionName (Android) or short bundle version. platform VARCHAR STRING The data stream platform (Web, IOS or Android) from which the event originated. project_id VARCHAR STRING The project id associated with the app. items SUPER ARRAY Key-value records contain information about items associated with the event user_id VARCHAR STRING The unique ID assigned to a user through <code>setUserId()</code> API. user_pseudo_id VARCHAR STRING The pseudonymous id generated by SDK for the user."},{"location":"pipeline-mgmt/data-processing/data-schema/#event_parameter-table-fields","title":"Event_parameter table fields","text":"Field Name Data Type - Redshift Data Type - Athena Description event_timestamp BIGINT STRING The time (in microseconds, UTC) when the event was logged on the client. event_id VARCHAR BIGINT Unique ID for the event. event_name VARCHAR STRING The name of the event. event_params_key VARCHAR STRING The name of the event parameter. event_params_string_value VARCHAR STRING If the event parameter is represented by a string, such as a URL or campaign name, it is populated in this field. event_params_int_value BIGINT INTEGER If the event parameter is represented by an integer, it is populated in this field. event_params_double_value DOUBLE PRECISION FLOAT If the event parameter is represented by a double value, it is populated in this field. event_params_float_value DOUBLE PRECISION FLOAT If the event parameter is represented by a floating point value, it is populated in this field. This field is not currently in use."},{"location":"pipeline-mgmt/data-processing/data-schema/#user-table-fields","title":"User table fields","text":"Field name Data type - Redshift Data type - Athena Description event_timestamp BIGINT STRING The timestamp of when the user attributes was collected. user_id VARCHAR STRING The unique ID assigned to a user through <code>setUserId()</code> API. user_pseudo_id VARCHAR STRING The pseudonymous id generated by SDK for the user. user_first_touch_timestamp BIGINT BIGINT The time (in microseconds) at which the user first opened the app or visited the site. user_properties SUPER ARRAY Properties of the user. user_properties.key VARCHAR STRING The name of the user property. user_properties.value SUPER ARRAY A record for the user property value. user_properties.value.string_value VARCHAR STRING The string value of the user property. user_properties.value.int_value BIGINT BIGINT The integer value of the user property. user_properties.value.double_value DOUBLE PRECISION FLOAT The double value of the user property. user_properties.value.float_value DOUBLE PRECISION FLOAT This field is currently unused. user_ltv SUPER ARRAY The Lifetime Value of the user. _first_visit_date Date Date Date of the user's first visit _first_referer VARCHAR STRING The first referer detected for the user _first_traffic_source_type VARCHAR STRING The the network source that acquired the user that was first detected for the user, e.g., Google, Baidu _first_traffic_source_medium VARCHAR STRING The medium of the network source that acquired the user that was first detected for the user, e.g., paid search, organic search, email, etc. _first_traffic_source_name VARCHAR STRING The name of the marketing campaign that acquired the user that was first detected for the user. device_id_list SUPER ARRAY A record of all device_id associated with the user_pseudo_id _channel VARCHAR STRING The install channel for the user, e.g., Google Play"},{"location":"pipeline-mgmt/data-processing/data-schema/#item-table-fields","title":"Item table fields","text":"Field name Data type - RedShift Data type - Athena Description event_timestamp BIGINT STRING The timestamp of when the item attributes was collected. id VARCHAR STRING The id for the item properties SUPER ARRAY A record for the item property value. properties.value.string_value VARCHAR STRING The string value of the item property. properties.value.int_value BIGINT BIGINT The integer value of the item property. properties.value.double_value DOUBLE PRECISION FLOAT The double value of the item property. properties.value.float_value DOUBLE PRECISION FLOAT The float_value of the item property."},{"location":"pipeline-mgmt/ingestion/","title":"Data ingestion","text":"<p>Data ingestion module contains a web service that provides an endpoint to collect data through HTTP/HTTPS requests, which mainly is composed of Amazon Application Load Balancer and Amazon Elastic Container Service. It also supports sinking data into a stream service or S3 directly. </p> <p>You can create a data ingestion module with the following settings:</p> <ul> <li> <p>Ingestion endpoint settings: Create a web service as an ingestion endpoint to collect data sent from your SDKs.</p> </li> <li> <p>Data sink settings: Configure how the solution sinks the data for downstream consumption. Currently, the solution supports three types of data sink:</p> <ul> <li>Apache Kafka</li> <li>Amazon S3</li> <li>Amazon Kinesis Data Stream (KDS)</li> </ul> </li> </ul>"},{"location":"pipeline-mgmt/ingestion/#throttle","title":"Throttle","text":"<p>Currently, there is no built-in throttling feature available with this solution. If you require throttling feature, you can configure AWS WAF to implement throttling feature. Please refer WAF document.</p>"},{"location":"pipeline-mgmt/ingestion/configure-ingestion-endpoint/","title":"Ingestion endpoint settings","text":"<p>The solution creates a web service as an ingestion endpoint to collect data sent from your SDKs. You can set below configurations for ingestion endpoint.</p> <ul> <li> <p>Public Subnets: select at least two existing VPC public subnets, and the Amazon Application Load Balancers (ALBs) will be deployed in these subnets.</p> </li> <li> <p>Private Subnets: select at least two existing VPC private subnets, and the EC2 instances running in ECS will be deployed in these subnets.</p> <p>Tip</p> <p>The availability zones where the public subnets are located must be consistent with those of the private subnets.</p> </li> <li> <p>Ingestion capacity: This configuration sets the capacity of the ingestion server, and the ingestion server will automatically scale up or down based on the utilization of the processing CPU.</p> <ul> <li>Ingestion Capacity Unit (ICU): A single Ingestion Compute Unit (ICU) represents billable compute and memory units, approximately 8 gigabytes (GB) of memory and 2 vCPUs. 1 ICU generally can support 4000~6000 requests per second.</li> <li>Minimum capacity: The minimum capacity to which the ingestion server will scale down.</li> <li>Maximum capacity: The maximum capacity to which the ingestion server will scale up.</li> <li>Warm pool: Warm pool gives you the ability to decrease latency of ingestion service scale up. For more information, please refer to Warm pools for Amazon EC2 Auto Scaling.</li> </ul> </li> <li> <p>Enable HTTPS: Users can choose HTTPS/HTTP protocol for the Ingestion endpoint.</p> <ul> <li> <p>Enable HTTPS: If users choose to enable HTTPS, the ingestion server will provide HTTPS endpoint. </p> <ul> <li>Domain name: input a domain name.  </li> </ul> <p>Note</p> <p>Once the ingestion server is created, use the custom endpoint to create an alias or CNAME mapping in your Domain Name System (DNS) for the custom endpoint.</p> <ul> <li>SSL Certificate: User need to select an ACM certificate corresponding to the domain name that you input. If there is no ACM certificate, please refer create public certificate to create it.<ul> <li>Disable HTTPS: If users choose to disable HTTPS, the ingestion server will provide HTTP endpoint.</li> </ul> </li> </ul> <p>Warning</p> <p>DO NOT use HTTP in production, because data will be sent without any encryption, and there are high risks of data being leaked or tampered during transmission. Please acknowledge the risk to proceed.</p> <ul> <li>Cross-Origin Resource Sharing (CORS): You can enable CORS to limit requests to data ingestion API from a specific domain. Note that, you need to input a complete internet address, e.g., <code>https://www.example.com</code>, <code>http://localhost:8080</code>. Use comma to separate domain if you have multiple domain for this setting.</li> </ul> <p>Warning</p> <p>CORS is a mandatory setting if you are collecting data from a website. If you do not set value for this parameter, the ingestion server to reject all the requests from Web platform.</p> </li> </ul> </li> <li> <p>Additional Settings</p> <ul> <li>Request path: User can input the path of ingestion endpoint to collect data, the default path is \"/collect\".</li> <li>AWS Global Accelerator: User can choose to create an accelerator to get static IP addresses that act as a global fixed entry point to your ingestion server, which will improves the availability and performance of your ingestion server.    Note That additional charges apply.</li> <li> <p>Authentication: User can use OIDC provider to authenticate the request sent to your ingestion server. If you plan to enable it, please create an OIDC client in the OIDC provider then create a secret in AWS Secret Manager with information:</p> <ul> <li>issuer</li> <li>token endpoint</li> <li>User endpoint</li> <li>Authorization endpoint</li> <li>App client ID</li> <li>App Client Secret</li> </ul> <p>The format is like: <pre><code>  {\n    \"issuer\":\"xxx\",\n    \"userEndpoint\":\"xxx\",\n    \"authorizationEndpoint\":\"xxx\",\n    \"tokenEndpoint\":\"xxx\",\n    \"appClientId\":\"xxx\",\n    \"appClientSecret\":\"xxx\"\n  }\n</code></pre> Note: In the OIDC provider, you need to add <code>https://&lt;ingestion server endpoint&gt;/oauth2/idpresponse</code> to \"Allowed callback URLs\"</p> <p>Note: If you need to obtain the authentication token directly without inputting credential(username/password) manually, you can refer to alb headless authentication client code to setup your client to obtain the authentication token automatically.</p> </li> <li> <p>Access logs: ALB supports delivering detailed logs of all requests it receives. If you enable this option, the solution will automatically enable access logs for you and store the logs into the S3 bucket you selected in previous step.</p> <p>Tip</p> <p>The bucket must have a bucket policy that grants Elastic Load Balancing permission to write to the bucket.</p> <p>Below is an example policy for the bucket in regions available before August 2022,</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::&lt;elb-account-id&gt;:root\"\n      },\n      \"Action\": \"s3:PutObject\",\n      \"Resource\": \"arn:aws:s3:::&lt;BUCKET&gt;/clickstream/*\"\n    }\n  ]\n}\n</code></pre> <p>Replace <code>elb-account-id</code> with the ID of the AWS account for Elastic Load Balancing for your Region:</p> <ul> <li>US East (N. Virginia) \u2013 127311923021</li> <li>US East (Ohio) \u2013 033677994240</li> <li>US West (N. California) \u2013 027434742980</li> <li>US West (Oregon) \u2013 797873946194</li> <li>Africa (Cape Town) \u2013 098369216593</li> <li>Asia Pacific (Hong Kong) \u2013 754344448648</li> <li>Asia Pacific (Jakarta) \u2013 589379963580</li> <li>Asia Pacific (Mumbai) \u2013 718504428378</li> <li>Asia Pacific (Osaka) \u2013 383597477331</li> <li>Asia Pacific (Seoul) \u2013 600734575887</li> <li>Asia Pacific (Singapore) \u2013 114774131450</li> <li>Asia Pacific (Sydney) \u2013 783225319266</li> <li>Asia Pacific (Tokyo) \u2013 582318560864</li> <li>Canada (Central) \u2013 985666609251</li> <li>Europe (Frankfurt) \u2013 054676820928</li> <li>Europe (Ireland) \u2013 156460612806</li> <li>Europe (London) \u2013 652711504416</li> <li>Europe (Milan) \u2013 635631232127</li> <li>Europe (Paris) \u2013 009996457667</li> <li>Europe (Stockholm) \u2013 897822967062</li> <li>Middle East (Bahrain) \u2013 076674570225</li> <li>South America (S\u00e3o Paulo) \u2013 507241528517</li> <li>China (Beijing) \u2013 638102146993</li> <li>China (Ningxia) \u2013 037604701340</li> </ul> </li> </ul> </li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/","title":"Apache Kafka","text":"<p>This data sink will stream the clickstream data collected by the ingestion endpoint into an topic in a Kafka cluster. Currently, solution support Amazon Managed Streaming for Apache Kafka (Amazon MSK) and Self-hosted Kafka cluster.</p>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/#amazon-msk","title":"Amazon MSK","text":"<ul> <li> <p>Select an existing Amazon MSK cluster. Select an MSK cluster from the drop-down list, the MSK cluster needs to meet the following requirements:</p> <ul> <li>MSK cluster and this solution need to be in the same VPC</li> <li>Enable Unauthenticated access in Access control methods</li> <li>Enable Plaintext in Encryption</li> <li>Set auto.create.topics.enable as <code>true</code> in MSK cluster configuration. This configuration sets whether MSK cluster can create topic automatically. Or You need to create the specific topic in your Kafka cluster before creating the data pipeline.</li> <li>The value of default.replication.factor cannot be larger than the number of MKS cluster brokers</li> </ul> <p>Note: If there is no MSK cluster, the user needs to create an MSK Cluster follow above requirements.</p> </li> <li> <p>Topic: The user can specify a topic name. By default, the solution will create a topic with \"project-id\".</p> </li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/#self-hosted-kafka","title":"Self-hosted Kafka","text":"<p>Users can also use self-hosted Kafka clusters. In order to integrate the solution with Kafka clusters, provide the following configurations:</p> <ul> <li> <p>Broker link: Enter the brokers link of Kafka cluster that you wish to connect to, the Kafka cluster needs to meet the following requirements:</p> <ul> <li>The Kafka cluster and this solution need to be in the same VPC</li> <li>The number of Kafka cluster brokers cannot be less than two</li> </ul> </li> <li> <p>Topic: User can specify the topic for storing the data. </p> </li> <li>Security Group: This VPC security group defines which subnets and IP ranges can access the Kafka cluster.</li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/#connector","title":"Connector","text":"<p>Enable solution to create Kafka connector and a custom plugin for this connector. This connector will sink the data from Kafka cluster to S3 bucket.</p>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/#additional-settings","title":"Additional Settings:","text":"<ul> <li>Sink maximum interval: Specifies the maximum length of time (in seconds) that records should be buffered before streaming to the AWS service.</li> <li>Batch size: The maximum number of records to deliver in a single batch.</li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kinesis/","title":"Amazon Kinesis Data Stream","text":"<p>This data sink will stream the clickstream data collected by the ingestion endpoint into KDS. The solution will create a KDS in your AWS account based on your specifications.</p>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kinesis/#provision-mode","title":"Provision mode","text":"<p>Two modes are available: On-demand and Provisioned</p> <ul> <li> <p>On-demand: In this mode, KDS shards are provisioned based on the workshop automatically. On-demand mode is suited for workloads with unpredictable and highly-variable traffic patterns. </p> </li> <li> <p>Provisioned: In this mode, KDS shards are set at creation. The provisioned mode is suited for predictable traffic with capacity requirements that are easy to forecast. You can also use the provisioned mode if you want fine-grained control over how data is distributed across shards. </p> <ul> <li>Shard number: With the provisioned mode, you must specify the number of shards for the data stream.  For more information about shard, please refer to provisioned mode</li> </ul> </li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kinesis/#addtional-settings","title":"Addtional Settings","text":"<ul> <li>Sink maximum interval: With this configuration, you can specify the maximum interval (in seconds) that records should be buffered before streaming to the AWS service.</li> <li>Batch size: With this configuration, you can specify the maximum number of records to deliver in a single batch.</li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-s3/","title":"Amazon S3","text":"<p>In this option, clickstream data is buffered in the memory of ingestion Server, then sink into a S3 bucket. This option provides the best cost-performance in case real-time data consumption is not required. </p> <p>Note</p> <p>Unlike Kafka and KDS data sink, this option buffers data in the ingestion server and responses 200 code to SDK client before sink into S3, so there is chance data could be lost while ingestion server fails and auto-scaled machine is in the process of creation. But it is worth to note that this probability is very low because of the High-availability design of the solution.</p> <ul> <li>Buffer size: Specify the data size to buffer before sending to Amazon S3. The higher buffer size may be lower in cost with higher latency, the lower buffer size will be faster in delivery with higher cost. Min: 1 MiB, Max: 50 MiB</li> <li>Buffer interval: Specify the max interval (second) for saving buffer to S3. The higher interval allows more time to collect data and the size of data may be bigger. The lower interval sends the data more frequently and may be more advantageous when looking at shorter cycles of data activity. Min: 60 Second, Max: 3600 Second</li> </ul>"},{"location":"pipeline-mgmt/quicksight/configure-quicksight/","title":"Configure Reporting","text":""},{"location":"pipeline-mgmt/quicksight/configure-quicksight/#reporting","title":"Reporting","text":"<p>Once the data are processed and modeled by the data pipeline, you can enable the Analytics Studio for the pipeline, which will allow the solution create out-of-the-box dashboards in QuickSight, provide advanced analytics model for user to query their clickstream data, and data management functionalities.</p> <p>Note</p> <p>To enable this module, your AWS account needs to have subscription in QuickSight. If it hasn't, please follow this sign up for Amazon QuickSight to create a subscription first.</p>"},{"location":"plan-deployment/cost/","title":"Cost","text":"<p>Important</p> <p>The following cost estimations are examples and may vary depending on your environment.</p> <p>You are responsible for the cost of AWS services used when running this solution. Deploying this solution will only create a solution web console in your AWS account, which is completely serverless and typically can be covered within free tier.</p> <p>The majority of the cost for this solution is incurred by the data pipeline. As of this revision, the main factors affecting the solution cost include:</p> <ul> <li> <p>Ingestion module: the cost depends on the size of the ingestion server and the type of the data sink you choose.</p> </li> <li> <p>Data processing and modeling module (optional): the cost depends on whether you choose to enabled this module and its relevant configurations</p> </li> <li> <p>Enabled Dashboards (optional): the cost depends on whether you choose to enabled this module and its relevant configurations</p> </li> <li> <p>Additional features</p> </li> </ul> <p>The following are cost estimations for monthly data volumes of 10/100/1000 RPS (request per second) with different data pipeline configurations. Cost estimation are provided by modules. To get a total cost for your use case, sum the cost by modules based on your actual configuration.</p> <p>Important</p> <p>As of this revision, the following cost is calculated with <code>On-Demand</code> prices in <code>us-east-1</code> region measured in USD.</p>"},{"location":"plan-deployment/cost/#ingestion-module","title":"Ingestion module","text":"<p>Ingestion module includes the following cost components:</p> <ul> <li>Application load balancer</li> <li>EC2 for ECS</li> <li>Data sink (Kinesis Data Streams | Kafka | Direct to S3)</li> <li>S3 storage</li> </ul> <p>Key assumptions include:</p> <ul> <li>Compressed request payload: 2KB (10 events per request)</li> <li>MSK configurations (m5.large * 2)</li> <li>KDS configuration (on-demand, provision)</li> <li>10/100/1000RPS</li> </ul> Request Per Second ALB cost EC2 cost Buffer type Buffer cost S3 cost Total (USD/Month) 10RPS (49GB/month) $18 $122 Kinesis (On-Demand) $38 $3 $181 $18 $122 Kinesis (Provisioned 2 shard) $22 $3 $165 $18 $122 MSK (m5.large * 2, connector MCU * 1) $417 $3 $560 $18 $122 None $3 $143 100RPS (490GB/month) $43 $122 Kinesis(On-demand) $115 $4 $284 $43 $122 Kinesis (Provisioned 2 shard) $26 $4 $195 $43 $122 MSK (m5.large * 2, connector MCU * 1) $417 $4 $586 $43 $122 None $4 $169 1000RPS (4900GB/month) $252 $122 Kinesis(On-demand) $1051 $14 $1439 $252 $122 Kinesis (Provisioned 10 shard) $180 $14 $568 $252 $122 MSK (m5.large * 2, connector MCU * 2~3) $590 $14 $978 $252 $122 None $14 $388"},{"location":"plan-deployment/cost/#data-transfer","title":"Data transfer","text":"<p>There are associated costs when data is transferred from EC2 to the downstream data sink. Below is an example of data transfer costs based on 1000 RPS and a 2KB request payload.</p> <ol> <li>EC2 Network In: This does not incur any costs.</li> <li> <p>EC2 Network Out: There are three data sink options:</p> Data Sink Type Way to access data sink Dimensions Total (USD/Month) S3 S3 Gateway endpoints The S3 Gateway endpoints does not incur any costs $0 MSK Data processed cost ($0.010 per GB in/out/between EC2 AZs) $210 KDS NAT NAT fixed cost: $64 (2 Availability Zones and a NAT per AZ, $0.045 per NAT Gateway Hour).  Data processed cost: $1201 ($0.045 per GB Data Processed by NAT Gateways). $1266 KDS VPC Endpoint VPC Endpoint fixed cost: $14.62 (Availability Zones $0.01 per AZ Hour).  Data processed cost: $267 ($0.01 per GB Data Processed by Interface endpoints). $281.62 <p>We suggest using a VPC endpoint for the KDS data sink. For more information on using the VPC endpoint, please refer to the VPC endpoint documentation. </p> </li> </ol>"},{"location":"plan-deployment/cost/#data-processing-data-modeling-modules","title":"Data processing &amp; data modeling modules","text":"<p>Data processing &amp; modeling module include the following cost components if you choose to enable:</p> <ul> <li> <p>EMR Serverless</p> </li> <li> <p>Redshift</p> </li> </ul> <p>Key assumptions include:</p> <ul> <li>10/100/1000 RPS</li> <li>Data processing interval: hourly/6-hourly/daily</li> <li>EMR running three built-in plugins to process data</li> </ul> Request Per Second EMR schedule interval EMR Cost Redshift type Redshift cost Total (USD/Month) 10RPS Hourly $61 ($1.24/GB) Serverless (8 based RPU) $104 $165 6-hourly $40.9 ($0.83/GB) Serverless(8 based RPU) $16 $56.9 Daily $34.3 ($0.7/GB) Serverless(8 based RPU) $11 $45.3 100RPS Hourly $403 ($0.82/GB) Serverless (8 based RPU) $170 $573 6-hourly $192 ($0.39/GB) Serverless(8 based RPU) $119 $311 Daily $245 ($0.5/GB) Serverless(8 based RPU) $78 $323 1000RPS Hourly $2815 ($0.57/GB) Serverless (32 based RPU) $668 $3483 8-Hourly $2604 ($0.53/GB) Serverless (32 based RPU) $359 $2963"},{"location":"plan-deployment/cost/#reporting-module","title":"Reporting module","text":"<p>Reporting module include the following cost components if you choose to enable:</p> <ul> <li>QuickSight</li> </ul> <p>Key assumptions include</p> <ul> <li>QuickSight Enterprise subscription</li> <li>Exclude Q cost</li> <li>Access through Analytics Studio</li> <li>Two authors with monthly subscription</li> <li>10GB SPICE capacity</li> </ul> Daily data volume/RPS Authors SPICE Total cost (USD/Month) All size $48 0 $48 <p>Note</p> <p>All your data pipelines are applied to the above QuickSight costs, even the visualizations managed outside the solution.</p>"},{"location":"plan-deployment/cost/#logs-and-monitoring","title":"Logs and Monitoring","text":"<p>The solution utilizes CloudWatch Logs\uff0c CloudWatch Metrics and CloudWatch Dashboard to implement logging, monitoring and visualizating features. The total cost is around $14 per month and may fluctuate based on the volume of logs and the number of metrics being monitored.</p>"},{"location":"plan-deployment/cost/#additional-features","title":"Additional features","text":"<p>You will be charged with additional cost only if you choose to enable the following features.</p>"},{"location":"plan-deployment/cost/#secrets-manager","title":"Secrets Manager","text":"<ul> <li> <p>If you enable reporting, the solution creates a secret in Secrets Manager to store the Redshift credentials used by QuickSight visualization. Cost: 0.4 USD/month.</p> </li> <li> <p>If you enable the authentication feature of the ingestion module, you need to create a secret in Secrets Manager to store the information for OIDC. Cost: 0.4 USD/month.</p> </li> </ul>"},{"location":"plan-deployment/cost/#amazon-global-accelerator","title":"Amazon Global Accelerator","text":"<p>It incurs a fixed hourly charge and a per-day volume data transfer cost.</p> <p>Key assumptions:</p> <ul> <li>Ingestion deployment in <code>us-east-1</code></li> </ul> Request Per Second Fixed hourly cost Data transfer cost Total cost (USD/Month) 10RPS $18 $0.6 $18.6 100RPS $18 $6 $24 1000RPS $18 $60 $78"},{"location":"plan-deployment/cost/#application-load-balancer-access-log","title":"Application Load Balancer Access log","text":"<p>You are charged storage costs for Amazon S3, but not charged for the bandwidth used by Elastic Load Balancing to send log files to Amazon S3. For more information about storage costs, see\u00a0Amazon S3 pricing.</p> Request Per Second Log size(GB) S3 cost(USD/Month) 10 RPS 16.5 $0.38 100 RPS 165 $3.8 1000 RPS 1650 $38"},{"location":"plan-deployment/regions/","title":"Supported AWS Regions","text":""},{"location":"plan-deployment/regions/#regional-deployments","title":"Regional deployments","text":"<p>This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List.</p> <p>Clickstream Analytics on AWS provides two types of authentication for its web console, Cognito User Pool and OpenID Connect (OIDC) Provider. You must choose to launch the solution with OpenID Connect in case one of the following scenarios:</p> <ul> <li>Cognito User Pool is not available in your AWS Region.</li> <li>You already have an OpenID Connect Provider and want to authenticate against it.</li> </ul> <p>Supported regions for web console deployment</p> Region Name Launch with Cognito User Pool Launch with OpenID Connect US East (N. Virginia) US East (Ohio) US West (N. California) US West (Oregon) Africa (Cape Town) Asia Pacific (Hong Kong) Asia Pacific (Mumbai) Asia Pacific (Osaka) Asia Pacific (Seoul) Asia Pacific (Singapore) Asia Pacific (Sydney) Asia Pacific (Tokyo) Canada (Central) Europe (Frankfurt) Europe (Ireland) Europe (London) Europe (Milan) Europe (Paris) Europe (Stockholm) Middle East (Bahrain) South America (Sao Paulo) China (Beijing) Region Operated by Sinnet China (Ningxia) Regions operated by NWCD <p>This solution provides modular components for supporting different data pipeline architecture. The data processing, data modeling and reporting modules are optional, that is, you can create a data pipeline without data processing, data modeling and reporting modules if needed.</p> <p>Pipeline modules availability</p> Region Name Data ingestion with MSK as buffer Data ingestion with KDS as buffer Data ingestion with S3 as buffer Data processing Data modeling with Redshift Serverless Data modeling with Provisioned Redshift Reporting with QuickSight US East (N. Virginia) US East (Ohio) US West (N. California) US West (Oregon) Africa (Cape Town) Asia Pacific (Hong Kong) Asia Pacific (Mumbai) Asia Pacific (Osaka) Asia Pacific (Seoul) Asia Pacific (Singapore) Asia Pacific (Sydney) Asia Pacific (Tokyo) Canada (Central) Europe (Frankfurt) Europe (Ireland) Europe (London) Europe (Milan) Europe (Paris) Europe (Stockholm) Middle East (Bahrain) South America (Sao Paulo) China (Beijing) Region Operated by Sinnet* China (Ningxia) Region Operated by NWCD* <p>Note(*)</p> <p>AWS China Regions don't support using AWS Global Accelerator to accelerate the ingestion endpoint.</p>"},{"location":"plan-deployment/security/","title":"Security","text":"<p>When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared responsibility model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, see AWS Cloud Security.</p>"},{"location":"plan-deployment/security/#iam-roles","title":"IAM Roles","text":"<p>AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions, Amazon API Gateway and Amazon Cognito or OpenID connect access to create regional resources.</p>"},{"location":"plan-deployment/security/#amazon-vpc","title":"Amazon VPC","text":"<p>This solution optionally deploys a web console within your VPC. You can isolate access to the web console via Bastion hosts, VPNs, or Direct Connect. You can create VPC endpoints to let traffic between your Amazon VPC and AWS services not leave the Amazon network to satisfy the compliance requirements.</p>"},{"location":"plan-deployment/security/#security-groups","title":"Security Groups","text":"<p>The security groups created in this solution are designed to control and isolate network traffic between the solution components. We recommend that you review the security groups and further restrict access as needed once the deployment is up and running.</p>"},{"location":"plan-deployment/security/#amazon-cloudfront","title":"Amazon CloudFront","text":"<p>This solution optionally deploys a web console hosted in an Amazon S3 bucket and Amazon API Gateway. To help reduce latency and improve security, this solution includes an Amazon CloudFront distribution with an Origin Access Control (OAC), which is a CloudFront user that provides public access to the solution\u2019s website bucket contents. For more information, refer to Restricting access to an Amazon S3 origin in the Amazon CloudFront Developer Guide.</p>"},{"location":"resources/upload-ssl-certificate/","title":"Upload SSL Certificate to IAM","text":"<p>Upload the SSL certificate by running the AWS CLI command <code>upload-server-certificate</code> similar to the following:</p> <pre><code>aws iam upload-server-certificate --path /cloudfront/ \\\n--server-certificate-name YourCertificate \\\n--certificate-body file://Certificate.pem \\\n--certificate-chain file://CertificateChain.pem \\\n--private-key file://PrivateKey.pem\n</code></pre> <p>Replace the file names and YourCertificate with the names for your uploaded files and certificate. You must specify the <code>file://</code> prefix in the certificate-body, certificate-chain and private-key parameters in the API request.  Otherwise, the request fails with a <code>MalformedCertificate: Unknown</code> error message.</p> <p>Note</p> <p>You must specify a path using the --path option. The path must begin with /cloudfront and must include a trailing slash (for example, /cloudfront/test/).</p> <p>After the certificate is uploaded, the AWS command <code>upload-server-certificate</code> returns metadata for the uploaded certificate, including the certificate's Amazon Resource Name (ARN), friendly name, identifier (ID), and expiration date.</p> <p>To view the uploaded certificate, run the AWS CLI command <code>list-server-certificates</code>:</p> <pre><code>aws iam list-server-certificates\n</code></pre> <p>For more information, see uploading a server certificate to IAM.</p>"},{"location":"sdk-manual/","title":"Overview","text":"<p>Clickstream Analytics on AWS provides different client-side SDKs, which can make it easier for you to report click stream data to the data pipeline created in the solution. Currently, the solution supports the following platforms:</p> <ul> <li>Android </li> <li>Swift </li> <li>Web</li> <li>Flutter</li> <li>WeChat Miniprogram</li> </ul> <p>In addition, we also provide HTTP API to collect clickstream data from other platforms (e.g., server) through http request.</p>"},{"location":"sdk-manual/#key-features-and-benefits","title":"Key features and benefits","text":"<ul> <li>Automatic data collection. Clickstream SDKs provide built-in capabilities to automatically collect common events, such as screen view or page view, session, and user engagement, so that you only need to focus on recording business-specific events.</li> <li>Ease of use. Clickstream SDKs provide multiple APIs and configuration options to simplify the event reporting and attribute setting operation.</li> <li>Cross-platform analytics. Clickstream SDKs are consistent in event data structure, attribute validation rules, and event sending mechanism, so that data can be normalized in the same structure for cross-platform analytics.</li> </ul> <p>Note</p> <p>All Clickstream SDKs are open source under Apache 2.0 License in GitHub. You can customize the SDKs if needed. All contributions are welcome.</p>"},{"location":"sdk-manual/android/","title":"Clickstream Android SDK","text":""},{"location":"sdk-manual/android/#introduction","title":"Introduction","text":"<p>Clickstream Android SDK can help you easily collect in-app click stream data from Android devices to your AWS environments through the data pipeline provisioned by this solution.</p> <p>The SDK is based on the Amplify for Android SDK Core Library and developed according to the Amplify Android SDK plug-in specification. In addition, the SDK provides features that automatically collect common user events and attributes (for example, screen view and first open) to accelerate data collection for users.</p>"},{"location":"sdk-manual/android/#platform-support","title":"Platform Support","text":"<p>Clickstream Android SDK supports Android 4.1 (API level 16) and later. </p>"},{"location":"sdk-manual/android/#integrate-the-sdk","title":"Integrate the SDK","text":""},{"location":"sdk-manual/android/#1-include-the-sdk","title":"1. Include the SDK","text":"<p>Add the Clickstream SDK dependency to your <code>app</code> module's <code>build.gradle</code> file, for example:</p> <pre><code>dependencies {\n    implementation 'software.aws.solution:clickstream:0.9.0'\n}\n</code></pre> <p>You can synchronize your project with the latest version:  </p>"},{"location":"sdk-manual/android/#2-configure-parameters","title":"2. Configure parameters","text":"<p>Find the <code>res</code> directory under your <code>project/app/src/main</code>, and manually create a raw folder in the <code>res</code> directory. </p> <p> </p> <p>Download your <code>amplifyconfiguration.json</code> file from your clickstream web console, and paste it to the raw folder. The JSON file is like:</p> <pre><code>{\n  \"analytics\": {\n    \"plugins\": {\n      \"awsClickstreamPlugin\": {\n        \"appId\": \"your appId\",\n        \"endpoint\": \"https://example.com/collect\",\n        \"isCompressEvents\": true,\n        \"autoFlushEventsInterval\": 10000,\n        \"isTrackAppExceptionEvents\": false\n      }\n    }\n  }\n}\n</code></pre> <p>In the file, your <code>appId</code> and <code>endpoint</code> are already set up in it. The explanation for each property is as follows:</p> <ul> <li>appId (Required): the app id of your project in web console.</li> <li>endpoint (Required): the endpoint url you will upload the event to AWS server.</li> <li>isCompressEvents: whether to compress event content when uploading events, and the default value is <code>true</code></li> <li>autoFlushEventsInterval: event sending interval, and the default value is <code>10s</code></li> <li>isTrackAppExceptionEvents: whether auto track exception event in app, and the default value is <code>false</code></li> </ul>"},{"location":"sdk-manual/android/#3-initialize-the-sdk","title":"3. Initialize the SDK","text":"<p>Initialize the SDK in the application <code>onCreate()</code> method.</p> <pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\n\npublic void onCreate() {\n    super.onCreate();\n\n    try{\n        ClickstreamAnalytics.init(getApplicationContext());\n        Log.i(\"MyApp\", \"Initialized ClickstreamAnalytics\");\n    } catch (AmplifyException error){\n        Log.e(\"MyApp\", \"Could not initialize ClickstreamAnalytics\", error);\n    } \n}\n</code></pre>"},{"location":"sdk-manual/android/#4-start-using","title":"4. Start using","text":""},{"location":"sdk-manual/android/#record-event","title":"Record event","text":"<p>Add the following code where you need to record event.</p> <pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\nimport software.aws.solution.clickstream.ClickstreamEvent;\n\n// for record an event with custom attributes\nClickstreamEvent event = ClickstreamEvent.builder()\n    .name(\"button_click\")\n    .add(\"category\", \"shoes\")\n    .add(\"currency\", \"CNY\")\n    .add(\"value\", 279.9)\n    .build();\nClickstreamAnalytics.recordEvent(event);\n\n// for record an event directly\nClickstreamAnalytics.recordEvent(\"button_click\");\n</code></pre>"},{"location":"sdk-manual/android/#add-global-attribute","title":"Add global attribute","text":"<pre><code>import software.aws.solution.clickstream.ClickstreamAttribute;\nimport software.aws.solution.clickstream.ClickstreamAnalytics;\n\nClickstreamAttribute globalAttribute = ClickstreamAttribute.builder()\n    .add(\"channel\", \"Play Store\")\n    .add(\"level\", 5.1)\n    .add(\"class\", 6)\n    .add(\"isOpenNotification\", true)\n    .build();\nClickstreamAnalytics.addGlobalAttributes(globalAttribute);\n\n// for delete an global attribute\nClickstreamAnalytics.deleteGlobalAttributes(\"level\");\n</code></pre> <p>Please add the global attribute after the SDK initialization is completed, the global attribute will be added to the attribute object in all events.</p>"},{"location":"sdk-manual/android/#login-and-logout","title":"Login and logout","text":"<pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\n\n// when user login success\nClickstreamAnalytics.setUserId(\"UserId\");\n\n// when user logout\nClickstreamAnalytics.setUserId(null);\n</code></pre>"},{"location":"sdk-manual/android/#add-user-attribute","title":"Add user attribute","text":"<pre><code>import software.aws.solution.clickstream.ClickstreamAnalytcs;\nimport software.aws.solution.clickstream.ClickstreamUserAttribute;\n\nClickstreamUserAttribute clickstreamUserAttribute = ClickstreamUserAttribute.builder()\n    .add(\"_user_age\", 21)\n    .add(\"_user_name\", \"carl\")\n    .build();\nClickstreamAnalytics.addUserAttributes(clickstreamUserAttribute);\n</code></pre> <p>Current login user's attributes will be cached in disk, so the next time app launch you don't need to set all user's attribute again, of course you can use the same api <code>ClickstreamAnalytics.addUserAttributes()</code> to update the current user's attribute when it changes.</p> <p>Important</p> <p>If your application is already published and most users have already logged in, please manually set the user attributes once when integrate the Clickstream SDK for the first time to ensure that subsequent events contains user attributes.</p>"},{"location":"sdk-manual/android/#send-event-immediately","title":"Send event immediately","text":"<pre><code>// for send event immediately.\nClickstreamAnalytics.flushEvent();\n</code></pre>"},{"location":"sdk-manual/android/#disable-sdk","title":"Disable SDK","text":"<p>You can disable the SDK in the scenario you need. After disabling the SDK, the SDK will not handle the logging and sending of any events. Of course you can enable the SDK when you need to continue logging events.</p> <p>Please note that the disable and enable code needs to be run in the main thread.</p> <pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\n\n// disable SDK\nClickstreamAnalytics.disable();\n\n// enable SDK\nClickstreamAnalytics.enable();\n</code></pre>"},{"location":"sdk-manual/android/#configuration-update","title":"Configuration update","text":"<p>After initializing the SDK, you can use the following code to customize the configuration of the SDK.</p> <p>Important</p> <p>This configuration will override the default configuration in <code>amplifyconfiguration.json</code> file.</p> <pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\n\n// config the SDK after initialize.\nClickstreamAnalytics.getClickStreamConfiguration()\n        .withAppId(\"your appId\")\n        .withEndpoint(\"https://example.com/collect\")\n        .withAuthCookie(\"your authentication cookie\")\n        .withSendEventsInterval(10000)\n        .withSessionTimeoutDuration(1800000)\n        .withTrackScreenViewEvents(false)\n        .withTrackUserEngagementEvents(false)\n        .withTrackAppExceptionEvents(false)\n        .withLogEvents(true)\n        .withCustomDns(CustomOkhttpDns.getInstance())\n        .withCompressEvents(true);\n</code></pre> <p>Here is an explanation of each method.</p> Method name Parameter type Required Default value Description withAppId() String true -- the app id of your application in web console withEndpoint() String true -- the endpoint path you will upload the event to Clickstream ingestion server withAuthCookie() String false -- your auth cookie for AWS application load balancer auth cookie withSendEventsInterval() long false 1800000 event sending interval in milliseconds withSessionTimeoutDuration() long false 5000 the duration of the session timeout in milliseconds withTrackScreenViewEvents() boolean false true whether to auto-record screen view events withTrackUserEngagementEvents() boolean false true whether to auto-record user engagement events withTrackAppExceptionEvents() boolean false true whether to auto-record app exception events withLogEvents() boolean false true whether to automatically print event JSON for debugging events, Learn more withCustomDns() String false -- the method for setting your custom DNS, Learn more withCompressEvents() boolean false true whether to compress event content by gzip when uploading events."},{"location":"sdk-manual/android/#debug-events","title":"Debug events","text":"<p>You can follow the steps below to view the event raw JSON and debug your events.</p> <ol> <li>Using <code>ClickstreamAnalytics.getClickStreamConfiguration()</code> api and set the <code>withLogEvents()</code> method with true in debug mode, for example:     <pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\n\n// log the event in debug mode.\nClickstreamAnalytics.getClickStreamConfiguration()\n            .withLogEvents(BuildConfig.DEBUG);\n</code></pre></li> <li>Integrate the SDK and launch your app by Android Studio, then open the  Logcat window.</li> <li>Input <code>EventRecorder</code> to the filter, and you will see the JSON content of all events recorded by Clickstream Android SDK.</li> </ol>"},{"location":"sdk-manual/android/#configure-custom-dns","title":"Configure custom DNS","text":"<pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\n\n// config custom DNS.\nClickstreamAnalytics.getClickStreamConfiguration()\n        .withCustomDns(CustomOkhttpDns.getInstance());\n</code></pre> <p>If you want to use custom DNS for network request, you can create your <code>CustomOkhttpDns</code> which implementation <code>okhttp3.Dns</code>, then configure <code>.withCustomDns(CustomOkhttpDns.getInstance())</code> to make it works, you can refer to the example code .</p>"},{"location":"sdk-manual/android/#data-format-definition","title":"Data format definition","text":""},{"location":"sdk-manual/android/#data-types","title":"Data types","text":"<p>Clickstream Android SDK supports the following data types.</p> Data type Range Example int -2147483648 \uff5e 2147483647 12 long -9223372036854775808 \uff5e 9223372036854775807 26854775808 double 4.9E-324 \uff5e 1.7976931348623157E308 3.14 boolean true, false true String max 1024 characters \"Clickstream\""},{"location":"sdk-manual/android/#naming-rules","title":"Naming rules","text":"<ol> <li> <p>The event name and attribute name cannot start with a number, and only contain uppercase and lowercase letters, numbers, and underscores. In case of an invalid event name, it will throw <code>IllegalArgumentException</code>. In case of an invalid attribute name or user attribute name, it will discard the attribute and record error.</p> </li> <li> <p>Do not use <code>_</code> as prefix in an event name or attribute name, because the <code>_</code> prefix is reserved for the solution.</p> </li> <li> <p>The event name and attribute name are case-sensitive, so <code>Add_to_cart</code> and <code>add_to_cart</code> will be recognized as two different event names.</p> </li> </ol>"},{"location":"sdk-manual/android/#event-and-attribute-limitation","title":"Event and attribute limitation","text":"<p>In order to improve the efficiency of querying and analysis, we apply limits to event data as follows:</p> Name Suggestion Hard limit Strategy Error code Event name invalid -- -- discard event, print log and record <code>_clickstream_error</code> event 1001 Length of event name under 25 characters 50 characters discard event, print log and record <code>_clickstream_error</code> event 1002 Length of event attribute name under 25 characters 50 characters discard the attribute,  print log and record error in event attribute 2001 Attribute name invalid -- -- discard the attribute,  print log and record error in event attribute 2002 Length of event attribute value under 100 characters 1024 characters discard the attribute,  print log and record error in event attribute 2003 Event attribute per event under 50 attributes 500 evnet attributes discard the attribute that exceed, print log and record error in event attribute 2004 User attribute number under 25 attributes 100 user attributes discard the attribute that exceed, print log and record <code>_clickstream_error</code> event 3001 Length of User attribute name under 25 characters 50 characters discard the attribute, print log and record <code>_clickstream_error</code> event 3002 User attribute name invalid -- -- discard the attribute, print log and record <code>_clickstream_error</code> event 3003 Length of User attribute value under 50 characters 256 characters discard the attribute, print log and record <code>_clickstream_error</code> event 3004 <p>Important</p> <ul> <li>The character limits are the same for single-width character languages (e.g., English) and double-width character languages (e.g., Chinese).</li> <li>The limit of event attribute per event include preset attributes.</li> <li>If the attribute or user attribute with the same name is added more than twice, the latest value will apply.</li> <li>All errors that exceed the limit will be recorded <code>_error_code</code> and <code>_error_message</code> these two attribute in the event attributes.</li> </ul>"},{"location":"sdk-manual/android/#preset-events","title":"Preset events","text":""},{"location":"sdk-manual/android/#automatically-collected-events","title":"Automatically collected events","text":"Event name Triggered Event Attributes _first_open when the user launches an app the first time after installation _session_start when a user first open the app or a user returns to the app after 30 minutes of inactivity period, Learn more 1. _session_id 2. _session_start_timestamp _screen_view when new screen is opens, Learn more 1. _screen_name 2. _screen_id 3. _screen_unique_id 4. _previous_screen_name5. _previous_screen_id6. _previous_screen_unique_id7. _entrances8. _previous_timestamp9. _engagement_time_msec _user_engagement when user navigates away from current screen and the screen is in focus for at least one second, Learn more 1. _engagement_time_msec _app_start every time the app goes to visible 1. _is_first_time(when it is the first <code>_app_start</code> event after the application starts, the value is <code>true</code>) _app_end every time the app goes to invisible _profile_set when the <code>addUserAttributes()</code> or <code>setUserId()</code> API is called _app_exception when the app crashes 1. _exception_message2. _exception_stack _app_update when the app is updated to a new version and launched again 1. _previous_app_version _os_update when device operating system is updated to a new version 1. _previous_os_version _clickstream_error event_name is invalid or user attribute is invalid 1. _error_code 2. _error_message"},{"location":"sdk-manual/android/#session-definition","title":"Session definition","text":"<p>In Clickstream Android SDK, we do not limit the total time of a session. As long as the time between the next entry of the app and the last exit time is within the allowable timeout period, the current session is considered to be continuous.</p> <p>The <code>_session_start</code> event triggered when the app open for the first time, or the app was open to the foreground and the time between the last exit exceeded <code>session_time_out</code> period. The following are session-related attributes.</p> <ol> <li>_session_id: We calculate the session id by concatenating the last 8 characters of uniqueId and the current millisecond, for example: dc7a7a18-20230905-131926703.</li> <li>_session_duration : We calculate the session duration by minus the current event create timestamp and the session's <code>_session_start_timestamp</code>, this attribute will be added in every event during the session.</li> <li>_session_number : The auto increment number of session in current device, the initial value is 1</li> <li>Session timeout duration: By default is 30 minutes, which can be customized through the configuration update api.</li> </ol>"},{"location":"sdk-manual/android/#screen-view-definition","title":"Screen view definition","text":"<p>In Clickstream Android SDK, we define the <code>_screen_view</code> as an event that records a user's browsing path of screen, when a screen transition started, the <code>_screen_view</code> event will be recorded when meet any of the following conditions:</p> <ol> <li>No screen was previously set.</li> <li>The new screen name differs from the previous screen title.</li> <li>The new screen id differ from the previous screen id.</li> <li>The new screen unique id differ from the previous screen unique id.</li> </ol> <p>This event listens for Activity's <code>onResume</code> lifecycle method to judgment the screen transition. In order to track screen browsing path, we use <code>_previous_screen_name</code> , <code>_previous_screen_id</code> and <code>_previous_screen_unique_id</code> to link the previous screen. In addition, there are some other attributes in screen view event.</p> <ol> <li>_screen_unique_id: We calculate the screen unique id by getting the current screen's hashcode, for example: \"126861252\".</li> <li>_entrances: The first screen view event in a session is 1, others is 0.</li> <li>_previous_timestamp: The timestamp of the previous <code>_screen_view</code> event.</li> <li>_engagement_time_msec: The previous page last engagement milliseconds.</li> </ol>"},{"location":"sdk-manual/android/#user-engagement-definition","title":"User engagement definition","text":"<p>In Clickstream Android SDK, we define the <code>_user_engagement</code> as an event that records the screen browsing time, and we only send this event when user leave the screen and the screen has focus for at least one second.</p> <p>We define that users leave the screen in the following situations.</p> <ol> <li>When the user navigates to another screen.</li> <li>The user moves the app screen to the background.</li> <li>The user exit the app, or kill the process of app.</li> </ol> <p>engagement_time_msec: We calculate the milliseconds from when a screen is visible to when the user leave the screen.</p>"},{"location":"sdk-manual/android/#event-attributes","title":"Event attributes","text":""},{"location":"sdk-manual/android/#sample-event-structure","title":"Sample event structure","text":"<pre><code>{\n    \"hashCode\": \"80452b0\",\n    \"unique_id\": \"c84ad28d-16a8-4af4-a331-f34cdc7a7a18\",\n    \"event_type\": \"add_to_cart\",\n    \"event_id\": \"460daa08-0717-4385-8f2e-acb5bd019ee7\",\n    \"timestamp\": 1667877566697,\n    \"device_id\": \"f24bec657ea8eff7\",\n    \"platform\": \"Android\",\n    \"os_version\": \"10\",\n    \"make\": \"Samsung\",\n    \"brand\": \"Samsung\",\n    \"model\": \"TAS-AN00\",\n    \"locale\": \"zh_CN_#Hans\",\n    \"carrier\": \"CDMA\",\n    \"network_type\": \"Mobile\",\n    \"screen_height\": 2259,\n    \"screen_width\": 1080,\n    \"zone_offset\": 28800000,\n    \"system_language\": \"zh\",\n    \"country_code\": \"CN\",\n    \"sdk_version\": \"0.7.1\",\n    \"sdk_name\": \"aws-solution-clickstream-sdk\",\n    \"app_version\": \"1.0\",\n    \"app_package_name\": \"com.notepad.app\",\n    \"app_title\": \"Notepad\",\n    \"app_id\": \"notepad-4a929eb9\",\n    \"user\": {\n        \"_user_id\": {\n            \"value\": \"312121\",\n            \"set_timestamp\": 1667877566697\n        },\n        \"_user_name\": {\n            \"value\": \"carl\",\n            \"set_timestamp\": 1667877566697\n        },\n        \"_user_first_touch_timestamp\": {\n            \"value\": 1667877267895,\n            \"set_timestamp\": 1667877566697\n        }\n    },\n    \"attributes\": {\n        \"event_category\": \"recommended\",\n        \"currency\": \"CNY\",\n        \"_session_id\": \"dc7a7a18-20221108-031926703\",\n        \"_session_start_timestamp\": 1667877566703,\n        \"_session_duration\": 391809,\n        \"_session_number\": 1,\n        \"_screen_name\": \"ProductDetailActivity\",\n        \"_screen_unique_id\": \"126861252\"\n    }\n}\n</code></pre> <p>All user attributes will be stored in <code>user</code> object, and all custom and global attributes are in <code>attributes</code> object.</p>"},{"location":"sdk-manual/android/#common-attribute","title":"Common attribute","text":"Attribute name Data type Description How to generate Usage and purpose hashCode String the event object's hash code generated from <code>Integer.toHexString(AnalyticsEvent.hashCode())</code> distinguish different events app_id String the app_id for your app app_id was generated by clickstream solution when you register an app to a data pipeline identify the events for your apps unique_id String the unique id for user generated from <code>UUID.randomUUID().toString()</code> during the SDK first initializationit will be changed if user logout and then login to a new user. When user re-login to the previous user in same device, the unique_id will be reset to the same previous unique_id the unique id to identity different users and associating the behavior of logged-in and not logged-in device_id String the unique id for device generated from <code>Settings.System.getString(context.getContentResolver(), Settings.Secure.ANDROID_ID)</code>, if Android ID is null or \"\", we will use UUID instead. distinguish different devices event_type String event name set by developer or SDK distinguish different events type event_id String the unique id for event generated from <code>UUID.randomUUID().toString()</code> when the event create distinguish different events timestamp long event create timestamp generated from <code>System.currentTimeMillis()</code> when event create data analysis needs platform String the platform name for Android device is always \"Android\" data analysis needs os_version String the platform version code generated from <code>Build.VERSION.RELEASE</code> data analysis needs make String manufacturer of the device generated from <code>Build.MANUFACTURER</code> data analysis needs brand String brand of the device generated from <code>Build.BRAND</code> data analysis needs model String model of the device generated from <code>Build.MODEL</code> data analysis needs carrier String the device network operator name <code>TelephonyManager.getNetworkOperatorName()</code>default is: \"UNKNOWN\" data analysis needs network_type String the current device network type \"Mobile\", \"WIFI\" or \"UNKNOWN\"generated from <code>android.netConnectivityManager</code> data analysis needs screen_height int the absolute height of the available display size in pixels generated from <code>applicationContext.resources.displayMetrics.heightPixels</code> data analysis needs screen_width int the absolute width of the available display size in pixels generated from <code>applicationContext.resources.displayMetrics.widthPixels</code> data analysis needs zone_offset int the device raw offset from GMT in milliseconds. generated from <code>java.util.Calendar.get(Calendar.ZONE_OFFSET)</code> data analysis needs locale String the default locale(language, country and variant) for this device of the Java Virtual Machine generated from <code>java.util.Local.getDefault()</code> data analysis needs system_language String the device language code generated from <code>java.util.Local.getLanguage()</code>default is: \"UNKNOWN\" data analysis needs country_code String country/region code for this device generated from <code>java.util.Local.getCountry()</code>default is: \"UNKNOWN\" data analysis needs sdk_version String clickstream sdk version generated from <code>BuildConfig.VERSION_NAME</code> data analysis needs sdk_name String clickstream sdk name this will always be \"aws-solution-clickstream-sdk\" data analysis needs app_version String the app version name of user's app generated from <code>android.content.pm.PackageInfo.versionName</code>default is: \"UNKNOWN\" data analysis needs app_package_name String the app package name of user's app generated from <code>android.content.pm.PackageInfo.packageName</code>default is: \"UNKNOWN\" data analysis needs app_title String the app's display name generated from <code>android.content.pm.getApplicationLabel(appInfo)</code> data analysis needs"},{"location":"sdk-manual/android/#user-attributes","title":"User attributes","text":"Attribute name Description _user_id Reserved for user id that is assigned by app _user_ltv_revenue Reserved for user lifetime value _user_ltv_currency Reserved for user lifetime value currency _user_first_touch_timestamp Added to the user object for all events. The time (in milliseconds) when the user first opened the app"},{"location":"sdk-manual/android/#event-attributes_1","title":"Event attributes","text":"Attribute name Data type Auto track Description _traffic_source_medium String false Reserved for traffic medium. Use this attribute to store the medium that acquired user when events were logged. Example: Email, Paid search, Search engine _traffic_source_name String false Reserved for traffic name. Use this attribute to store the marketing campaign that acquired user when events were logged. Example: Summer promotion _traffic_source_source String false Reserved for traffic source. Name of the network source that acquired the user when the event were reported. Example: Google, Facebook, Bing, Baidu _channel String false Reserved for install source, it is the channel for app was downloaded _session_id String true Added in all events. _session_start_timestamp long true Added in all events. _session_duration long true Added in all events. _session_number int true Added in all events. _screen_name String true Added in all events. _screen_unique_id String true Added in all events."},{"location":"sdk-manual/android/#change-log","title":"Change log","text":"<p>GitHub change log</p>"},{"location":"sdk-manual/android/#reference-link","title":"Reference link","text":"<p>Source code</p> <p>Project issue</p>"},{"location":"sdk-manual/flutter/","title":"Clickstream Flutter SDK","text":""},{"location":"sdk-manual/flutter/#introduction","title":"Introduction","text":"<p>Clickstream Flutter SDK can help you easily collect in-app click stream data from mobile devices to your AWS environments through the data pipeline provisioned by this solution.</p> <p>The SDK is relies on the Clickstream Android SDK and Clickstream Swift SDK. Therefore, flutter SDK also supports automatically collect common user events and attributes (e.g., session start, first open). In addition, we've added easy-to-use APIs to simplify data collection in Flutter apps.</p>"},{"location":"sdk-manual/flutter/#platform-support","title":"Platform Support","text":"<p>Android: 4.1 (API level 16) and later</p> <p>iOS: 13 and later</p>"},{"location":"sdk-manual/flutter/#integrate-the-sdk","title":"Integrate the SDK","text":""},{"location":"sdk-manual/flutter/#1-include-sdk","title":"1. Include SDK","text":"<pre><code>flutter pub add clickstream_analytics\n</code></pre> <p>After complete, rebuild your Flutter application.</p> <pre><code>flutter run\n</code></pre>"},{"location":"sdk-manual/flutter/#2-initialize-the-sdk","title":"2. Initialize the SDK","text":"<p>Copy your configuration code from your clickstream solution web console, the configuration code should look like as follows. You can also manually add this code snippet and replace the values of appId and endpoint after you registered app to a data pipeline in the Clickstream Analytics solution console.</p> <pre><code>import 'package:clickstream_analytics/clickstream_analytics.dart';\n\nfinal analytics = ClickstreamAnalytics();\nanalytics.init(\n  appId: \"your appId\",\n  endpoint: \"https://example.com/collect\"\n);\n</code></pre> <p>Important</p> <ul> <li>Your <code>appId</code> and <code>endpoint</code> are already set up in it.</li> <li>We only need to initialize the SDK once after the application starts. It is recommended to do it in the main function of your App.</li> <li>We can use <code>bool result = await analytics.init()</code> to get the boolean value of the initialization result.</li> </ul>"},{"location":"sdk-manual/flutter/#3-start-using","title":"3. Start using","text":""},{"location":"sdk-manual/flutter/#record-event","title":"Record event","text":"<p>Add the following code where you need to record event.</p> <pre><code>import 'package:clickstream_analytics/clickstream_analytics.dart';\n\nfinal analytics = ClickstreamAnalytics();\n\n// record event with attributes\nanalytics.record(name: 'button_click', attributes: {\n  \"event_category\": \"shoes\",\n  \"currency\": \"CNY\",\n  \"value\": 279.9\n});\n\n// record event with name\nanalytics.record(name: \"button_click\");\n</code></pre>"},{"location":"sdk-manual/flutter/#login-and-logout","title":"Login and logout","text":"<pre><code>// when user login success.\nanalytics.setUserId(\"userId\");\n\n// when user logout\nanalytics.setUserId(null);\n</code></pre>"},{"location":"sdk-manual/flutter/#add-user-attribute","title":"Add user attribute","text":"<pre><code>analytics.setUserAttributes({\n  \"userName\":\"carl\",\n  \"userAge\": 22\n});\n</code></pre> <p>Current login user's attributes will be cached in disk, so the next time app launch you don't need to set all user's attribute again, of course you can use the same api <code>analytics.setUserAttributes()</code> to update the current user's attribute when it changes.</p>"},{"location":"sdk-manual/flutter/#add-global-attribute","title":"Add global attribute","text":"<pre><code>analytics.addGlobalAttributes({\n  \"_traffic_source_medium\": \"Search engine\",\n  \"_traffic_source_name\": \"Summer promotion\",\n  \"level\": 10\n});\n\n// delete global attribute\nanalytics.deleteGlobalAttributes([\"level\"]);\n</code></pre> <p>It is recommended to set global attributes after each SDK initialization, global attributes will be included in all events that occur after it is set.</p>"},{"location":"sdk-manual/flutter/#other-configurations","title":"Other configurations","text":"<p>In addition to the required <code>appId</code> and <code>endpoint</code>, you can configure other information to get more customized usage:</p> <pre><code>final analytics = ClickstreamAnalytics();\nanalytics.init(\n  appId: \"your appId\",\n  endpoint: \"https://example.com/collect\",\n  isLogEvents: false,\n  isCompressEvents: false,\n  sendEventsInterval: 10000,\n  isTrackScreenViewEvents: true,\n  isTrackUserEngagementEvents: true,\n  isTrackAppExceptionEvents: false,\n  authCookie: \"your auth cookie\",\n  sessionTimeoutDuration: 1800000\n);\n</code></pre> <p>Here is an explanation of each option:</p> Name Required Default value Description appId true -- the app id of your application in control plane endpoint true -- the endpoint path you will upload the event to Clickstream ingestion server isLogEvents false false whether to print out event json in console for debugging events, Learn more isCompressEvents false true whether to compress event content by gzip when uploading events sendEventsInterval false 10000 event sending interval in milliseconds isTrackScreenViewEvents false true whether auto record screen view events in app isTrackUserEngagementEvents false true whether auto record user engagement events in app isTrackAppExceptionEvents false false whether auto track exception event in app authCookie false -- your auth cookie for AWS application load balancer auth cookie sessionTimeoutDuration false 1800000 the duration for session timeout in milliseconds"},{"location":"sdk-manual/flutter/#configuration-update","title":"Configuration update","text":"<p>You can update the default configuration after initializing the SDK, below are the additional configuration options you can customize.</p> <pre><code>final analytics = ClickstreamAnalytics();\nanalytics.updateConfigure(\n    appId: \"your appId\",\n    endpoint: \"https://example.com/collect\",\n    isLogEvents: true,\n    isCompressEvents: false,\n    isTrackScreenViewEvents: false\n    isTrackUserEngagementEvents: false,\n    isTrackAppExceptionEvents: false,\n    sessionTimeoutDuration: 100000,\n    authCookie: \"test cookie\");\n</code></pre>"},{"location":"sdk-manual/flutter/#send-event-immediately","title":"Send event immediately","text":"<pre><code>final analytics = ClickstreamAnalytics();\nanalytics.flushEvents();\n</code></pre>"},{"location":"sdk-manual/flutter/#disable-sdk","title":"Disable SDK","text":"<p>You can disable the SDK in the scenario you need. After disabling the SDK, the SDK will not handle the logging and sending of any events. Of course, you can enable the SDK when you need to continue logging events.</p> <pre><code>final analytics = ClickstreamAnalytics();\n\n// disable SDK\nanalytics.disable();\n\n// enable SDK\nanalytics.enable();\n</code></pre>"},{"location":"sdk-manual/flutter/#debug-events","title":"Debug events","text":"<p>You can follow the steps below to view the event raw JSON and debug your events.</p> <ol> <li> <p>Using <code>analytics.updateConfigure()</code> api and set the <code>isLogEvents</code> attributes with true in debug mode, for example:     <pre><code>// log the event in debug mode.\nanalytics.updateConfigure(isLogEvents: true);\n</code></pre></p> </li> <li> <p>Integrate the SDK and launch your app.</p> <ol> <li>For Android application logs, we can see the logs directly in the terminal window. You can also use filters in    the Android Studio Logcat window to view logs.</li> <li>For iOS application logs, we should launch it via Xcode and open the log panel to see it.</li> </ol> </li> <li> <p>Input <code>EventRecorder</code> to the filter, and you will see the JSON content of all events recorded by Clickstream Flutter    SDK.</p> </li> </ol>"},{"location":"sdk-manual/flutter/#data-format-definition","title":"Data format definition","text":""},{"location":"sdk-manual/flutter/#data-types","title":"Data types","text":"<p>Clickstream Flutter SDK supports the following data types.</p> Data type Range Example int -9223372036854775808 \uff5e 9223372036854775807 12 double 5e-324 ~ 1.79e+308 3.14 bool true, false true String max 1024 characters \"Clickstream\""},{"location":"sdk-manual/flutter/#naming-rules","title":"Naming rules","text":"<ol> <li> <p>The event name and attribute name cannot start with a number, and only contain uppercase and lowercase letters,    numbers, and underscores. In case of an invalid attribute name or user attribute name, it will discard the attribute    and record error.</p> </li> <li> <p>Do not use <code>_</code> as prefix in an event name or attribute name, because the <code>_</code> prefix is reserved for the solution.</p> </li> <li> <p>The event name and attribute name are case-sensitive, so <code>Add_to_cart</code> and <code>add_to_cart</code> will be recognized as two    different event names.</p> </li> </ol>"},{"location":"sdk-manual/flutter/#event-and-attribute-limitation","title":"Event and attribute limitation","text":"<p>In order to improve the efficiency of querying and analysis, we apply limits to event data as follows:</p> Name Suggestion Hard limit Strategy Error code Event name invalid -- -- discard event, print log and record <code>_clickstream_error</code> event 1001 Length of event name under 25 characters 50 characters discard event, print log and record <code>_clickstream_error</code> event 1002 Length of event attribute name under 25 characters 50 characters discard the attribute,  print log and record error in event attribute 2001 Attribute name invalid -- -- discard the attribute,  print log and record error in event attribute 2002 Length of event attribute value under 100 characters 1024 characters discard the attribute,  print log and record error in event attribute 2003 Event attribute per event under 50 attributes 500 event attributes discard the attribute that exceed, print log and record error in event attribute 2004 User attribute number under 25 attributes 100 user attributes discard the attribute that exceed, print log and record <code>_clickstream_error</code> event 3001 Length of User attribute name under 25 characters 50 characters discard the attribute, print log and record <code>_clickstream_error</code> event 3002 User attribute name invalid -- -- discard the attribute, print log and record <code>_clickstream_error</code> event 3003 Length of User attribute value under 50 characters 256 characters discard the attribute, print log and record <code>_clickstream_error</code> event 3004 <p>Important</p> <ul> <li>The character limits are the same for single-width character languages (e.g., English) and double-width character languages (e.g., Chinese).</li> <li>The limit of event attribute per event include preset attributes.</li> <li>If the attribute or user attribute with the same name is added more than twice, the latest value will apply.</li> <li>All errors that exceed the limit will be recorded <code>_error_code</code> and <code>_error_message</code> these two attribute in the event attributes.</li> </ul>"},{"location":"sdk-manual/flutter/#preset-events","title":"Preset events","text":"<p>For Android: Refer to Android SDK preset events</p> <p>For iOS: Refer to Swift SDK preset events</p>"},{"location":"sdk-manual/flutter/#event-attributes","title":"Event attributes","text":"<p>For Android: Refer to Android SDK event attributes</p> <p>For iOS: Refer to Swift SDK event attributes</p>"},{"location":"sdk-manual/flutter/#change-log","title":"Change log","text":"<p>GitHub change log</p> <p>Native SDK version dependencies</p> Flutter SDK Version Android SDK Version Swift SDK Version 0.1.0 0.9.0 0.8.0"},{"location":"sdk-manual/flutter/#reference-link","title":"Reference link","text":"<p>Source code</p> <p>Project issue</p>"},{"location":"sdk-manual/http-api/","title":"Clickstream HTTP API","text":""},{"location":"sdk-manual/http-api/#introduction","title":"Introduction","text":"<p>This documentation will help you send your clickstream data directly to the Clickstream ingestion server via HTTP requests. The Clickstream data processing module will correctly process your data simultaneously by following the guidelines below. Then, you can visually analyze them in the subsequent report module.</p>"},{"location":"sdk-manual/http-api/#request-endpoint","title":"Request endpoint","text":"<p>After creating the application in the solution web console, you will get the Server Endpoint and App ID on the details page. For example:</p> <ul> <li>Server Endpoint: <code>https://example.com/collect</code></li> <li>App ID: <code>my_app</code></li> </ul>"},{"location":"sdk-manual/http-api/#api-specification","title":"API Specification","text":"<ol> <li>The app ID in the query parameters must be one of the applications you create for the project in the solution web    console. Otherwise, the server will respond to HTTP status code 403.</li> <li>The request body contains four parts: common attributes, items, user, and attributes. The public attributes require    the <code>event_type</code>, <code>event_id</code>, <code>timestamp</code>, and <code>app_id</code>; the rest are optional parameters.</li> <li>The total size of the body of a single request cannot exceed 1MB. The HTTP status code 413 will return if it exceeds.</li> </ol>"},{"location":"sdk-manual/http-api/#reqeust-method","title":"Reqeust method","text":"<p><code>POST</code></p>"},{"location":"sdk-manual/http-api/#request-headers","title":"Request headers","text":"Parameter name Required Example Description Content-Type YES application/json; charset=utf-8 Content type X-Forwarded-For NO 101.188.67.134 Source IP address, it's required if you forward the client requests to clickstream servers from your servers User-Agent NO Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Mobile Safari/537.36 User-Agent cookie NO your auth cookie The authentication token for your request. Please refer to server side configuration."},{"location":"sdk-manual/http-api/#request-query-parameters","title":"Request query parameters","text":"Parameter name Required Example Description appId YES test_app The app ID for your application is created in the solution web console platform NO Android/iOS/Web/... Distinguish between platforms event_bundle_sequence_id NO 1 Request sequence number, an incrementing integer starting from 1 hashCode NO 478acd09 The first eight digits of the sha256 calculation result of the request body string compression NO gzip Request body compression method. Currently, only gzip is supported. Keeping it absent means no compression"},{"location":"sdk-manual/http-api/#request-body","title":"Request body","text":"<p>The request body is an array structure that contains the JSON string of one or more events. For example:</p> <pre><code>[\n  {\n    \"event_type\": \"button_click\",\n    \"event_id\": \"460daa08-0717-4385-8f2e-acb5bd019ee7\",\n    \"timestamp\": 1667877566697,\n    \"app_id\": \"your appId\",\n    \"attributes\": {\n      \"productName\": \"shoes\",\n      \"Price\": 99.9\n    }\n  },\n  {\n    \"event_type\": \"item_view\",\n    \"event_id\": \"c6067c1c-fd8d-4fdb-bfaf-cc1212ca0195\",\n    \"timestamp\": 1667877565698,\n    \"app_id\": \"your appId\",\n    \"attributes\": {\n      \"productName\": \"book\",\n      \"Price\": 39.9\n    }\n  }\n]\n</code></pre>"},{"location":"sdk-manual/http-api/#event-attributes","title":"Event attributes","text":"Attributes name Required Data Type Example Description event_type YES String button_click Event type event_id YES String 460daa08-0717-4385-8f2e-acb5bd019ee7 Event's unique ID, we recommend using <code>UUID</code> to generate timestamp YES Long 1667877566697 The timestamp when the event was generated, in milliseconds app_id YES String shopping_dev The corresponding id when creating the application in the Clickstream web console platform NO String Android/iOS/Web/... Device platform os_version NO String 10 System version unique_id NO String c84ad28d-16a8-4af4-a331-f34cdc7a7a18 Unique ID to identify different users and associate the behavior of logged-in and not logged-in device_id NO String f24bec657ea8eff7 Distinguish between different devices make NO String Samsung Device manufactory brand NO String Samsung Device brand model NO String S23 Ultra Device model carrier NO String CDMA Device network operator name network_type NO String Mobile Current device network type locale NO String zh_CN Local information system_language NO String zh Device language code country_code NO String CN Device country code zone_offset NO int 2880000 Device's raw offset from GMT in milliseconds screen_height NO int 1920 Screen height in pixels screen_width NO int 1080 Screen width in pixels viewport_height NO int 540 App viewport height viewport_width NO int 360 App viewport width sdk_version NO String 1.2.3 SDK version sdk_name NO String aws-solution-clickstream-sdk SDK name app_package_name NO String com.example.app User's Application package name app_version NO String 1.1.0 Application version number app_title NO String shopping Application name items NO Object [{ \u00a0\u00a0\"id\": \"b011ddc3-632f-47cb-a68a-ad83678ecfed\",  \u00a0\u00a0\"name\": \"Classic coat-rack\",   \u00a0\u00a0\"category\": \"housewares\",  \u00a0\u00a0\"price\": 167}] Item list, Supports uploading multiple items at one time. A maximum of 100 items can be uploaded at one timeFor the item quantity limit, please refer to Event and Attribute Limitation For the supported attributes of the item, please refer to item attribute user NO Object {\u00a0\u00a0\"_user_id\": {\u00a0\u00a0\u00a0\u00a0\"value\": \"0202d0e1\",\u00a0\u00a0\u00a0\u00a0\"set_timestamp\": 1695006816345  \u00a0\u00a0}, \u00a0\u00a0 \"username\": {   \u00a0\u00a0\u00a0\u00a0 \"value\": \"carl\",    \u00a0\u00a0\u00a0\u00a0\u00a0\"set_timestamp\": 1695006816345  \u00a0\u00a0}} User attributes. Each attribute key is the user attribute name. Each attribute contains an object. The object contains two attributes: <code>value</code>: The value of the user attribute. <code>set_timestamp</code>: The timestamp millisecond value when setting the attribute.  Up to 100 user attributes can be added to an event. For specific restrictions, please refer to: Event and Attribute Limitations attributes NO Object {  \u00a0\u00a0\"productName\": \"book\", \u00a0\u00a0\"Price\": 39.9} Custom attributes. Up to 500 custom attributes can be added to an event, and the attribute name must meet the naming rules"},{"location":"sdk-manual/http-api/#request-response","title":"Request response","text":"<p>If the HttpCode status code returned by the request is 200, the request is considered successful, other status codes are failures, and the request does not return any other content.</p>"},{"location":"sdk-manual/http-api/#httpcode","title":"HttpCode","text":"Code Description 200 Request successful 403 Request failed. Please check if appId and endpoint match, if configured with authentication, please check whether the authentication cookie is correct 413 Request failed. The request body exceeds 1MB"},{"location":"sdk-manual/http-api/#request-code-example","title":"Request code example","text":"cURLC# HttpClientJava OkhttpJavaScript FetchPython Request <pre><code>curl --location 'https://example.com/collect?appId=test_release&amp;platform=Android&amp;event_bundle_sequence_id=1' \\\n--header 'Content-Type: application/json; charset=utf-8' \\\n--header 'X-Forwarded-For: 101.188.67.134' \\\n--data '[{\"event_type\":\"button_click\",\"event_id\":\"460daa08-0717-4385-8f2e-acb5bd019ee7\",\"timestamp\":1667877566697,\"app_id\":\"your appId\",\"attributes\":{\"productName\":\"shoes\",\"Price\":99.9}},{\"event_type\":\"item_view\",\"event_id\":\"c6067c1c-fd8d-4fdb-bfaf-cc1212ca0195\",\"timestamp\":1667877565698,\"app_id\":\"your appId\",\"attributes\":{\"productName\":\"book\",\"Price\":39.9}}]'\n</code></pre> <pre><code>var client = new HttpClient();\nvar request = new HttpRequestMessage(HttpMethod.Post, \"https://example.com/collect?appId=test_release&amp;platform=Android&amp;event_bundle_sequence_id=1\");\nrequest.Headers.Add(\"X-Forwarded-For\", \"101.188.67.134\");\nvar content = new StringContent(\"[{\\\"event_type\\\":\\\"button_click\\\",\\\"event_id\\\":\\\"460daa08-0717-4385-8f2e-acb5bd019ee7\\\",\\\"timestamp\\\":1667877566697,\\\"app_id\\\":\\\"your appId\\\",\\\"attributes\\\":{\\\"productName\\\":\\\"shoes\\\",\\\"Price\\\":99.9}},{\\\"event_type\\\":\\\"item_view\\\",\\\"event_id\\\":\\\"c6067c1c-fd8d-4fdb-bfaf-cc1212ca0195\\\",\\\"timestamp\\\":1667877565698,\\\"app_id\\\":\\\"your appId\\\",\\\"attributes\\\":{\\\"productName\\\":\\\"book\\\",\\\"Price\\\":39.9}}]\", null, \"application/json; charset=utf-8\");\nrequest.Content = content;\nvar response = await client.SendAsync(request);\nresponse.EnsureSuccessStatusCode();\nConsole.WriteLine(await response.Content.ReadAsStringAsync());\n</code></pre> <pre><code>OkHttpClient client=new OkHttpClient().newBuilder()\n        .build();\n        MediaType mediaType=MediaType.parse(\"application/json; charset=utf-8\");\n        RequestBody body=RequestBody.create(mediaType,\"[{\\\"event_type\\\":\\\"button_click\\\",\\\"event_id\\\":\\\"460daa08-0717-4385-8f2e-acb5bd019ee7\\\",\\\"timestamp\\\":1667877566697,\\\"app_id\\\":\\\"your appId\\\",\\\"attributes\\\":{\\\"productName\\\":\\\"shoes\\\",\\\"Price\\\":99.9}},{\\\"event_type\\\":\\\"item_view\\\",\\\"event_id\\\":\\\"c6067c1c-fd8d-4fdb-bfaf-cc1212ca0195\\\",\\\"timestamp\\\":1667877565698,\\\"app_id\\\":\\\"your appId\\\",\\\"attributes\\\":{\\\"productName\\\":\\\"book\\\",\\\"Price\\\":39.9}}]\");\n        Request request=new Request.Builder()\n        .url(\"https://example.com/collect?appId=test_release&amp;platform=Android&amp;event_bundle_sequence_id=1\")\n        .method(\"POST\",body)\n        .addHeader(\"Content-Type\",\"application/json; charset=utf-8\")\n        .addHeader(\"X-Forwarded-For\",\"101.188.67.134\")\n        .build();\n        Response response=client.newCall(request).execute();\n</code></pre> <pre><code>var myHeaders = new Headers();\nmyHeaders.append(\"Content-Type\", \"application/json; charset=utf-8\");\nmyHeaders.append(\"X-Forwarded-For\", \"101.188.67.134\");\n\nvar raw = \"[{\\\"event_type\\\":\\\"button_click\\\",\\\"event_id\\\":\\\"460daa08-0717-4385-8f2e-acb5bd019ee7\\\",\\\"timestamp\\\":1667877566697,\\\"app_id\\\":\\\"your appId\\\",\\\"attributes\\\":{\\\"productName\\\":\\\"shoes\\\",\\\"Price\\\":99.9}},{\\\"event_type\\\":\\\"item_view\\\",\\\"event_id\\\":\\\"c6067c1c-fd8d-4fdb-bfaf-cc1212ca0195\\\",\\\"timestamp\\\":1667877565698,\\\"app_id\\\":\\\"your appId\\\",\\\"attributes\\\":{\\\"productName\\\":\\\"book\\\",\\\"Price\\\":39.9}}]\";\n\nvar requestOptions = {\n    method: 'POST',\n    headers: myHeaders,\n    body: raw,\n    redirect: 'follow'\n};\n\nfetch(\"https://example.com/collect?appId=test_release&amp;platform=Android&amp;event_bundle_sequence_id=1\", requestOptions)\n    .then(response =&gt; response.text())\n    .then(result =&gt; console.log(result))\n    .catch(error =&gt; console.log('error', error));\n</code></pre> <pre><code>import requests\n\nurl = \"https://example.com/collect?appId=test_release&amp;platform=Android&amp;event_bundle_sequence_id=1\"\n\npayload = \"[{\\\"event_type\\\":\\\"button_click\\\",\\\"event_id\\\":\\\"460daa08-0717-4385-8f2e-acb5bd019ee7\\\",\\\"timestamp\\\":1667877566697,\\\"app_id\\\":\\\"your appId\\\",\\\"attributes\\\":{\\\"productName\\\":\\\"shoes\\\",\\\"Price\\\":99.9}},{\\\"event_type\\\":\\\"item_view\\\",\\\"event_id\\\":\\\"c6067c1c-fd8d-4fdb-bfaf-cc1212ca0195\\\",\\\"timestamp\\\":1667877565698,\\\"app_id\\\":\\\"your appId\\\",\\\"attributes\\\":{\\\"productName\\\":\\\"book\\\",\\\"Price\\\":39.9}}]\"\nheaders = {\n    'Content-Type': 'application/json; charset=utf-8',\n    'X-Forwarded-For': '101.188.67.134'\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nprint(response.text)\n</code></pre>"},{"location":"sdk-manual/http-api/#verification-data-reported-successfully","title":"Verification data reported successfully","text":"<ul> <li>Method 1: You can query the ods_events table in Athena directly through SQL. (Requires Athena to be enabled in   data modeling)</li> <li>Method 2: You can query the ods_events table in Redshift directly through SQL. (Requires Redshift to be   enabled in data modeling)</li> </ul>"},{"location":"sdk-manual/migrate-from-third-party-sdks/","title":"Migrate from third-party SDKs","text":""},{"location":"sdk-manual/migrate-from-third-party-sdks/#introduction","title":"Introduction","text":"<p>This article provides a best practice for you to migrate from a third-party SDK to Clickstream SDK. If you already have an SDK in your app or website, and you want to replace it with Clickstream SDK, we recommend you adopt this practice, which allow you to achieve a smooth migration with the following benefits:</p> <ul> <li>Minimum code changes</li> <li>Reuse existing data tracking codes</li> <li>Quick implementation time</li> <li>Dual measurement to ensure data completeness</li> </ul> <p>In summary, we recommend you create one overarching analytic logger function that encapsulates all the event logging methods from both legacy SDK and Clickstream SDK, so that you have one API to log event data to multiple destinations. Once satisfied with the data, you can easily update the function to disable the legacy SDK data logging.</p> <p>To make it easier to understand, we will use Clickstream Web SDK to replace Firebase Web SDK (GA4 SDK) as an example to illustrate. Assuming you have integrated Firebase Web SDK into your website, follow the steps below.</p>"},{"location":"sdk-manual/migrate-from-third-party-sdks/#step-1-integrate-clickstream-web-sdk","title":"Step 1: Integrate Clickstream Web SDK","text":""},{"location":"sdk-manual/migrate-from-third-party-sdks/#include-sdk","title":"Include SDK","text":"<pre><code>npm install @aws/clickstream-web\n</code></pre>"},{"location":"sdk-manual/migrate-from-third-party-sdks/#initialize-the-sdk","title":"Initialize the SDK","text":"<p>Copy your configuration code from your clickstream solution web console. We recommend you add the code to your app's root entry point, for example <code>index.js/app.tsx</code> in React or <code>main.ts</code> in Vue/Angular. The configuration code should look as follows.</p> <pre><code>import { ClickstreamAnalytics } from '@aws/clickstream-web';\n\nClickstreamAnalytics.init({\n   appId: \"your appId\",\n   endpoint: \"https://example.com/collect\",\n});\n</code></pre>"},{"location":"sdk-manual/migrate-from-third-party-sdks/#step-2-encapsulate-common-data-logger-methods","title":"Step 2: Encapsulate common data logger methods","text":"<p>When integrating multiple data analysis SDKs, it is strongly recommended that you encapsulate all event-logging methods in one function.  Processing data logging codes of different SDKs in the same place can make the code concise and easy for you to maintain. Below is an example of our encapsulation that you can copy directly into your project.</p> <pre><code>import { ClickstreamAnalytics } from \"@aws/clickstream-web\";\nimport { getAnalytics, logEvent, setUserProperties, setUserId } from \"firebase/analytics\";\n\nexport const AnalyticsLogger = {\n\n  log(eventName, attributes, items) {\n    attributes = attributes ?? {}\n    const {[\"items\"]: items, ...mAttributes} = attributes;\n\n    // Clickstream SDK\n    ClickstreamAnalytics.record({\n      name: eventName,\n      attributes: mAttributes,\n      items: items\n    })\n\n    //Firebase SDK\n    const analytics = getAnalytics();\n    logEvent(analytics, eventName, attributes);\n  },\n\n  setUserAttributes(attributes) {\n    // Clickstream SDK\n    ClickstreamAnalytics.setUserAttributes(attributes);\n\n    // Firebase SDK\n    const analytics = getAnalytics();\n    setUserProperties(analytics, attributes);\n  },\n\n  setUserId(userId) {\n    //Clickstream SDK\n    ClickstreamAnalytics.setUserId(userId)\n\n    //Firebase SDK\n    const analytics = getAnalytics();\n    setUserId(analytics, userId);\n  },\n}\n</code></pre> <p>We need to encapsulate three APIs <code>log()</code> \u3001<code>setUserAttributes()</code> and <code>setUserId()</code>, that's all. When we invoke the <code>AnalyticsLogger.log('testEvent')</code>  method, both Clickstream and Firebase SDK will log the event, so we only need to call the <code>AnalyticsLogger</code> API when you need to log event data.</p>"},{"location":"sdk-manual/migrate-from-third-party-sdks/#step-3-migrate-to-common-apis-in-minutes","title":"Step 3: Migrate to common APIs in minutes","text":""},{"location":"sdk-manual/migrate-from-third-party-sdks/#for-log-events","title":"For log events","text":"<pre><code>  onSignedUp(user) {\n    let attributes = {\n      _user_id: user.id,\n      username: user.username,\n      email: user.email,\n    };\n--  logEvent(analytics, 'sign_up', attributes);\n++  AnalyticsLogger.log('sign_up', attributes);\n  }\n</code></pre> <p>For the events log API. We need to get the event name and attributes for the events log API and pass them into the new API. Of course, you can also use the \"Replace in File\" feature to make quick changes, as shown in the image below.</p> <p> </p>"},{"location":"sdk-manual/migrate-from-third-party-sdks/#for-log-user-attributes","title":"For log user attributes","text":"<pre><code>  userSignedIn(user) {\n--  setUserId(analytics, user.id);\n++  AnalyticsLogger.setUserId(user.id);\n    let attributes = {\n      _user_id: user.id,\n      username: user.username,\n      email: user.email,\n    };\n--  setUserProperties(analytics, attributes);\n++  AnalyticsLogger.setUserAttributes(attributes);\n  }\n</code></pre> <p>For user id, replace  <code>setUserId()</code> with <code>AnalyticsLogger.setUserId()</code> .</p> <p>For user attributes, replace <code>setUserProperties()</code> with <code>AnalyticsLogger.setUserAttributes()</code> .</p>"},{"location":"sdk-manual/migrate-from-third-party-sdks/#summary","title":"Summary","text":"<p>As we saw above, it is easy to get Clickstream SDK and Firebase SDK to work together. After these three steps, your data will be uploaded to Clickstream Analytics and Firebase, these two SDKs will work well together and will not influence each other. After you are satisfied with the data, you only need to modify the <code>AnalyticsLogger</code> file to remove or disable another SDK smoothly.</p>"},{"location":"sdk-manual/swift/","title":"Clickstream Swift SDK","text":""},{"location":"sdk-manual/swift/#introduction","title":"Introduction","text":"<p>Clickstream Swift SDK can help you easily collect in-app click stream data from iOS devices to your AWS environments through the data pipeline provisioned by this solution.</p> <p>The SDK is based on the Amplify for Swift SDK Core Library and developed according to the Amplify Swift SDK plug-in specification. In addition, the SDK provides features that automatically collect common user events and attributes (for example, screen view, and first open) to accelerate data collection for users.</p>"},{"location":"sdk-manual/swift/#platform-support","title":"Platform Support","text":"<p>Clickstream Swift SDK supports iOS 13+.</p> <p>Clickstream Swift SDK requires Xcode 13.4 or higher to build.</p>"},{"location":"sdk-manual/swift/#integrate-sdk","title":"Integrate SDK","text":""},{"location":"sdk-manual/swift/#1add-package","title":"1.Add Package","text":"<p>We use Swift Package Manager(SPM) to distribute Clickstream Swift SDK, open your project in Xcode and select File &gt; Add Packages.</p> <p></p> <ol> <li>Copy the Clickstream Swift SDK GitHub repository URL and paste it into the search bar.     <pre><code>https://github.com/awslabs/clickstream-swift\n</code></pre></li> <li>Check the rules for the version of the SDK that you want Swift Package Manager to install, it is recommended to choose Up to Next Major Version, then click Add Package.</li> <li>Keep the Clickstream product checked as default.</li> <li>Choose Add Package again to finish the package installation.</li> </ol> <p></p>"},{"location":"sdk-manual/swift/#2parameter-configuration","title":"2.Parameter configuration","text":"<p>Download your <code>amplifyconfiguration.json</code> file from your Clickstream solution web console, and paste it to your project root folder.</p> <p></p> <p>the JSON file will be as follows:</p> <pre><code>{\n  \"analytics\": {\n    \"plugins\": {\n      \"awsClickstreamPlugin \": {\n        \"appId\": \"your appId\",\n        \"endpoint\": \"https://example.com/collect\",\n        \"isCompressEvents\": true,\n        \"autoFlushEventsInterval\": 10000,\n        \"isTrackAppExceptionEvents\": false\n      }\n    }\n  }\n}\n</code></pre> <p>Your <code>appId</code> and <code>endpoint</code> are already set up in it, here's an explanation of each property:</p> <ul> <li>appId (Required): the app id of your project in web console.</li> <li>endpoint (Required): the endpoint url you will upload the event to AWS server.</li> <li>isCompressEvents: whether to compress event content when uploading events, default is <code>true</code></li> <li>autoFlushEventsInterval: event sending interval, the default is <code>10s</code></li> <li>isTrackAppExceptionEvents: whether auto track exception event in app, default is <code>false</code></li> </ul>"},{"location":"sdk-manual/swift/#3initialize-the-sdk","title":"3.Initialize the SDK","text":"<p>Once you have configured the parameters, you need to initialize it in AppDelegate's <code>didFinishLaunchingWithOptions</code> lifecycle method to use the SDK.</p> SwiftObjective-C <pre><code>import Clickstream\n\nfunc application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -&gt; Bool {\n    do {\n        try ClickstreamAnalytics.initSDK()\n    } catch {\n        assertionFailure(\"Fail to initialize ClickstreamAnalytics: \\(error)\")\n    }\n    return true\n}\n</code></pre> <pre><code>@import Clickstream;\n\n- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions { \n    NSError *error = nil;\n    [ClickstreamObjc initSDKAndReturnError:&amp;error];\n    if (error) {\n        NSLog(@\"Fail to initialize ClickstreamAnalytics: %@\", error.localizedDescription);\n    }\n    return YES;\n}\n</code></pre> <p>If your project is developed with SwiftUI, you need to create an application delegate and attach it to your <code>App</code> through <code>UIApplicationDelegateAdaptor</code>.</p> <pre><code>@main\nstruct YourApp: App {\n    @UIApplicationDelegateAdaptor(AppDelegate.self) var appDelegate\n    var body: some Scene {\n        WindowGroup {\n            YourView()\n        }\n    }\n}\n</code></pre> <p>For SwiftUI, we do not yet support automatic collection of screen view events, you need to disable screen view event by setting <code>configuration.isTrackScreenViewEvents = false</code>, see the configuration steps.</p>"},{"location":"sdk-manual/swift/#4start-using","title":"4.Start  using","text":""},{"location":"sdk-manual/swift/#record-event","title":"Record event","text":"<p>Add the following code where you need to record event.</p> SwiftObjective-C <pre><code>import Clickstream\n\n// for record an event with custom attributes\nlet attributes: ClickstreamAttribute = [\n    \"category\": \"shoes\",\n    \"currency\": \"CNY\",\n    \"value\": 279.9\n]\nClickstreamAnalytics.recordEvent(\"button_click\", attributes)\n\n// for record an event directly\nClickstreamAnalytics.recordEvent(\"button_click\")\n</code></pre> <pre><code>import Clickstream;\n\n// for record an event with custom attributes\nNSDictionary *attributes =@{\n    @\"category\": @\"shoes\",\n    @\"currency\": @\"CNY\",\n    @\"value\": @12.34\n};\n[ClickstreamObjc recordEvent:@\"button_click\" :attributes];\n\n// for record an event directly\n[ClickstreamObjc recordEvent:@\"button_click\"];\n</code></pre>"},{"location":"sdk-manual/swift/#add-global-attribute","title":"Add global attribute","text":"SwiftObjective-C <pre><code>import Clickstream\n\nlet globalAttribute: ClickstreamAttribute = [\n    \"channel\": \"apple\",\n    \"class\": 6,\n    \"level\": 5.1,\n    \"isOpenNotification\": true,\n]\nClickstreamAnalytics.addGlobalAttributes(globalAttribute)\n\n// for delete an global attribute\nClickstreamAnalytics.deleteGlobalAttributes(\"level\")\n</code></pre> <pre><code>import Clickstream;\n\nNSDictionary *attributes =@{\n    @\"channel\": @\"apple\",\n    @\"class\": @6,\n    @\"level\": @5.1,\n    @\"isOpenNotification\": @YES\n};\n[ClickstreamObjc addGlobalAttributes :attributes];\n\n// for delete an global attribute\n[ClickstreamObjc deleteGlobalAttributes: @[@\"level\"]];\n</code></pre> <p>Please add the global attribute after the SDK initialization is completed, the global attribute will be added to the attribute object in all events.</p>"},{"location":"sdk-manual/swift/#login-and-logout","title":"Login and logout","text":"SwiftObjective-C <pre><code>import Clickstream\n\n// when user login usccess.\nClickstreamAnalytics.setUserId(\"userId\")\n\n// when user logout\nClickstreamAnalytics.setUserId(nil)\n</code></pre> <pre><code>import Clickstream;\n\n// when user login usccess.\n[ClickstreamObjc setUserId:@\"userId\"];\n\n// when user logout\n[ClickstreamObjc setUserId:NULL];\n</code></pre>"},{"location":"sdk-manual/swift/#add-user-attribute","title":"Add user attribute","text":"SwiftObjective-C <pre><code>import Clickstream\n\nlet userAttributes : ClickstreamAttribute=[\n    \"_user_age\": 21,\n    \"_user_name\": \"carl\"\n]\nClickstreamAnalytics.addUserAttributes(userAttributes)\n</code></pre> <pre><code>import Clickstream;\n\nNSDictionary *userAttributes =@{\n    @\"_user_age\": @21,\n    @\"user_name\": @\"carl\"\n};\n[ClickstreamObjc addUserAttributes:userAttributes];\n</code></pre> <p>Current login user's attributes will be cached in disk, so the next time app launch you don't need to set all user's attribute again, of course you can use the same api <code>ClickstreamAnalytics.addUserAttributes()</code> to update the current user's attribute when it changes.</p> <p>Important</p> <p>If your application is already published and most users have already logged in, please manually set the user attributes once when integrate the Clickstream SDK for the first time to ensure that subsequent events contains user attributes.</p>"},{"location":"sdk-manual/swift/#send-event-immediately","title":"Send event immediately","text":"SwiftObjective-C <pre><code>import Clickstream\n// for send event immediately.\nClickstreamAnalytics.flushEvents()\n</code></pre> <pre><code>import Clickstream;\n// for send event immediately.\n[ClickstreamObjc flushEvents];\n</code></pre>"},{"location":"sdk-manual/swift/#disable-sdk","title":"Disable SDK","text":"<p>You can disable the SDK in the scenario you need. After disabling the SDK, the SDK will not handle the logging and sending of any events. Of course you can enable the SDK when you need to continue logging events.</p> SwiftObjective-C <pre><code>import Clickstream\n\n// disable SDK\nClickstreamAnalytics.disable()\n\n// enable SDK\nClickstreamAnalytics.enable()\n</code></pre> <pre><code>import Clickstream;\n\n// disable SDK\n[ClickstreamObjc disable];\n\n// enable SDK\n[ClickstreamObjc enable];\n</code></pre>"},{"location":"sdk-manual/swift/#configuration-update","title":"Configuration update","text":"<p>After initializing the SDK, you can use the following code to customize the configuration of the SDK.</p> SwiftObjective-C <pre><code>import Clickstream\n\n// config the sdk after initialize.\ndo {\n    var configuration = try ClickstreamAnalytics.getClickstreamConfiguration()\n    configuration.appId = \"your appId\"\n    configuration.endpoint = \"https://example.com/collect\"\n    configuration.authCookie = \"your authentication cookie\"\n    configuration.sessionTimeoutDuration = 1800000\n    configuration.isTrackScreenViewEvents = false\n    configuration.isTrackUserEngagementEvents = false\n    configuration.isLogEvents = true\n    configuration.isCompressEvents = true\n} catch {\n    print(\"Failed to config ClickstreamAnalytics: \\(error)\")\n}\n</code></pre> <pre><code>import Clickstream;\n\n// config the sdk after initialize.\nClickstreamContextConfiguration *configuration = [ClickstreamObjc getClickstreamConfigurationAndReturnError:&amp;error];\nif (configuration) {\n    [configuration setAppId:@\"your appId\"];\n    [configuration setEndpoint:@\"https://example.com/collect\"];\n    [configuration setAuthCookie:@\"your authentication cookie\"];\n    [configuration setSessionTimeoutDuration:1800000];\n    [configuration setIsTrackScreenViewEvents:0];\n    [configuration setIsTrackUserEngagementEvents:0];\n    [configuration setIsLogEvents:1];\n    [configuration setIsCompressEvents:1];\n}else{\n    NSLog(@\"Failed to get configuration: %@\", error.localizedDescription);\n}\n</code></pre> <p>Note: this configuration will override the default configuration in <code>amplifyconfiguration.json</code> file</p> <p>Here is an explanation of each option.</p> Method name Parameter type Required Default value Description appId String true -- the app id of your application in web console endpoint String true -- the endpoint path you will upload the event to Clickstream ingestion server authCookie String false -- your auth cookie for AWS application load balancer auth cookie sessionTimeoutDuration Int64 false 1800000 the duration for session timeout in milliseconds isTrackScreenViewEvents Bool false true whether to auto-record screen view events isTrackUserEngagementEvents Bool false true whether to auto-record user engagement events isLogEvents Bool false false whether to automatically print event JSON for debugging events, Learn more isCompressEvents Bool false true whether to compress event content by gzip when uploading events"},{"location":"sdk-manual/swift/#debug-events","title":"Debug events","text":"<p>You can follow the steps below to view the event raw JSON and debug your events.</p> <ol> <li>set the <code>isLogEvents</code> option with true in debug mode.</li> <li>Integrate the SDK and launch your app by Xcode, then open the log panel.</li> <li>Input <code>EventRecorder</code> to the filter, and you will see the JSON content of all events recorded by Clickstream Swift SDK.</li> </ol>"},{"location":"sdk-manual/swift/#data-format-definition","title":"Data format definition","text":""},{"location":"sdk-manual/swift/#data-type","title":"Data type","text":"<p>Clickstream Swift SDK supports the following data types:</p> Data type Range Sample Int -2147483648\uff5e2147483647 12 Int64 -9,223,372,036,854,775,808\uff5e 9,223,372,036,854,775,807 26854775808 Double -2.22E-308~1.79E+308 3.14 Boolean true, false true String max support 1024 characters \"clickstream\""},{"location":"sdk-manual/swift/#naming-rules","title":"Naming rules","text":"<ol> <li> <p>The event name and attribute name cannot start with a number, and only contains uppercase and lowercase letters, numbers, underscores, if the event name is invalid will throw <code>precondition failure</code>, if the attribute or user attribute name is invalid the attribute will be discarded and record error.</p> </li> <li> <p>Do not use <code>_</code> as prefix to naming event name and attribute name, <code>_</code> prefix is the reserved from Clickstream Analytics.</p> </li> <li> <p>The event name and attribute name are in case-sensitive, So the event <code>Add_to_cart</code> and <code>add_to_cart</code> will be recognized as two different event.</p> </li> </ol>"},{"location":"sdk-manual/swift/#event-and-attribute-limitation","title":"Event and Attribute Limitation","text":"<p>In order to improve the efficiency of querying and analysis, we need to limit events as follows:</p> Name Suggestion Hard limit Strategy Error code Event name invalid -- -- discard event, print log and record <code>_clickstream_error</code> event 1001 Length of event name under 25 characters 50 characters discard event, print log and record <code>_clickstream_error</code> event 1002 Length of event attribute name under 25 characters 50 characters discard the attribute,  print log and record error in event attribute 2001 Attribute name invalid -- -- discard the attribute,  print log and record error in event attribute 2002 Length of event attribute value under 100 characters 1024 characters discard the attribute,  print log and record error in event attribute 2003 Event attribute per event under 50 attributes 500 evnet attributes discard the attribute that exceed, print log and record error in event attribute 2004 User attribute number under 25 attributes 100 user attributes discard the attribute that exceed, print log and record <code>_clickstream_error</code> event 3001 Length of User attribute name under 25 characters 50 characters discard the attribute, print log and record <code>_clickstream_error</code> event 3002 User attribute name invalid -- -- discard the attribute, print log and record <code>_clickstream_error</code> event 3003 Length of User attribute value under 50 characters 256 characters discard the attribute, print log and record <code>_clickstream_error</code> event 3004 <p>Important</p> <ul> <li>The character limits are the same for single-width character languages (e.g., English) and double-width character languages (e.g., Chinese).</li> <li>The limit of event attribute per event include preset attributes.</li> <li>If the attribute or user attribute with the same name is added more than twice, the latest value will apply.</li> <li>All errors that exceed the limit will be recorded <code>_error_code</code> and <code>_error_message</code> these two attribute in the event attributes.</li> </ul>"},{"location":"sdk-manual/swift/#preset-events","title":"Preset events","text":""},{"location":"sdk-manual/swift/#automatically-collected-events","title":"Automatically collected events","text":"Event name Triggered Event Attributes _first_open when the user launches an app the first time after installation _session_start when a user first open the app or a user returns to the app after 30 minutes of inactivity period, Learn more 1. _session_id 2. _session_start_timestamp _screen_view when new screen is opens, Learn more 1. _screen_name 2. _screen_id 3. _screen_unique_id 4. _previous_screen_name5. _previous_screen_id6. _previous_screen_unique_id7. _entrances8. _previous_timestamp9. _engagement_time_msec _user_engagement when user navigates away from current screen and the screen is in focus for at least one second, Learn more 1. _engagement_time_msec _app_start every time the app goes to visible 1. _is_first_time(when it is the first <code>_app_start</code> event after the application starts, the value is <code>true</code>) _app_end every time the app goes to invisible _profile_set when the <code>addUserAttributes()</code> or <code>setUserId()</code> API is called _app_exception when the app crashes 1. _exception_message2. _exception_stack _app_update when the app is updated to a new version and launched again 1. _previous_app_version _os_update when device operating system is updated to a new version 1. _previous_os_version _clickstream_error event_name is invalid or user attribute is invalid 1. _error_code 2. _error_message"},{"location":"sdk-manual/swift/#session-definition","title":"Session definition","text":"<p>In Clickstream Swift SDK, we do not limit the total time of a session. As long as the time between the next entry of the app and the last exit time is within the allowable timeout period, the current session is considered to be continuous.</p> <p>The <code>_session_start</code> event triggered when the app open for the first time, or the app was open to the foreground and the time between the last exit exceeded <code>session_time_out</code> period. The following are session-related attributes.</p> <ol> <li>_session_id: We calculate the session id by concatenating the last 8 characters of uniqueId and the current millisecond, for example: dc7a7a18-20230905-131926703.</li> <li>_session_duration : We calculate the session duration by minus the current event create timestamp and the session's <code>_session_start_timestamp</code>, this attribute will be added in every event during the session.</li> <li>_session_number : The auto increment number of session in current device, the initial value is 1</li> <li>Session timeout duration: By default is 30 minutes, which can be customized through the configuration update api.</li> </ol>"},{"location":"sdk-manual/swift/#screen-view-definition","title":"Screen view definition","text":"<p>In Clickstream Swift SDK, we define the <code>_screen_view</code> as an event that records a user's browsing path of screen, when a screen transition started, the <code>_screen_view</code> event will be recorded when meet any of the following conditions:</p> <ol> <li>No screen was previously set.</li> <li>The new screen name differs from the previous screen title.</li> <li>The new screen id differ from the previous screen id.</li> <li>The new screen unique id differ from the previous screen unique id.</li> </ol> <p>This event listens for UIViewController's <code>onViewDidAppear</code> lifecycle method to judgment the screen transition. In order to track screen browsing path, we use <code>_previous_screen_name</code> , <code>_previous_screen_id</code> and <code>_previous_screen_unique_id</code> to link the previous screen. In addition, there are some other attributes in screen view event.</p> <ol> <li>_screen_unique_id: We calculate the screen unique id by getting the current screen's hash value, for example: \"5260751568\".</li> <li>_entrances: The first screen view event in a session is 1, others is 0.</li> <li>_previous_timestamp: The timestamp of the previous <code>_screen_view</code> event.</li> <li>_engagement_time_msec: The previous page last engagement milliseconds.</li> </ol>"},{"location":"sdk-manual/swift/#user-engagement-definition","title":"User engagement definition","text":"<p>In Clickstream Swift SDK, we define the <code>_user_engagement</code> as an event that records the screen browsing time, and we only send this event when user leave the screen and the screen has focus for at least one second.</p> <p>We define that users leave the screen in the following situations.</p> <ol> <li>When the user navigates to another screen.</li> <li>The user moves the app screen to the background.</li> <li>The user exit the app, or kill the process of app.</li> </ol> <p>engagement_time_msec: We calculate the milliseconds from when a screen is visible to when the user leave the screen.</p>"},{"location":"sdk-manual/swift/#event-attributes","title":"Event attributes","text":""},{"location":"sdk-manual/swift/#sample-event-structure","title":"Sample event structure","text":"<pre><code>{\n    \"app_id\": \"Shopping\",\n    \"app_package_name\": \"com.company.app\",\n    \"app_title\": \"Shopping\",\n    \"app_version\": \"1.0\",\n    \"brand\": \"apple\",\n    \"carrier\": \"UNKNOWN\",\n    \"country_code\": \"US\",\n    \"device_id\": \"A536A563-65BD-49BE-A6EC-6F3CE7AC8FBE\",\n    \"device_unique_id\": \"\",\n    \"event_id\": \"91DA4BBE-933F-4DFA-A489-8AEFBC7A06D8\",\n    \"event_type\": \"add_to_cart\",\n    \"hashCode\": \"63D7991D\",\n    \"locale\": \"en_US\",\n    \"make\": \"apple\",\n    \"model\": \"iPhone 14 Pro\",\n    \"network_type\": \"WIFI\",\n    \"os_version\": \"16.4\",\n    \"platform\": \"iOS\",\n    \"screen_height\": 2556,\n    \"screen_width\": 1179,\n    \"sdk_name\": \"aws-solution-clickstream-sdk\",\n    \"sdk_version\": \"0.4.1\",\n    \"system_language\": \"en\",\n    \"timestamp\": 1685082174195,\n    \"unique_id\": \"0E6614B7-2D2C-4774-AB2F-B0A9E6C3BFAC\",\n    \"zone_offset\": 28800000,\n    \"user\": {\n        \"_user_city\": {\n            \"set_timestamp\": 1685006678437,\n            \"value\": \"Shanghai\"\n        },\n        \"_user_first_touch_timestamp\": {\n            \"set_timestamp\": 1685006678434,\n            \"value\": 1685006678432\n        },\n        \"_user_name\": {\n            \"set_timestamp\": 1685006678437,\n            \"value\": \"carl\"\n        }\n    },\n    \"attributes\": {\n        \"event_category\": \"recommended\",\n        \"currency\": \"CNY\",\n        \"_session_duration\": 15349,\n        \"_session_id\": \"0E6614B7-20230526-062238846\",\n        \"_session_number\": 3,\n        \"_session_start_timestamp\": 1685082158847,\n        \"_screen_name\": \"ProductDetailViewController\",\n        \"_screen_unique_id\": \"5260751568\"\n    }\n}\n</code></pre> <p>All user attributes will be included in <code>user</code> object, and all custom and global attribute are stored in <code>attributes</code> object.</p>"},{"location":"sdk-manual/swift/#common-attribute","title":"Common attribute","text":"Attribute name Data type Description How to generate Usage and purpose hashCode String the AnalyticsEvent Object's hashCode generated from<code>String(format: \"%08X\", hasher.combine(eventjson))</code> distinguish different events app_id String the app_id for your app app_id was generated by clickstream solution when you register an app to a data pipeline identify the events for your apps unique_id String the unique id for user generated from <code>UUID().uuidString</code> when the sdk first initializationit will be changed if user logout and then login to a new user. When user re-login to the previous user in same device, the unique_id will be reset to the same previous unique_id the unique id to identity different users and associating the behavior of logged-in and not logged-in device_id String the unique id for device generated from<code>UIDevice.current.identifierForVendor?.uuidString ?? UUID().uuidString</code>it will be changed after app reinstall distinguish different devices device_unique_id String the device advertising Id generated from<code>ASIdentifierManager.shared().advertisingIdentifier.uuidString ?? \"\"</code> distinguish different devices event_type String event name set by user or sdk. distinguish different events type event_id String the unique id for event generated from <code>UUID().uuidString</code> when the event create. distinguish different events timestamp Int64 event create timestamp generated from <code>Date().timeIntervalSince1970 * 1000</code> when event create data analysis needs platform String the platform name for iOS device is always \"iOS\" data analysis needs os_version String the iOS os version generated from<code>UIDevice.current.systemVersion</code> data analysis needs make String manufacturer of the device for iOS device is always \"apple\" data analysis needs brand String brand of the device for iOS device is always \"apple\" data analysis needs model String model of the device generated from mapping of device identifier data analysis needs carrier String the device network operator name generated from<code>CTTelephonyNetworkInfo().serviceSubscriberCellularProviders?.first?.value</code>default is: \"UNKNOWN\" data analysis needs network_type String the current device network type \"Mobile\", \"WIFI\" or \"UNKNOWN\"generate by  <code>NWPathMonitor</code> data analysis needs screen_height int The absolute height of the available display size in pixels generated from<code>UIScreen.main.bounds.size.height * UIScreen.main.scale</code> data analysis needs screen_width int The absolute width of the available display size in pixels. generated from<code>UIScreen.main.bounds.size.width * UIScreen.main.scale</code> data analysis needs zone_offset int the device raw offset from GMT in milliseconds. generated from<code>TimeZone.current.secondsFromGMT()*1000</code> data analysis needs locale String the default locale(language, country and variant) for this device generated from <code>Locale.current</code> data analysis needs system_language String the device language code generated from <code>Locale.current.languageCode</code>default is: \"UNKNOWN\" data analysis needs country_code String country/region code for this device generated from <code>Locale.current.regionCode</code>default is: \"UNKNOWN\" data analysis needs sdk_version String clickstream sdk version generated from<code>PackageInfo.version</code> data analysis needs sdk_name String clickstream sdk name this will always be <code>aws-solution-clickstream-sdk</code> data analysis needs app_version String the app version name generated from <code>Bundle.main.infoDictionary[\"CFBundleShortVersionString\"] ?? \"\"</code> data analysis needs app_package_name String the app package name generated from<code>Bundle.main.infoDictionary[\"CFBundleIdentifier\"] ?? \"\"</code> data analysis needs app_title String the app's display name generated from <code>Bundle.main.infoDictionary[\"CFBundleName\"] ?? \"\"</code> data analysis needs"},{"location":"sdk-manual/swift/#user-attributes","title":"User attributes","text":"attribute name description _user_id Reserved for user id that is assigned by app _user_ltv_revenue Reserved for user lifetime value _user_ltv_currency Reserved for user lifetime value currency _user_first_touch_timestamp Added to the user object for all events. The time (in milliseconds) at which the user first opened the app"},{"location":"sdk-manual/swift/#event-attributes_1","title":"Event attributes","text":"Attribute name Data type Auto track Description _traffic_source_medium String false Reserved for traffic medium. Use this attribute to store the medium that acquired user when events were logged. Example: Email, Paid search, Search engine _traffic_source_name String false Reserved for traffic name. Use this attribute to store the marketing campaign that acquired user when events were logged. Example: Summer promotion _traffic_source_source String false Reserved for traffic source. Name of the network source that acquired the user when the event were reported. Example: Google, Facebook, Bing, Baidu _channel String false Reserved for install source, it is the channel for app was downloaded _session_id String true Added in all events. _session_start_timestamp long true Added in all events. _session_duration long true Added in all events. _session_number int true Added in all events. _screen_name String true Added in all events. _screen_unique_id String true Added in all events."},{"location":"sdk-manual/swift/#change-log","title":"Change log","text":"<p>GitHub change log</p>"},{"location":"sdk-manual/swift/#reference-link","title":"Reference link","text":"<p>Source code</p> <p>Project issue</p> <p>API Documentation</p> <p>ClickstreamObjc Api Reference</p>"},{"location":"sdk-manual/user-identifier/","title":"User identifier","text":"<p>When you perform data analysis, you usually need to select an appropriate user identifier for analysis based on your business analysis scenario. This will help you improve the accuracy of your analysis, especially in funnel, retention, session and other analysis scenarios.</p> <p>Clickstream Analytics on AWS solution mainly contains three types of IDs, namely:</p> <ul> <li>User ID</li> <li>Device ID</li> <li>User Pseudo ID</li> </ul> <p>Next, we will introduce these three IDs respectively, and you will learn in detail how we use User ID and User Pseudo ID to correlate user behavior.</p>"},{"location":"sdk-manual/user-identifier/#user-id","title":"User ID","text":"<p>User ID is usually a unique identifier that describes the user in your business database, which is relatively more accurate and unique.</p> <ul> <li>When the user is not registered or logged in, the value of User ID is empty.</li> <li>The SDK provides the <code>ClickstreamAnalytics.setUserId(\"your user id\")</code> method to set the User ID. When logging out, set <code>null/nil</code> to clear the User ID.</li> <li>The User ID is stored in the <code>user_id</code> field in the user table.</li> </ul>"},{"location":"sdk-manual/user-identifier/#device-id","title":"Device ID","text":"<p>We identify user devices by Device ID.</p> <ul> <li>The Device ID will be automatically generated when the app is launched for the first time after integrating the SDK.</li> <li>The Device ID may not be the unique identifier of the device. Usually the Device ID may be regenerated after the user uninstalls the app or clears the cache on the web page.</li> <li>The Device ID is stored in the <code>device_id_list</code> field in the user table.</li> </ul> <p>The following table will introduce how the SDK on each end generates the Device ID.</p> SDK Types Generate Rules Storage Location Is Unique Android SDK By default, AndroidId is used as the Device ID. If the AndroidId cannot be obtained, a random UUID is used instead Stored in SharedPreference key-value pair file Usually, the AndroidId will not change even if the app is uninstalled and reinstalled. If using the UUID as Device ID, it will change after the user uninstalls and reinstall the app Swift SDK If your app has been authorized to obtain IDFA, use IDFA as the Device ID. Otherwise, use IDFV as the Device ID. If IDFV cannot be obtained, use a random UUID UserDefault key-value pair file Usually, the IDFA does not change even if the app is uninstalled and reinstalled.When using IDFV or UUID, the Device ID will change after the user uninstalls and reinstalls the app Web SDK By default, a random UUID is used as the device ID In the browser's localStorage Device ID will be regenerated after user clears browser cache"},{"location":"sdk-manual/user-identifier/#user-pseudo-id","title":"User Pseudo ID","text":"<p>Clickstream Analytics on AWS solution uses User Pseudo ID to correlate logged-in and non-logged-in behavior on the same device.</p> <ul> <li>User Pseudo IDs are generated from random UUIDs in all SDKs.</li> <li>User Pseudo ID will only be reassigned when a new user logs in on the current device. When switching to a user who has already logged in on the current device, it will revert to the User Pseudo ID of the previous user.</li> <li>The User Pseudo ID is stored in the <code>user_pseudo_id</code> field of the user table.</li> </ul> <p>The following table lists the correspondence between Device ID, User ID, and User Pseudo ID under various scenarios.</p> Sequence Events Device ID User ID User Pseudo ID 1 Install App S -- 1 2 Use the App S -- 1 3 Logged in user A S A 1 4 Use the App S A 1 5 Sign out and view S -- 1 6 Logged in user B S B 2 7 Use the App S B 2 8 Sign out and view S -- 2 9 Logged in user A S A 1 10 Use the App S A 1 11 Sign out and view S -- 1 12 Logged in user C S C 3 13 Use the App S C 3 <p>As you can see from the above table, we can count all the behavioral events of user A on device S when he did not log in and after logging in twice by looking for <code>user_pseudo_id</code>=1. Additionally, you can use User ID to join user clickstream data with data from your business systems to build a more complete customer data platform.</p> <p>Note</p> <p>When the user uninstalls the app or clears the browser cache, the relationship between the original User Pseudo ID and User ID will be cleared on the device, and a new User Pseudo ID will be generated on the device.</p>"},{"location":"sdk-manual/web/","title":"Clickstream Web SDK","text":""},{"location":"sdk-manual/web/#introduction","title":"Introduction","text":"<p>Clickstream Web SDK can help you easily collect click stream data from browser to your AWS environments through the data pipeline provisioned by this solution.</p> <p>The SDK is based on the amplify-js SDK core library and developed according to the amplify-js SDK plug-in specification. In addition, the SDK provides features that automatically collect common user events and attributes (for example, page view and first open) to accelerate data collection for users.</p>"},{"location":"sdk-manual/web/#integrate-the-sdk","title":"Integrate the SDK","text":""},{"location":"sdk-manual/web/#initialize-the-sdk","title":"Initialize the SDK","text":"Using NPMUsing JS File <p>1.Include the SDK</p> <pre><code>npm install @aws/clickstream-web\n</code></pre> <p>2.Initialize the SDK</p> <p>You need to configure the SDK with default information before using it. Copy your initialize code from your clickstream solution web console, we recommended you add the code to your app's root entry point, for example <code>index.js/app.tsx</code> in React or <code>main.ts</code> in Vue/Angular, the initialize code should look like as follows.</p> <pre><code>import { ClickstreamAnalytics } from '@aws/clickstream-web';\n\nClickstreamAnalytics.init({\n   appId: \"your appId\",\n   endpoint: \"https://example.com/collect\",\n});\n</code></pre> <p>Your <code>appId</code> and <code>endpoint</code> are already set up in it, you can also manually add this code snippet and replace the values of appId and endpoint after you registered app to a data pipeline in the Clickstream web console.</p> <p>1.Download the <code>clickstream-web.min.js</code> from the assets in GitHub Release page then copy it into your project.</p> <p>2.Add the following initial code into your <code>index.html</code>.</p> <pre><code>&lt;script src=\"clickstream-web.min.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n    window.ClickstreamAnalytics.init({\n        appId: 'your appId',\n        endpoint: 'https://example.com/collect',\n    })\n&lt;/script&gt;\n</code></pre> <p>You can find the <code>appId</code> and <code>endpoint</code> in the application detail page of the Clickstream web console. </p> <p>To lazy loading the SDK, use the <code>async</code> attribute and place the <code>ClickstreamAnalytics.init()</code> method after <code>window.onload</code> or <code>DOMContentLoaded</code>.</p>"},{"location":"sdk-manual/web/#start-using","title":"Start using","text":""},{"location":"sdk-manual/web/#record-event","title":"Record event","text":"<p>Add the following code where you need to record event.</p> <pre><code>import { ClickstreamAnalytics } from '@aws/clickstream-web';\n\n// record event with attributes\nClickstreamAnalytics.record({\n  name: 'button_click',\n  attributes: { \n    category: 'shoes', \n    currency: 'CNY',\n    value: 279.9,\n  }\n});\n\n//record event with name\nClickstreamAnalytics.record({ name: 'button_click' });\n</code></pre>"},{"location":"sdk-manual/web/#login-and-logout","title":"Login and logout","text":"<pre><code>import { ClickstreamAnalytics } from '@aws/clickstream-web';\n\n// when user login success.\nClickstreamAnalytics.setUserId(\"1234\");\n\n// when user logout\nClickstreamAnalytics.setUserId(null);\n</code></pre>"},{"location":"sdk-manual/web/#add-user-attribute","title":"Add user attribute","text":"<pre><code>ClickstreamAnalytics.setUserAttributes({\n  userName:\"carl\",\n  userAge: 22\n});\n</code></pre> <p>Current login user's attributes will be cached in localStorage, so the next time browser open you don't need to set all user's attribute again, of course you can use the same api <code>ClickstreamAnalytics.setUserAttributes()</code> to update the current user's attribute when it changes.</p> <p>Important</p> <p>If your application is already published and most users have already logged in, please manually set the user attributes once when integrate the Clickstream SDK for the first time to ensure that subsequent events contains user attributes.</p>"},{"location":"sdk-manual/web/#record-event-with-items","title":"Record event with items","text":"<p>You can add the following code to log an event with an item.</p> <pre><code>import { ClickstreamAnalytics, Item } from '@aws/clickstream-web';\n\nconst itemBook: Item = {\n  id: '123',\n  name: 'Nature',\n  category: 'book',\n  price: 99,\n};\n\nClickstreamAnalytics.record({\n  name: 'view_item',\n  attributes: {\n    currency: 'USD',\n    event_category: 'recommended',\n  },\n  items: [itemBook],\n});\n</code></pre> <p>For logging more attribute in an item, please refer to item attributes.</p>"},{"location":"sdk-manual/web/#send-event-immediate-in-batch-mode","title":"Send event immediate in batch mode","text":"<p>When you are in batch mode, you can still send an event immediately by setting the <code>isImmediate</code> attribute to <code>true</code>, as the following code.</p> <pre><code>import { ClickstreamAnalytics } from '@aws/clickstream-web';\n\nClickstreamAnalytics.record({\n  name: 'button_click',\n  isImmediate: true,\n});\n</code></pre>"},{"location":"sdk-manual/web/#other-configurations","title":"Other configurations","text":"<p>In addition to the required <code>appId</code> and <code>endpoint</code>, you can configure other information to get more customized usage:</p> <pre><code>import { ClickstreamAnalytics, SendMode, PageType } from '@aws/clickstream-web';\n\nClickstreamAnalytics.init({\n   appId: \"your appId\",\n   endpoint: \"https://example.com/collect\",\n   sendMode: SendMode.Batch,\n   sendEventsInterval: 5000,\n   isTrackPageViewEvents: true,\n   isTrackUserEngagementEvents: true,\n   isTrackClickEvents: true,\n   isTrackSearchEvents: true,\n   isTrackScrollEvents: true,\n   pageType: PageType.SPA,\n   isLogEvents: false,\n   authCookie: \"your auth cookie\",\n   sessionTimeoutDuration: 1800000,\n   searchKeyWords: ['product', 'class'],\n   domainList: ['example1.com', 'example2.com'],\n});\n</code></pre> <p>Here is an explanation of each option:</p> Name Required Default value Description appId true -- the app id of your application in control plane endpoint true -- the endpoint path you will upload the event to Clickstream ingestion server sendMode false Immediate there are two ways for send events <code>Immediate</code> and <code>Batch</code> sendEventsInterval false 5000 event sending interval millisecond, works only in <code>Batch</code> mode isTrackPageViewEvents false true whether auto record page view events in browser isTrackUserEngagementEvents false true whether auto record user engagement events in browser isTrackClickEvents false true whether auto record link click events in browser isTrackSearchEvents false true whether auto record search result page events in browser isTrackScrollEvents false true whether auto record page scroll events in browser pageType false SPA the website type, <code>SPA</code> for single page application, <code>multiPageApp</code> for multiple page application. This attribute works only when the value of attribute <code>isTrackPageViewEvents</code> is <code>true</code> isLogEvents false false whether to print out event json in console for debugging authCookie false -- your auth cookie for AWS application load balancer auth cookie sessionTimeoutDuration false 1800000 the duration for session timeout milliseconds searchKeyWords false -- the customized Keywords for trigger the <code>_search</code> event, by default we detect <code>q</code>, <code>s</code>, <code>search</code>, <code>query</code> and <code>keyword</code> in query parameters domainList false -- if your website cross multiple domain, you can customize the domain list. The <code>_outbound</code> attribute of the <code>_click</code> event will be true when a link leads to a website that's not a part of your configured domain."},{"location":"sdk-manual/web/#configuration-update","title":"Configuration update","text":"<p>You can update the default configuration after initializing the SDK, below are the additional configuration options you can customize.</p> <pre><code>import { ClickstreamAnalytics } from '@aws/clickstream-web';\n\nClickstreamAnalytics.updateConfigure({\n  isLogEvents: true,\n  authCookie: 'your auth cookie',\n  isTrackPageViewEvents: false,\n  isTrackUserEngagementEvents: false,\n  isTrackClickEvents: false,\n  isTrackScrollEvents: false,\n  isTrackSearchEvents: false,\n});\n</code></pre>"},{"location":"sdk-manual/web/#debug-events","title":"Debug events","text":"<p>You can follow the steps below to view the event raw json and debug your events.</p> <ol> <li>Using <code>ClickstreamAnalytics.init()</code> api and set the <code>isLogEvents</code> attribute to true in debug mode.</li> <li>Integrate the SDK and launch your web application in a browser, then open the Inspection page and switch to console tab.</li> <li>Input <code>EventRecorder</code> to Filter, and you will see the json content of all events recorded by Clickstream Web SDK.</li> </ol>"},{"location":"sdk-manual/web/#data-format-definition","title":"Data format definition","text":""},{"location":"sdk-manual/web/#data-types","title":"Data types","text":"<p>Clickstream Web SDK supports the following data types:</p> Data type Range Sample number 5e-324~1.79e+308 12, 26854775808, 3.14 boolean true\u3001false true string max support 1024 characters \"clickstream\""},{"location":"sdk-manual/web/#naming-rules","title":"Naming rules","text":"<ol> <li> <p>The event name and attribute name cannot start with a number, and only contains: uppercase and lowercase letters, numbers, underscores, if the event name is invalid, the SDK will record <code>_clickstream_error</code> event, if the attribute or user attribute name is invalid, the attribute will be discarded and also record <code>_clickstream_error</code> event.</p> </li> <li> <p>Do not use <code>_</code> as prefix to naming event name and attribute name, <code>_</code> is the preset from Clickstream Analytics.</p> </li> <li> <p>The event name and attribute name are in case-sensitive, so the event <code>Add_to_cart</code> and <code>add_to_cart</code> will be recognized as two different event.</p> </li> </ol>"},{"location":"sdk-manual/web/#event-and-attribute-limitation","title":"Event and attribute limitation","text":"<p>In order to improve the efficiency of querying and analysis, we apply limits to event data as follows:</p> Name Suggestion Hard limit Strategy Error code Event name invalid -- -- discard event, print log and record <code>_clickstream_error</code> event 1001 Length of event name under 25 characters 50 characters discard event, print log and record <code>_clickstream_error</code> event 1002 Length of event attribute name under 25 characters 50 characters discard the attribute,  print log and record error in event attribute 2001 Attribute name invalid -- -- discard the attribute,  print log and record error in event attribute 2002 Length of event attribute value under 100 characters 1024 characters discard the attribute,  print log and record error in event attribute 2003 Event attribute per event under 50 attributes 500 evnet attributes discard the attribute that exceed, print log and record error in event attribute 2004 User attribute number under 25 attributes 100 user attributes discard the attribute that exceed, print log and record <code>_clickstream_error</code> event 3001 Length of User attribute name under 25 characters 50 characters discard the attribute, print log and record <code>_clickstream_error</code> event 3002 User attribute name invalid -- -- discard the attribute, print log and record <code>_clickstream_error</code> event 3003 Length of User attribute value under 50 characters 256 characters discard the attribute, print log and record <code>_clickstream_error</code> event 3004 Item Number in one event under 50 items 100 items discard the item that exceed, print log and record error in event attribute 4001 Length of item attribute value under 100 characters 256 characters discard the item attribute, print log and record error in event attribute 4002 <p>Important</p> <ul> <li>The character limits are the same for single-width character languages (e.g., English) and double-width character languages (e.g., Chinese).</li> <li>The limit of event attribute per event include preset attributes.</li> <li>If the attribute or user attribute with the same name is added more than twice, the latest value will apply.</li> <li>All errors that exceed the limit will be recorded <code>_error_code</code> and <code>_error_message</code> these two attribute in the event attributes.</li> </ul>"},{"location":"sdk-manual/web/#preset-events","title":"Preset events","text":""},{"location":"sdk-manual/web/#automatically-collected-events","title":"Automatically collected events","text":"Event name Triggered Event Attributes _first_open the first time user launches the site in a browser _session_start when a user first visit the site or a user returns to the website after 30 minutes of inactivity period, Learn more 1. _session_id 2. _session_start_timestamp _page_view when new page is opens, Learn more 1. _page_referrer2. _page_referrer_title3. _entrances4. _previous_timestamp5. _engagement_time_msec _user_engagement when user navigates away from current webpage and the page is in focus for at least one second, Learn more 1._engagement_time_msec _app_start every time the browser goes to visible 1. _is_first_time(when it is the first <code>_app_start</code> event after the application starts, the value is <code>true</code>) _app_end every time the browser goes to invisible _profile_set when the <code>addUserAttributes()</code> or <code>setUserId()</code> api called _scroll the first time a user reaches the bottom of each page (i.e., when a 90% vertical depth becomes visible) 1. _engagement_time_msec _search each time a user performs a site search, indicated by the presence of a URL query parameter, by default we detect <code>q</code>, <code>s</code>, <code>search</code>, <code>query</code> and <code>keyword</code> in query parameters 1. _search_key (the keyword name)2. _search_term (the search content) _click each time a user clicks a link that leads away from the current domain (or configured domain list) 1. _link_classes (the content of <code>class</code> in tag <code>&lt;a&gt;</code> )2. _link_domain (the domain of <code>herf</code> in tag <code>&lt;a&gt;</code> )3. _link_id (the content of <code>id</code> in tag <code>&lt;a&gt;</code> )4. _link_url (the content of <code>herf</code> in tag <code>&lt;a&gt;</code> )5. _outbound (if the domain is not in configured domain list, the attribute value is <code>true</code>) _clickstream_error event_name is invalid or user attribute is invalid 1. _error_code 2. _error_message"},{"location":"sdk-manual/web/#session-definition","title":"Session definition","text":"<p>In Clickstream Web SDK, we do not limit the total time of a session, as long as the time between the next entry of the browser and the last exit time is within the allowable timeout period, we consider the current session to be continuous. </p> <p>The <code>_session_start</code> event triggered when the website open for the first time, or the browser was open to the foreground and the time between the last exit exceeded <code>session_time_out</code> period, and the following are session-related attributes.</p> <ol> <li> <p>_session_id: We calculate the session id by concatenating the last 8 characters of uniqueId and the current millisecond, for example: dc7a7a18-20230905-131926703.</p> </li> <li> <p>_session_duration : We calculate the session duration by minus the current event create timestamp and the session's <code>_session_start_timestamp</code>, this attribute will be added in every event during the session.</p> </li> <li> <p>_session_number : The auto increment number of session in current browser, the initial value is 1</p> </li> <li> <p>Session timeout duration: By default is 30 minutes, which can be customized through the configuration api.</p> </li> </ol>"},{"location":"sdk-manual/web/#page-view-definition","title":"Page view definition","text":"<p>In Clickstream Web SDK, we define the <code>_page_view</code> as an event that records a user's browsing path of page, when a page transition started, the <code>_page_view</code> event will be recorded when meet any of the following conditions:</p> <ol> <li>No page was previously set.</li> <li>The new page title differs from the previous page title.</li> <li>The new page url differ from the previous page url.</li> </ol> <p>This event listens for <code>pushState</code>, <code>popState</code> in history, and <code>replaceState</code> of window to judgment the page transition. In order to track page browsing path, we use <code>_page_referrer</code>(last page url) and <code>page_referrer_title</code> to link the previous page. In addition, there are some other attributes in page view event.</p> <ol> <li>_entrances: The first page view event in a session is 1, others is 0.</li> <li>_previous_timestamp: The timestamp of the previous <code>_page_view</code> event.</li> <li>_engagement_time_msec: The previous page last engagement milliseconds.</li> </ol>"},{"location":"sdk-manual/web/#user-engagement-definition","title":"User engagement definition","text":"<p>In Clickstream Web SDK, we define the <code>_user_engagement</code> as an event that records the page browsing time, and we only send this event when user leave the page and the page has focus for at least one second.</p> <p>We define that users leave the page in the following situations.</p> <ol> <li>When the user navigates to another page under the current domain.</li> <li>When user click a link that leads away from the current domain. </li> <li>When user click another browser tab or minimize the current browser window.</li> <li>When user close the website tab or close the browser application.</li> </ol> <p>engagement_time_msec: We calculate the milliseconds from when a page is visible to when the user leave the page.</p>"},{"location":"sdk-manual/web/#event-attributes","title":"Event attributes","text":""},{"location":"sdk-manual/web/#sample-event-structure","title":"Sample event structure","text":"<pre><code>{\n    \"hashCode\": \"80452b0\",\n    \"unique_id\": \"c84ad28d-16a8-4af4-a331-f34cdc7a7a18\",\n    \"event_type\": \"add_to_cart\",\n    \"event_id\": \"460daa08-0717-4385-8f2e-acb5bd019ee7\",\n    \"timestamp\": 1667877566697,\n    \"device_id\": \"f24bec657ea8eff7\",\n    \"platform\": \"Web\",\n    \"make\": \"Google Inc.\",\n    \"locale\": \"zh_CN\",\n    \"screen_height\": 1080,\n    \"screen_width\": 1920,\n    \"viewport_height\": 980,\n    \"viewport_width\": 1520,\n    \"zone_offset\": 28800000,\n    \"system_language\": \"zh\",\n    \"country_code\": \"CN\",\n    \"sdk_version\": \"0.2.0\",\n    \"sdk_name\": \"aws-solution-clickstream-sdk\",\n    \"host_name\": \"https://example.com\",\n    \"app_id\": \"appId\",\n    \"items\": [{\n        \"id\": \"123\",\n        \"name\": \"Nike\",\n        \"category\": \"shoes\",\n        \"price\": 279.9\n    }],\n    \"user\": {\n        \"_user_id\": {\n            \"value\": \"312121\",\n            \"set_timestamp\": 1667877566697\n        },\n        \"_user_name\": {\n            \"value\": \"carl\",\n            \"set_timestamp\": 1667877566697\n        },\n        \"_user_first_touch_timestamp\": {\n            \"value\": 1667877267895,\n            \"set_timestamp\": 1667877566697\n        }\n    },\n    \"attributes\": {\n        \"event_category\": \"recommended\",\n        \"currency\": \"CNY\",\n        \"_session_id\": \"dc7a7a18-20221108-031926703\",\n        \"_session_start_timestamp\": 1667877566703,\n        \"_session_duration\": 391809,\n        \"_session_number\": 1,\n        \"_latest_referrer\": \"https://amazon.com/s?k=nike\",\n        \"_latest_referrer_host\": \"amazon.com\",\n        \"_page_title\": \"index\",\n        \"_page_url\": \"https://example.com/index.html\"\n    }\n}\n</code></pre> <p>All user attributes will be stored in <code>user</code> object, and all custom attributes are in <code>attributes</code> object.</p>"},{"location":"sdk-manual/web/#common-attributes","title":"Common attributes","text":"Attribute name Data type Description How to generate Usage and purpose hashCode string the event object's hash code calculated by library <code>@aws-crypto/sha256-js</code> distinguish different events app_id string the app_id for your app app_id was generated by clickstream solution when you register an app to a data pipeline identify the events for your apps unique_id string the unique id for user generated from <code>uuidV4()</code> when the sdk first initializationit will be changed if user logout and then login to a new user. When user re-login to the previous user in the same browser, the unique_Id will be reset to the same previous unique_id the unique id to identity different users and associating the behavior of logged-in and not logged-in device_id string the unique id for device generated from <code>uuidV4()</code> when the website is first open, then the uuid will stored in localStorage and will never be changed distinguish different devices event_type string event name set by developer or SDK distinguish different events type event_id string the unique id for event generated from <code>uuidV4()</code> when the event create distinguish different events timestamp number event create timestamp in millisecond generated from <code>new Date().getTime()</code> when event create data analysis needs platform string the platform name for browser is always <code>Web</code> data analysis needs make string the browser make generated from <code>window.navigator.product</code> or <code>window.navigator.vendor</code> data analysis needs screen_height number the screen height pixel generated from <code>window.screen.height</code> data analysis needs screen_width number the screen width pixel generated from <code>window.screen.width</code> data analysis needs viewport_height number the website viewport height pixel generated from <code>window.innerHeight</code> data analysis needs viewport_width number the website viewport width pixel generated from <code>window.innerWidth</code> data analysis needs zone_offset number the device raw offset from GMT in milliseconds. generated from <code>-currentDate.getTimezoneOffset()*60000</code> data analysis needs locale string the default locale(language, country and variant) for the browser generated from <code>window.navigator.language</code> data analysis needs system_language string the browser language code generated from <code>window.navigator.language</code> data analysis needs country_code string country/region code for the browser generated from <code>window.navigator.language</code> data analysis needs sdk_version string clickstream sdk version generated from <code>package.json</code> data analysis needs sdk_name string clickstream sdk name this will always be <code>aws-solution-clickstream-sdk</code> data analysis needs host_name string the website hostname generated from <code>window.location.hostname</code> data analysis needs"},{"location":"sdk-manual/web/#user-attributes","title":"User attributes","text":"Attribute name Description _user_id Reserved for user id that is assigned by app _user_ltv_revenue Reserved for user lifetime value _user_ltv_currency Reserved for user lifetime value currency _user_first_touch_timestamp Added to the user object for all events. The time (in millisecond) when the user first visited the website."},{"location":"sdk-manual/web/#event-attributes_1","title":"Event attributes","text":"Attribute name Data type Auto   track Description _traffic_source_medium string false Reserved for traffic medium. Use this attribute to store the medium that acquired user when events were logged. Example: Email, Paid search, Search engine. _traffic_source_name string false Reserved for traffic name. Use this attribute to store the marketing campaign that acquired user when events were logged. Example: Summer promotion. _traffic_source_source string false Reserved for traffic source. Name of the network source that acquired the user when the event were reported. Example: Google, Facebook, Bing, Baidu. _session_id string true Added in all events. _session_start_timestamp number true Added in all events. The value is millisecond. _session_duration number true Added in all events. The value is millisecond. _session_number number true Added in all events. _page_title string true Added in all events. _page_url string true Added in all events. _latest_referrer string true Added in all events. The last off-site url. _latest_referrer_host string true Added in all events. The last off-site domain name."},{"location":"sdk-manual/web/#item-attributes","title":"Item attributes","text":"Attribute name Data type Required Description id string False The id of the item name string False The name of the item brand string False The brand of the item price number False The price of the item quantity string False The quantity of the item creative_name string False The creative name of the item creative_slot string False The creative slot of the item location_id string False The location id of the item category string False The category of the item category2 string False The category2 of the item category3 string False The category3 of the item category4 string False The category4 of the item category5 string False The category5 of the item <p>All attributes in item can only be the attributes in the above table, customization is not supported. Instead, you can use <code>category2</code> to <code>category5</code> to represent the custom attributes.</p>"},{"location":"sdk-manual/web/#change-log","title":"Change log","text":"<p>GitHub Change log</p>"},{"location":"sdk-manual/web/#reference-link","title":"Reference link","text":"<p>Source code</p> <p>Project issue</p>"},{"location":"sdk-manual/wechat/","title":"Clickstream WeChat Miniprogram SDK","text":""},{"location":"sdk-manual/wechat/#introduction","title":"Introduction","text":"<p>Clickstream WeChat Miniprogram SDK can help you easily collect click stream data from WeChat Miniprogram to your AWS environments through the data pipeline provisioned by this solution. This SDK is part of an AWS solution - Clickstream Analytics on AWS, which provisions data pipeline to ingest and process event data into AWS services such as S3, Redshift.</p> <p>The SDK leverages WeChat Mini Program framework and APIs. It provides features that automatically collect common user events and attributes (for example, page view and first open) to accelerate data collection for users.</p>"},{"location":"sdk-manual/wechat/#usage-guidance","title":"Usage Guidance","text":""},{"location":"sdk-manual/wechat/#initialize-the-sdk","title":"Initialize the SDK","text":"<p>1.Download the <code>clickstream-wechat.min.js</code> from the assets in GitHub Release page then copy it into your project.</p> <p>2.The SDK should be initialized with necessary configurations before it can work with Clickstream Analytics solution. Take TypeScript mini program project for example, add following code snippet in the app.ts file BEFORE default <code>App()</code> method and fill in <code>appId</code> and <code>endpoint</code> values, which can be got from Clickstream web console after registering the app to a Clickstream Analytics data pipeline.</p> <pre><code>import { ClickstreamAnalytics } from './clickstream-wechat';\n\nClickstreamAnalytics.init({\n    appId: \"your appId\",\n    endpoint: \"https://example.com/collect\"\n});\n</code></pre> <p>In addition to the required configuration <code>appId</code> and <code>endpoint</code>, there are optional configuration properties used for customizing the SDK.</p> Property Name Required Default Value Description appId yes - appId of the project in Clickstream Analytics control plane endpoint yes - the ingestion server endpoint sendMode no SendMode.Immediate options: SendMode.Immediate, SendMode.Batch sendEventsInterval no 5000 interval (in milliseconds) of sending events, only works for batch send mode autoTrackAppStart no true whether auto record app view event autoTrackAppEnd no true whether auto record app hide event autoTrackPageShow no true whether auto record page view event autoTrackUserEngagement no true whether auto record user engagement autoTrackMPShare no false whether auto record when user shares mini program autoTrackMPFavorite no false whether auto record when user adds mini program to favorites debug no false whether print out logs in the console authCookie no - auth cookie for AWS application load balancer auth sessionTimeoutDuration no 1800000 session timeout duration in millisecond <p>The SDK configurations can be updated after initialization by calling <code>configure()</code> method</p> <pre><code>import { ClickstreamAnalytics, SendMode } from './clickstream-wechat';\n\nClickstreamAnalytics.configure({\n  appId: \"your appId\",\n  endpoint: \"https://example.com/collect\",\n  sendMode: SendMode.Batch,\n  debug: true,\n  authCookie: 'auth cookie',\n  autoTrackPageShow: false\n});\n</code></pre>"},{"location":"sdk-manual/wechat/#use-the-sdk","title":"Use the SDK","text":""},{"location":"sdk-manual/wechat/#add-user-info","title":"Add User Info","text":"<pre><code>// add or update user attributes\nClickstreamAnalytics.setUserAttributes({\n  name:\"carl\",\n  age: 22\n});\n\n// when user login\nClickstreamAnalytics.setUserId(\"UserId\");\n\n// when user logout\nClickstreamAnalytics.setUserId(null);\n</code></pre> <p>Current login user's attributes will be cached in wxStorage.</p>"},{"location":"sdk-manual/wechat/#record-event","title":"Record Event","text":"<p>You can call <code>ClickstreamAnalytics.record()</code> method to record custom event. The property <code>name</code> is required, while the property <code>attributes</code> and <code>items</code> are optional. <code>attributes</code> property is an object, <code>items</code> property is an array list of <code>item</code> type object. For logging more attributes in an item, please refer to item attributes.</p> <p>Custom event record samples:</p> <pre><code>ClickstreamAnalytics.record({ name: 'albumVisit' });\n\nClickstreamAnalytics.record({\n  name: 'buttonClick',\n  attributes: { buttonName: 'confirm', itemNo: 12345, inStock: true },\n  items: [\n    {\n      id: 'p_123',\n      name: 'item_name',\n      price: 168.99\n    }\n  ]\n});\n</code></pre>"},{"location":"sdk-manual/wechat/#debug-events","title":"Debug events","text":"<p>You can follow the steps below to view the event raw json and debug your events.</p> <ol> <li>Use <code>ClickstreamAnalytics.configure()</code> api to set debug to <code>true</code>, which will enable the SDK debug mode.</li> <li>Integrate the SDK and launch your WeChat mini program on device or Weixin DevTools.</li> <li>In the Console tab, you can find the json content of all the events recorded by the SDK.</li> </ol>"},{"location":"sdk-manual/wechat/#data-format-definition","title":"Data format definition","text":""},{"location":"sdk-manual/wechat/#data-types","title":"Data types","text":"<p>Clickstream WeChat Miniprogram SDK supports following data types:</p> Data type Range Sample number 5e-324~1.79e+308 12, 26854775808, 3.14 boolean true\u3001false true string max support 1024 characters \"clickstream\""},{"location":"sdk-manual/wechat/#naming-rules","title":"Naming rules","text":"<ol> <li> <p>The event and attribute names cannot start with a number, and should only consist of alphanumeric characters and underscores. If the event name is invalid, the event won't be sent. If the name of any attribute is invalid, the event will be sent excluding the invalid attribute. In both cases, the SDK will record a <code>_clickstream_error</code> event.</p> </li> <li> <p>Do NOT use <code>_</code> as prefix of event or attribute name, those names are reserved for Clickstream Analytics preset events and attributes.</p> </li> <li> <p>The event and attribute names are case-sensitive, e.g. the event <code>Add_to_cart</code> and <code>add_to_cart</code> are recognized as two different events.</p> </li> </ol>"},{"location":"sdk-manual/wechat/#event-and-attribute-limitation","title":"Event and attribute limitation","text":"<p>In order to improve the efficiency of querying and analysis, we apply limits to event data as follows:</p> Name Suggestion Hard Limit Strategy Error Code EVENT_NAME_INVALID -- -- record <code>_clickstream_error</code> event 1001 EVENT_NAME_LENGTH_EXCEED &lt; 25 characters 50 characters discard, record error event 1002 ATTRIBUTE_NAME_LENGTH_EXCEED &lt; 25 characters 50 characters discard, log and record error 2001 ATTRIBUTE_NAME_INVALID -- -- record <code>_clickstream_error</code>event 2002 ATTRIBUTE_VALUE_LENGTH_EXCEED &lt; 100 characters 1024 characters discard, log and record error 2003 ATTRIBUTE_SIZE_EXCEED &lt; 50 attributes 500 event attributes discard, log and record error 2004 USER_ATTRIBUTE_SIZE_EXCEED &lt; 25 attributes 100 user attributes discard, log and record error 3001 USER_ATTRIBUTE_NAME_LENGTH_EXCEED &lt; 25 characters 50 characters discard, log and record error 3002 USER_ATTRIBUTE_NAME_INVALID -- -- record <code>_clickstream_error</code> event 3003 USER_ATTRIBUTE_VALUE_LENGTH_EXCEED &lt; 50 characters 256 characters discard, log and record error 3004 ITEM_SIZE_EXCEED &lt; 50 items 100 items discard, log and record error 4001 ITEM_VALUE_LENGTH_EXCEED &lt; 100 characters 256 characters discard, log and record error 4002 <p>Important</p> <ul> <li>The character limits are the same for single-width character languages (e.g., English) and double-width character languages (e.g., Chinese).</li> <li>The limit of attribute number per event includes common attributes and preset attributes.</li> <li>In case one attribute is added to the event record more than once, the SDK will use the last value.</li> </ul>"},{"location":"sdk-manual/wechat/#preset-events","title":"Preset events","text":""},{"location":"sdk-manual/wechat/#automatically-collected-events","title":"Automatically collected events","text":"Event Name Trigger Event Attributes Description _session_start first launch WeChat mini program or re-open the mini program when the user is not using it for more than 30 minutes 1. _session_id 2. _session_start_timestamp3. _session_number4. _session_duration _session_id is generated from <code>uniqueId</code> and startTime _screen_view when a screen transition occurs and any of the following criteria are met: No screen was previously set The new screen name differs from the previous screen name The new screen-class name differs from the previous screen-class name The new screen id differs from the previous screen id 1. _screen_name2. _screen_id3. _previous_screen_name4. _previous_screen_id5. _engagement_time_msec _first_open the first time user launches the WeChat mini program only record once _app_start every time the mini program goes to visible 1. _is_first_time _app_end every time the mini program goes to invisible _user_engagement when the mini program user leaves current page or closes/hides app 1. _engagement_time_msec calculate user engagement duration in mini program page level _profile_set when <code>addUserAttributes()</code> or <code>setUserId()</code> API is invoked _mp_share when user shares the mini program to others _mp_favorite when user adds the mini program to favorites _clickstream_error when event_name is invalid or attribute name/value is invalid 1. _error_code 2. _error_message"},{"location":"sdk-manual/wechat/#session-definition","title":"Session definition","text":"<p>In Clickstream WeChat Miniprogram SDK, we do not set duration max limit for a session. If the user opens the mini program within a certain time period after they quit the mini program, we still consider it being in the same session.</p> <ol> <li> <p>_session_start: When the mini program is launched for the first time, or the mini program is open to the foreground and the time between the last exit exceeds <code>session_time_out</code> period.</p> </li> <li> <p>_session_duration: We calculate the <code>_session_duration</code> based on the current event creation timestamp and the session's <code>_session_start_timestamp</code>, this attribute will be added in every event during the session.</p> </li> <li> <p>session_time_out: 30 minutes by default, which can be customized via the configuration API.</p> </li> <li> <p>_session_number: The total number of distinct sessions (by session id), this attribute will be added in every event's attribute object.</p> </li> </ol>"},{"location":"sdk-manual/wechat/#user-engagement-definition","title":"User engagement definition","text":"<p>In Clickstream WeChat Miniprogram SDK, we define the <code>_user_engagement</code> as the mini program page remains in focus for more than one second. <code>_engagement_time_msec</code> is calculated against each page, the SDK will calculate the engagement duration starting from a page being opened until the page is hidden, closed or redirected to other pages.</p>"},{"location":"sdk-manual/wechat/#event-attributes","title":"Event attributes","text":""},{"location":"sdk-manual/wechat/#common-attributes","title":"Common Attributes","text":"Attribute name Sample Description How to generate app_id \"appId\" Clickstream appId, configured in the control plane SDK user should provide it by calling ClickstreamAnalytics <code>init()</code> or <code>configure()</code> method. unique_id \"c84ad28d-16a8-4af4-a331-f34cdc7a7a18\" the unique ID to identify mini program user, this can be used for associating behaviors while logged in and not logged in SDK user can set this attribute later, e.g. using WeChat openId device_id \"a843d34c-15a8-2af4-a332-b34cdc4a7a12\" uuid() generated when the mini program is first opened, then the device_id will be stored in the wxStorage and won't be modified. event_type \"_session_start\" event name set by user or SDK. event_id \"460daa08-0717-4385-8f2e-acb5bd019ee7\" the unique id for an event generated from <code>uuid()</code> when the event was created. timestamp 1667877566697 event creation timestamp generated from <code>new Date().getTime()</code> when the event was created platform \"WeChatMP\" the platform name for WeChat mini program it's always \"WeChatMP\" os_name \"iOS\" the device OS name got from <code>wx.getSystemInfoSync()</code> API os_version \"iOS 12.0.1\" the device OS version got from <code>wx.getSystemInfoSync()</code> API wechat_version \"8.0.5\" the WeChat version got from <code>wx.getSystemInfoSync()</code> API wechat_sdk_version \"3.0.0\" the WeChat SDK version got from <code>wx.getSystemInfoSync()</code> API brand Google Inc. the brand of the device got from <code>wx.getSystemInfoSync()</code> API model \"iPhone 6/7/8\" the model of the device got from <code>wx.getSystemInfoSync()</code> API system_language \"zh\" the device language code got from <code>wx.getSystemInfoSync()</code> API screen_height 667 the height pixel of the device screen got from <code>wx.getSystemInfoSync()</code> API screen_width 375 the width pixel of the device screen got from <code>wx.getSystemInfoSync()</code> API zone_offset 28800000 the device raw offset from GMT in milliseconds <code>-currentDate.getTimezoneOffset() * 60 * 1000</code> network_type \"wifi\" the network type of the device got from <code>wx.getNetworkType()</code> API sdk_version \"0.2.0\" Clickstream SDK version get the version in package.json sdk_name \"aws-solution-clickstream-sdk\" Clickstream sdk name always be \"aws-solution-clickstream-sdk\" app_package_name \"wxbd614036ba3d1f05\" WeChat mini program Id got from <code>wx.getAccountInfoSync()</code> API app_version \"1.0.1\" WeChat mini program version got from <code>wx.getAccountInfoSync()</code> API"},{"location":"sdk-manual/wechat/#reserved-attributes","title":"Reserved Attributes","text":"<p>User attributes</p> Attribute Name Required Description _user_id yes user id _user_name yes user name _user_first_touch_timestamp yes when user first used the app <p>Reserved attributes</p> Attribute Name Required Description _error_code no error code, reserved for _clickstream_error event _error_message no error reason, reserved for _clickstream_error event _session_id yes session ID _session_start_timestamp yes session start time _session_duration yes session duration _session_number yes calculated by device, starting from 1. _screen_name no current page title _screen_id no current page id _screen_route no current page route _previous_screen_name no last viewed page title _previous_screen_id no last viewed page id _previous_screen_route no last viewed page route _engagement_time_msec no user engagement duration on the current page _is_first_time no <code>true</code> for the first <code>_app_start</code> event after the mini program starts"},{"location":"sdk-manual/wechat/#item-attributes","title":"Item attributes","text":"Attribute name Data Type Required Description id string no The id of the item name string no The name of the item brand string no The brand of the item price string | number no The price of the item quantity number no The quantity of the item creative_name string no The creative name of the item creative_slot string no The creative slot of the item location_id string no The location id of the item category string no The category of the item category2 string no The category2 of the item category3 string no The category3 of the item category4 string no The category4 of the item category5 string no The category5 of the item"},{"location":"sdk-manual/wechat/#sample-event-structure","title":"Sample event structure","text":"<pre><code>{\n  \"app_id\": \"appId\",\n  \"unique_id\": \"c84ad28d-16a8-4af4-a331-f34cdc7a7a18\",\n  \"device_id\": \"be4b3f1e-b2a8-4b7b-9055-4257e3e313c8\",\n  \"event_type\": \"_screen_view\",\n  \"event_id\": \"460daa08-0717-4385-8f2e-acb5bd019ee7\",\n  \"timestamp\": 1667877566697,\n  \"platform\": \"WeChatMP\",\n  \"os_name\": \"iOS\",\n  \"os_version\": \"iOS 12.0.1\",\n  \"wechat_version\": \"8.0.5\",\n  \"wechat_sdk_version\": \"3.0.0\",\n  \"brand\": \"devtools\",\n  \"model\": \"iPhone 6/7/8\",\n  \"system_language\": \"en\",\n  \"screen_height\": 667,\n  \"screen_width\": 375,\n  \"zone_offset\": 28800000,\n  \"network_type\": \"wifi\",\n  \"sdk_version\": \"0.2.0\",\n  \"sdk_name\": \"aws-solution-clickstream-sdk\",\n  \"app_version\": \"1.0.1\",\n  \"app_package_name\": \"wxbd614036ba3d1f05\",\n  \"user\": {\n    \"_user_id\": {\n      \"value\":\"312121\",\n      \"set_timestamp\": 1667877566697\n    },\n    \"_user_name\": {\n      \"value\":\"carl\",\n      \"set_timestamp\": 1667877566697\n    },\n    \"_user_first_touch_timestamp\": {\n      \"value\":1667877267895,\n      \"set_timestamp\": 1667877566697\n    }\n  },\n  \"attributes\": {\n    \"_session_id\":\"dc7a7a18-20221108-031926703\",\n    \"_session_start_timestamp\": 1667877566703,\n    \"_session_duration\": 391809,\n    \"_session_number\": 1,\n    \"_previous_screen_id\": \"pages/category/index\",\n    \"_screen_id\": \"pages/items/11223/detail\",\n    \"_engagement_time_msec\": 30000\n  },\n  \"items\": [\n    {\n      \"id\": \"p_123\",\n      \"name\": \"item_name\",\n      \"price\": 168.99\n    }\n  ]\n}\n</code></pre>"},{"location":"sdk-manual/wechat/#change-log","title":"Change log","text":"<p>GitHub Change log</p>"},{"location":"sdk-manual/wechat/#reference-link","title":"Reference link","text":"<p>Source code</p> <p>Project issue</p>"},{"location":"solution-overview/","title":"Overview","text":"<p>The Clickstream Analytics on AWS solution allows you to collect, ingest, process and analyze click stream data from websites and mobile applications in your AWS environment. You can use the solution to create an analytics platform that fits your organizational needs, and maintain complete ownership and control over the valuable user behavior data. The solution can be applied to various use cases such as user behavior analysis and marketing analysis to improve website and application's performance.</p> <p>The solution provides modularized and configurable components of a data pipeline so that you can accelerate the building of a Well-Architected data pipeline from weeks to minutes. The purpose-built SDKs and guidance allow you to collect click stream data from different application platforms (for example, Android, iOS, and JavaScript) to AWS. In addition, the solution provides out-of-the-box dashboards and explorative analytics models to enable you to derive actionable business insights easily and quickly.</p> <p> </p> <p>Use this navigation table to quickly find answers to these questions:</p> If you want to \u2026 Read\u2026 Know the cost for running this solution Cost Understand the security considerations for this solution Security Know which AWS Regions are supported for this solution Supported AWS Regions Get started with the solution quickly to build an end-to-end data pipeline, send data into the pipeline, and then view the out-of-the-box dashboards Getting Started Learn the concepts related to pipeline, and how to manage a data pipeline throughout its lifecycle Pipeline Management Learn to use Analytics Studio to uncover insights from clickstream data Analytics Stuido <p>The guide is intended for IT architects, data engineers, developers, DevOps, and data product managers with practical experience architecting in the AWS Cloud.</p>"},{"location":"solution-overview/features-and-benefits/","title":"Features and benefits","text":"<p>The solution includes the following key features:</p> <ul> <li>Automatic data pipeline builder. You can easily define a data pipeline from a web-based UI console. The solution will take care of the underlying infrastructure creation, required security setup, and data integrations. Each pipeline module is built with various features and designed to be loosely-coupled, making it flexible for you to customize for specific use cases. </li> <li>Purposed-built SDKs. The SDKs are optimized for collecting data from Android, iOS, and JavaScript platforms, which automatically collect common events (for example, first visit, screen view), support built-in local cache, retry, and verification mechanisms to ensure high completeness of data transmission.</li> <li>Out-of-the-box analytics. The solution offers built-in user life cycle dashboard (for example, acquisition, engagement, retention, and user demographic) and explorative analytics models (for example, funnel analysis, path analysis, event analysis, and retention analysis), powering various critical business analytics use cases such as user behavior analytics, marketing analytics, and product analytics.</li> </ul>"},{"location":"solution-overview/use-cases/","title":"Use cases","text":"<p>Clickstream data play a pivotal role in numerous online business analytics use cases, and the Clickstream Analytics on AWS can be applied to the following: </p> <ul> <li>User behavior analytics: Clickstream data in user behavior analytics provides insights into the sequential and chronological patterns of user interactions on a website or application, helping businesses understand user navigation, preferences, and engagement levels to enhance the overall product experience and drive product innovation.</li> <li>First-party customer data platform (CDP): Clickstream data, together with other business data sources (for example, order history, and user profile), allow customers to create a first-party customer data platform that offers a comprehensive view of their users, enabling businesses to personalize customer experiences, optimize customer journeys, and deliver targeted marketing messages.</li> <li>Marketing analytics: Clickstream data in marketing analytics offers detailed information about users' traffic sources, click paths and interactions with marketing campaigns, enabling businesses to measure campaign effectiveness, optimize marketing strategies, and enhance conversion rates.</li> </ul>"}]}