{
  "pipelines": "Pipelines",
  "basic": "Basic information",
  "id": "ID",
  "name": "Name",
  "desc": "Description",
  "status": "Status",
  "creationMethod": "Creation method",
  "lastEditDate": "Last edit date",
  "tagDesc": "Those tags will be propagated to all the resource in this pipeline",
  "valid": {
    "nameEmpty": "Please input pipeline name.",
    "regionEmpty": "Please select region.",
    "vpcEmpty": "Please select VPC.",
    "sdkEmpty": "Please specify which type of SDK you will use to send data to this data pipeline",
    "s3BucketEmpty": "Please select Amazon S3 Bucket",
    "publicSubnetEmpty": "Please select at least 2 public subnets cross AZs for the ingestion endpoint",
    "privateSubnetEmpty": "Please select at least 2 private subnets cross AZs for the ingestion endpoint",
    "privateSubnetDiffWithPublicError": "Private subnets must in the same AZs with public subnets.",
    "domainNameEmpty": "Please input domain name",
    "domainNameInvalid": "Domain name format invalid",
    "certificateEmpty": "Please select SSL certificate",
    "dataProcessorIntervalError": "Data processing interval could not be less than 3 minutes",
    "acknowledgeHTTPSecurity": "Please click acknowledge button to continue.",
    "sinkIntervalError": "The interval value must be greater than {{min}} and less than {{max}}",
    "sinkSizeError": "The size value must be greater than {{min}} and less than {{max}}",
    "minCapacityError": "Minimum capacity must be an integer and larger or equal than 1",
    "maxCapacityError": "Maximum capacity must be an integer and larger than minimum capacity",
    "warmPoolError": "Warm pool must be an integer and smaller than maximum capacity",
    "corsFormatError": "CORS domain format error",
    "secretEmptyError": "Please select a secret",
    "mskEmptyError": "Please select MSK Cluster",
    "topicFormatError": "Topic format error",
    "kafkaBrokerEmptyError": "Please input the Kafka brokers separated by a comma. For example, broker1:port,broker2:port",
    "kafkaBrokerFormatError": "The Kafka brokers separated by a comma. For example, broker1:port,broker2:port",
    "kafkaSGEmptyError": "Please select security group",
    "bufferS3PrefixError": "S3 prefix length must less than 1024 characters.",
    "bufferS3SizeError": "Buffer size number must between in 1 and 50.",
    "bufferS3IntervalError": "Buffer interval number must between in 60 and 3600.",
    "bufferKDSModeEmptyError": "Please select provision mode",
    "bufferKDSShardNumError": "Shard number must between in 1 and 10000",
    "redshiftServerlessVpcEmptyError": "Please select a vpc",
    "redshiftServerlessSGEmptyError": "Please select security groups",
    "redshiftServerlessSubnetEmptyError": "Please select subnets",
    "redshiftServerlessSubnetInvalidError": "Please select at least 3 subnets locates in at least 2 different AZ",
    "redshiftProvisionedClusterEmptyError": "Please select a cluster",
    "redshiftProvisionedDBUserEmptyError": "Please input the database user.",
    "redshiftProvisionedDBUserFormatError": "Database user format error",
    "quickSightUserEmptyError": "Please select a QuickSight user.",
    "emailInvalid": "Email invalid."
  },
  "create": {
    "basicInfo": "Basic configuration",
    "configIngestion": "Data ingestion",
    "dataProcessor": "Data processing & modeling",
    "reporting": "Reporting",
    "reviewLaunch": "Review and launch",
    "selectFile": "Select a mapping file",
    "selectFileDesc": "Please make sure you use the provided template",
    "selectFileConstraint": "Maximum file size is 25MB",
    "chooseFile": "Choose file",
    "fileSize": "File size in bytes Last date modified",
    "usingUIAlert": "Any field not specified here will be added as an parameter into event_param field.",
    "usingUIInfo": "You can add up to 42 more key.",
    "usingUIAdd": "Add new mapping",
    "dataKey": "Your data key",
    "dataValue": "Data model value",
    "enterKey": "Enter key",
    "enterValue": "Enter value",
    "noItems": "No items associated with the resource.",
    "s3Assets": "Data location",
    "s3AssetsDesc": "Choose an Amazon S3 location to store the raw and processed data for this project.",
    "selectS3": "Select an Amazon S3 Bucket",
    "sinkMaxInterval": "Sink maximum interval",
    "sinkMaxIntervalDesc": "Specifies the maximum length of time (in seconds) that records should be buffered before streaming to the AWS service",
    "sinkBatchSize": "Batch size",
    "sinkBatchSizeDesc": "The maximum number of records to deliver in a single batch",
    "kds": {
      "kdsSettings": "Amazon Kinesis Data Stream settings",
      "kdsSettingsDesc": "The solution will create a KDS for you based on your specifications.",
      "shardNum": "Shard number",
      "shardNumDesc": "Specify the number of Shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second.",
      "enableAS": "Enable autoscaling",
      "enableASDesc": "Choose Yes if you want to enable autoscaling for the KDS",
      "maxShard": "Maximum shard number",
      "maxShardDesc": "Specify maximum number of shards. ",
      "provisionMode": "Provision mode",
      "provisionModeDesc": "Choose On-demand if you want to enable autoscaling for the KDS",
      "selectMode": "Select provision mode",
      "onDemand": "On-demand",
      "provisioned": "Provisioned"
    },
    "msk": {
      "mskCluster": "Connect to an Apache Kafka Cluster",
      "mskClusterDesc": "You can choose to use Amazon Managed Streaming for Apache Kafka (Amazon MSK).",
      "select": "Amazon MSK",
      "topic": "Topic",
      "topicDesc": "By default, the solution will create a topic with “project id”, you can customize it.",
      "enterTopicName": "Enter a custom topic name",
      "manual": "Self-hosted",
      "brokerLink": "Broker link",
      "brokerLinkDesc": "Enter the connection URL for the cluster you wish to connect to.",
      "brokerLindPlaceHolder": "Enter the connection URL",
      "manualTopicDesc": "Please specify the topic for storing the data",
      "selectMSK": "Please select a MSK Cluster",
      "createMSK": "Create a quick-start cluster",
      "createMSKDesc": "The solution creates a small MSK cluster (2vCPU, 8RAM) as starter, suitable for testing purpose",
      "existingMSK": "Select an existing cluster",
      "existingMSKDesc": "The solution establish connection with a MSK cluster within the same VPC"
    },
    "s3": {
      "selectS3": "Select a S3 bucket for data buffer",
      "selectS3Desc": "Enter a destination in Amazon S3 where your data will be stored. You can consume the data from the selected S3 bucket",
      "s3URI": "S3 URI",
      "s3URIFormat": "Format: s3://bucket.",
      "s3Prefix": "S3 prefix",
      "s3PrefixDesc": "By default, the solution add prefix  of “<project_id>/YYYY/MM/dd/HH” (in UTC) to the data files when it deliver to S3. You can provide additional prefix which will be add before <project_id>. ",
      "s3Constraint": "You can repeat the same keys in your S3 bucket prefix. Maximum S3 bucket prefix characters: 1024",
      "enterAdditional": "Enter an additional prefix",
      "bufferSize": "Buffer size",
      "bufferSizeDesc": "Specify the data size to buffer before sending to Amazon S3. The higher buffer size may be lower in cost with higher latency, the lower buffer size will be faster in delivery with higher cost. Min: 1 MiB, Max: 50 MiB",
      "bufferInterval": "Buffer interval",
      "bufferIntervalDesc": "The higher interval allows more time to collect data and the size of data may be bigger. The lower interval sends the data more frequently and may be more advantageous when looking at shorter cycles of data activity. Min: 60, Max: 3600"
    },
    "pipeline": "Pipeline",
    "name": "Name",
    "nameDesc": "Please give a name of your  data pipeline that make sense to your organization.",
    "nameConstraint": "The name can be up to 200 characters long. Valid characters are a-z, A-Z, 0-9, . (period), _ (underscore) and - (hyphen).",
    "desc": "Description",
    "descPlaceholder": "your description",
    "awsRegion": "AWS Region",
    "awsRegionDesc": "Specify the region you want to deploy the pipeline into.",
    "awsRegionPlaceholder": "Select an AWS region",
    "vpc": "VPC",
    "vpcDesc": "Specify the VPC you want to deploy the pipeline into.",
    "vpcPlaceholder": "Select a VPC",
    "dataSDK": "Data collection SDK",
    "dataSDKDesc": "Specify the type of SDK you plan to use to send data to this pipeline. Note that Clickstream SDK refer to the SDK provided by this solution. ",
    "dataSDKPlaceholder": "Specify the SDK type",
    "selectSDKPlaceholder": "Specify the SDK type",
    "itemSelection": "Items selection",
    "lastEdit": "Last edit",
    "loading": "Loading resources",
    "noPlugin": "No plugins",
    "noPluginDisplay": "No plugins to display.",
    "findPlugin": "Find plugins",
    "selectEnrich": "Select the enrichment plugins that you want to enable or disable",
    "enrichPlugins": "Enrichment plugins",
    "code": "Code",
    "config": "Configuration",
    "enableEdp": "Enable custom endpoint",
    "enableEdpDesc": "The solution provides an auto-generated URL as ingestion endpoint, but you can also use a custom endpoint. Please note, the host name must be registered in Amazon Route53 service",
    "edpSettings": "Ingestion endpoint settings",
    "edpSettingsDesc": "The solution will spin up a web service as an ingestion endpoint to collect data sent from your SDKs.",
    "enableHttps": "Enable HTTPS",
    "domainName": "Domain name",
    "domainNameDesc": "Specify a domain name for your ingestion endpoint, the solution will create entry in Route53 for you automatically.",
    "domainNameR53Placeholder": "Choose a hosted zone",
    "hostedZone": "Hosted Zone",
    "requestPath": "Request path",
    "requestPathDesc": "The default path for the ingestion endpoint to collect data is “/collect”, you can overwrite it in below text box.",
    "requestPlaceholder": "collect",
    "cors": "Cross-Origin Resource Sharing (CORS)",
    "corsDesc": "CORS setting allows requests from specific domains to be sent by browsers. This is critical if you are collecting clickstream data from a web application, please input the allowed domain(s) in the following box.",
    "corsPlaceholder": "http://collect.example.com,https://data.example.com",
    "ingestionCapacity": "Ingestion capacity",
    "ingestionCapacityDesc": "A single Ingestion Compute Unit (ICU) represents billable compute and memory units, approximately 8 gigabytes (GB) of memory and 2 vCPUs. 1 ICU generally can support 4000~6000 requests per second.",
    "minSize": "Minimum capacity",
    "maxSize": "Maximum capacity",
    "warmPool": "Warm pool",
    "subnet": "Subnets",
    "subnetDesc": "Specify the subnets you want the ingestion endpoint to run in",
    "subnetPlaceholder": "Choose the subnet(s)",
    "dataSink": "Data sink settings",
    "dataSinkDesc": "Configure the how to sink the data for downstream consumption. ",
    "bufferType": "Sink type",
    "bufferTypeDesc": "Choose the type of data sink you want to use.",
    "bufferS3": "Amazon S3",
    "bufferS3Desc": "Data is cached in ingestion server memory then save into a S3 bucket, use this if you do not need real-time data streaming.",
    "bufferMSK": "Apache Kafka",
    "bufferMSKDesc": "Data will be streamed into an topic in a Kafka cluster for real-time consumption, and a Kafka connector will periodically read and store the data into a S3 bucket.",
    "bufferKDS": "Amazon Kinesis Data Stream (KDS)",
    "bufferKDSDesc": "Data will be streamed into Amazon KDS for real-time consumption, and a lambda program will periodically read and store the data into a S3 bucket.",
    "kafkaRequirements": "Your kafka cluster must meet the requirements highlighted in the documentation. ",
    "mskSecurityGroupDesc": "This VPC security group defines which subnets and IP ranges can access the Kafka cluster.",
    "enableModeling": "Enable data modeling",
    "enableModelingDesc": "This solution ships with an inbuilt data models for web and mobile client-side events to create out-of-the-box dashboards in reporting tool, which can also be customized to meet your needs. To enable the data model, you need to provide a mapping between your event data structure and the solution’s data model. If you choose not to use our data model, the raw event data with enrichments (if enabled in step 3) will be stored in the specified data destination without any modeling.",
    "enableDataModel": "Enable data model",
    "modelCreationMethod": "Creation method",
    "modelCreationMethodDesc": "Choose the method to create data mapping",
    "uploadFile": "Upload a mapping file",
    "uploadFileDesc": "Please use the template to define the mapping relationship",
    "usingUI": "Using UI",
    "usingUIDesc": "Define the mapping relationship for each field on the UI",
    "engineSetting": "Modeling engine settings",
    "engineSettingDesc": "Select the engine to model the data and configure how the data modeling job run",
    "engineRedshift": "Redshift",
    "engineRedshiftDesc": "Data will be loaded into Redshift, modeling job will run by Redshift",
    "engineAthena": "Athena",
    "engineAthenaDesc": "Data will be stored in S3, modeling job will be run by Athena",
    "engineRedshiftCluster": "Redshift cluster",
    "engineRedshiftClusterDesc": "Select a redshift cluster as the data modeling engine for this pipeline ",
    "engineRedshiftClusterPlaceholder": "Find a cluster",
    "engineDataRange": "Data range",
    "engineDataRangeDesc": "Specify the duration for the data you want to load into Redshift for data modeling, Redshift will delete the data beyond specified range. But all data will still be available in S3 for your query with Redshift Specturm or Athena.",
    "engineDuration": "Duration",
    "engineDurationPlaceholder": "Enter duration",
    "engineUnitOfTime": "Unit of time",
    "engineDataPartition": "Data partition",
    "engineDataPartitionDesc": "Specify how you want to partition the data in Redshift.",
    "engineDataPartitionAlert": "This setting will impact the dashboard and metrics for your business, please select carefully.",
    "engineBaseIngestionTime": "Partition based on ingestion time",
    "engineBaseIngestionTimeDesc": "Event data will be partitioned based on when it was received by the ingestion endpoint. ",
    "reportSettings": "Reporting settings",
    "reportSettingsDesc": "Specify QuickSight settings for the solution create reporting for you",
    "createSampleQuickSight": "Create sample dashboard in QuickSight",
    "createSampleQuickSightDesc": "Enabling this feature will allow the solution to create sample dashboards (<learnmore_anchor>Learn More</learnmore_anchor>) in your QuickSight Account. If your AWS account hasn't sign up for QuickSight, you will be asked to sign up first by following this <guide_anchor>guide</guide_anchor>.",
    "quickSightNotSub": "No QuickSight subscription in your account",
    "quickSightNotSubDesc1": "The solution detects that your account has not subscribed to QuickSight, please ",
    "quickSightNotSubDesc2": ". Note additional costs apply.",
    "quickSightNotEnterprise": "QuickSight edition is not enterprise in your account",
    "quickSightNotEnterpriseDesc": "The solution dashboard need the QuickSight Enterprise, you can upgrade edition in QuickSight Management Console.",
    "quickSightSubscription": "Subscription",
    "quickSightUser": "QuickSight user",
    "quickSightUserDesc": "Select a QuickSight user for the solution to create datasets and analyses. Click “Create new” button to create a new user.",
    "quickSIghtPlaceholder": "Select a quicksight user",
    "ingestSettings": "Ingestion setting",
    "clusterSize": "Cluster Size",
    "topic": "Topic",
    "modelSettings": "Data modeling setting",
    "modelSettingsDesc": "Data modeling setting description",
    "modelEngine": "Modeling engine",
    "enableETL": "Enable data processing ",
    "enableETLDesc1": "This solution ships with an inbuilt data schema ",
    "enableETLDesc2": " to model the raw event data sent from your web and mobile apps, so that it can generate out-of-the-box dashboards and makes it easy for you to build your business-specific analytics. To use our data model, you need to enable Data Processing module to transform the raw event data to our data model. If you choose otherwise, your data will be saved in S3 in its raw format and no dashboard will be generated.",
    "transform": "Transform",
    "transformDesc1": "Since you have chosen to use a third-party SDK, you need to provide a custom plugin to transform the raw data to the solution data model ",
    "transformDesc2": " to enable enrichment and data modeling",
    "publicSubnet": "Public Subnets",
    "publicSubnetDesc": "Please select public subnets. You need at least two public subnets across two Availability Zones (AZ).",
    "privateSubnet": "Private Subnets",
    "privateSubnetDesc": "Please select private subnets. You need at least two private subnets across two Availability Zones (AZ).",
    "workgroup": "Workgroup",
    "workgroupDesc": "Select a work group to perform the modeling and query",
    "findWorkGroup": "Find a work group",
    "duration": "Duration",
    "executionParam": "Execution parameters",
    "executionParamDesc": "Configure the key behaviors of the data processing jobs.",
    "processInterval": "Data processing interval",
    "processIntervalDesc": "Specify the interval to batch the data for data processing. Please make sure the specified interval is longer than the ingestion buffer interval.",
    "eventFreshness": "Event freshness",
    "eventFreshnessDesc": "Event freshness is the period after which the solution will ignore the event data. For example, if you specify 3 days for this parameter, the solution will ignore any event arrive more than 3 days after the events are triggered in clients.",
    "analyticEngine": "Data modeling settings",
    "analyticEngineDesc": "Select analytic engine to model and query your clickstream data.",
    "redshift": "Redshift",
    "redshiftAdditionalSettings": "Additional settings",
    "redshiftDesc": "Data will be load into Amazon Redshift data warehouse for further modeling and query.",
    "redshiftCluster": "Redshift cluster",
    "redshiftClusterDesc": "Select a redshift cluster as the data modeling engine for this pipeline.",
    "redshiftBaseCapacity": "Base RPU",
    "redshiftBaseCapacityDesc": "Set the base capacity in Redshift processing units (RPUs) used to process your workload. One RPU provides 16 GB of memory.",
    "redshiftVpc": "VPC",
    "redshiftVpcDesc": "Specify the VPC you want to deploy the Redshift Serverless workgroup into.",
    "securityGroup": "Security Group",
    "redshiftSecurityGroupDesc": "This VPC security group defines which subnets and IP ranges can access the Serverless workgroup.",
    "securityGroupPlaceholder": "Choose security group",
    "redshiftSubnet": "Subnet",
    "redshiftSubnetDesc": "Please select three subnets for Redshift serverless. We recommend to use private subnets for better security.",
    "redshiftDatabaseUser": "Database user",
    "redshiftDatabaseUserDesc": "The solution needs permissions to access and create database in Redshift cluster. By default, it grants Redshift Data API with the permissions of the admin user to execute the commands to create DB, tables, and views, as well as loading data.",
    "redshiftDatabaseUserPlaceholder": "database_admin_username",
    "redshiftUserTableUpsertFrequency": "User table upsert frequency",
    "redshiftUserTableUpsertFrequencyDesc": "The solution runs a SQL stored procedures to update a user dimension table periodically. Specify the interval to update the user dimension table.",
    "athena": "Athena",
    "athenaDesc": "Select this option will create table schema and sample queries in Athena for you to query the processed data on S3",
    "certificate": "SSL Certificate",
    "selectCertificate": "Choose SSL Certificate",
    "accessPermissions": "Access permissions",
    "accessPermissionsDesc": "The solution needs permissions to access and create database in Redshift Serverless. Create or choose an Identity and Access Management (IAM) role with required permissions. Click Info to see ",
    "permissionLink": "Permissions required to successfully create database in Redshift Serverless",
    "findIAMRole": "Find an IAM role",
    "aga": " AWS Global Accelerator",
    "agaDesc": "Create an accelerator to get static IP addresses that act as a global fixed entry point to your ingestion server, which will improves the availability and performance of your ingestion server. Note that additional charges apply.",
    "auth": "Authentication",
    "authDesc": "You can use OpenID Connector (OIDC) provider to authenticate the request sent to your ingestion server. If you plan to enable it, please create an OIDC client in the OIDC provider then create a secret in AWS Secret Manager with information “issuer”, “token endpoint”, User endpoint”, “Authorization endpoint”, “App client ID”, and “App Client Secret”.",
    "enableALBLog": "Access logs",
    "enableALBLogDesc": "Application Load Balancer support delivering detailed logs of all requests it receives. If you enable this option, the solution will automatically enable access logs for you and store the logs into the S3 bucket you selected in previous step. Note that you need add permission to the S3 bucket for ALB to deliver logs. ",
    "connector": "Connector",
    "connectorDesc": "Create a Apache Kafka connector to stream data to S3.",
    "connectorCheck": "The solution will download the Confluent <connector_anchor>Amazon S3 sink connector</connector_anchor> follow by this <guide_anchor>guide</guide_anchor>",
    "connectorCheckDesc": "Allow solution to create a custom plugin for the connector.",
    "securityWarning": "Security warning",
    "securityWarningDesc": "Using HTTP protocol is not secure, data will be sent without any encryption, there are high risks of data being leaked or tampered during transmission, please acknowledge the risk before proceed.",
    "nextSteps": "Next steps",
    "nextStepsDesc": "Once you create your domain, use the custom endpoint to create an alias or CNAME mapping in your Domain Name System (DNS) for the custom endpoint.",
    "dataProcessing": "Data processing",
    "secret": "Secret",
    "selectSecret": "Select a secret",
    "createQSSub": "Create QuickSight subscription",
    "qsAccountName": "QuickSight account name",
    "qsAccountNameDesc": "Provide a name for your QuickSight, Note that this name needs to be global unique.",
    "qsUserEmail": "QuickSight user email",
    "qsUserEmailDesc": "Provide a email for your QuickSight.",
    "createQSUser": "Create QuickSight user",
    "qsCreateUserDesc": "provide a link for you to activate the user and change password.",
    "qsUserActive": "Activate user by clicking below link",
    "redshiftServerless": "Serverless",
    "redshiftServerlessDesc": "Low starting cost, auto-scaling capacity, paid-as-you-go pricing.",
    "redshiftProvisioned": "Provisioned",
    "redshiftProvisionedDesc": "Provisioned Redshift cluster that meet your cost and performance specifications",
    "reportNotSupported": "Not Supported",
    "reportNotSupportedDesc": "Reporting is not supported if data processing is not enabled.",
    "notSupportedServices": "These service(s): {{unSupportedServices}} are not available in this region, some of features may not supported."
  },
  "detail": {
    "ingestion": "Ingestion",
    "processing": "Processing",
    "reporting": "Reporting",
    "monitoring": "Monitoring",
    "alarms": "Alarms",
    "publicSubnet": "Public Subnets",
    "privateSubnet": "Private Subnets",
    "ingestionCapacity": "Ingestion capacity",
    "enableHTTPS": "Enable HTTPS",
    "domainName": "Domain name",
    "dns": "DNS",
    "endpoint": "Endpoint",
    "acm": "ACM",
    "enableAGA": "Enable Global Accelerator",
    "enableAuth": "Enable Authentication",
    "dataBuffer": "Data Buffer",
    "topic": "Topic",
    "enableALBLog": "Enable ALB access log",
    "pipelineID": "pipeline ID",
    "version": "Version",
    "s3Bucket": "Amazon S3 Bucket",
    "sdk": "Data Collection SDK",
    "cfnStack": "Cloudformation Stack",
    "region": "Region",
    "vpc": "VPC",
    "creationTime": "Creation Time",
    "updateTime": "Update Time",
    "min": "Min",
    "max": "Max",
    "warm": "Warm",
    "status": "Status",
    "dataProcessingInt": "Data processing interval",
    "eventFreshness": "Event freshness",
    "transform": "Transformation",
    "enrichment": "Enrichment",
    "analyticEngine": "Analytic engine",
    "redshiftPermission": "Redshift Permissions",
    "redshiftUserTableUpsertFrequency": "User table upsert frequency",
    "dataRange": "Data range",
    "redshift": "Redshift",
    "athena": "Athena",
    "hours": "Hours",
    "minutes": "Minutes",
    "quicksightRole": "QuickSight Role",
    "datasetName": "Dataset name",
    "accessFromConsole": "Access monitoring dashboard in CloudWatch from AWS console.",
    "accessFromConsoleDesc": "The solution had create a CloudWatch Dashboard that includes key metrics for monitoring this data pipeline, click above button to access the dashboard in your AWS console.",
    "accessFromSolution": "Access monitoring dashboard in the solution console.",
    "accessFromSolutionDesc": "The solution had create a CloudWatch dashboard that includes key metrics for monitoring this data pipeline, to integrate the CloudWatch dashboard into the solution console, please follow below steps:",
    "accessFromSolutionStep1": "Go to CloudWatch Settings",
    "accessFromSolutionStep2": "Set up SSO for CloudWatch dashboard sharing",
    "accessFromSolutionStep3": "Go to the Cognito user pool created for CloudWatch dashboard sharing, add the OIDC provider of control plane as the federated OIDC",
    "accessFromSolutionStep4": "Go to CloudWatch Settings, select the federated OIDC as SSO provider",
    "accessFromSolutionStep5": "Go to the clickstream pipeline dashboard, share dashboard to get the sharing URL",
    "accessFromSolutionStep6": "Provide the sharing URL in below input box and click Submit button.",
    "sharingUrl": "Dashboard sharing URL",
    "alarmName": "Alarm name",
    "alarmTable": "Alarms",
    "alarmTableDesc": "Alarms of the pipeline of this project.",
    "alarmTableColumnName": "Name",
    "alarmTableColumnDesc": "Description",
    "alarmTableColumnState": "State",
    "alarmTableColumnAction": "Action",
    "alarmTableLoading": "Loading resources",
    "alarmTableNoAlarm": "No alarms",
    "alarmTableNoAlarmDisplay": "No alarms to display.",
    "alarmFindAlarm": "Find your alarm",
    "alarmTableActionEnable": "Enable",
    "alarmTableActionDisable": "No actions",
    "action": "Action",
    "stackDetails": "Stack Details",
    "redshiftServerless": "Redshift Serverless",
    "dashboards": "Dashboards",
    "alarmDescription": "Description",
    "tags": "Tags",
    "bufferSize": "Buffer size",
    "bufferInterval": "Buffer interval"
  },
  "list": {
    "id": "Pipeline ID",
    "name": "Pipeline name",
    "region": "Region",
    "status": "Status",
    "created": "Creation date",
    "loading": "Loading resources",
    "noPipeline": "No pipelines",
    "noPipelineDisplay": "No pipelines to display.",
    "findPipeline": "Find your pipeline",
    "pipelineDesc": "All the clickstream analytics data pipelines in your AWS account.",
    "pipelineList": "Pipeline List"
  },
  "upgrade": {
    "title": "Upgrade Pipeline",
    "tip": "Are you sure you want to upgrade the pipeline to this version?"
  }
}